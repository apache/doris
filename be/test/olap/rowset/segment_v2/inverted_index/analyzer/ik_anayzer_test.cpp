// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

#include <gtest/gtest.h>

#include <fstream>
#include <limits>
#include <memory>
#include <sstream>

#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/IKAnalyzer.h"
#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/cfg/Configuration.h"
#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/core/AnalyzeContext.h"
#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/core/IKSegmenter.h"
#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/core/Lexeme.h"
#include "vec/common/arena.h"
using namespace lucene::analysis;

namespace doris::segment_v2 {

class IKTokenizerTest : public ::testing::Test {
protected:
    void tokenize(const std::string& s, std::vector<std::string>& datas, bool isSmart) {
        try {
            IKAnalyzer analyzer;
            analyzer.initDict("./be/dict/ik");
            analyzer.setMode(isSmart);
            analyzer.set_lowercase(true);

            lucene::util::SStringReader<char> reader;
            reader.init(s.data(), s.size(), false);

            std::unique_ptr<IKTokenizer> tokenizer;
            tokenizer.reset((IKTokenizer*)analyzer.tokenStream(L"", &reader));

            Token t;
            while (tokenizer->next(&t)) {
                std::string term(t.termBuffer<char>(), t.termLength<char>());
                datas.emplace_back(term);
            }
        } catch (CLuceneError& e) {
            std::cout << e.what() << std::endl;
            throw;
        }
    }

    /**
     * Token information class, contains token text and type
     */
    struct TokenInfo {
        std::string text;
        Lexeme::Type type;

        TokenInfo(const std::string& text, Lexeme::Type type) : text(text), type(type) {}
    };

    /**
     * Simplified function to get tokens with type information
     * Reuses the same configuration as tokenize()
     */
    void getTokensWithType(const std::string& s, std::vector<TokenInfo>& tokenInfos,
                           bool isSmart = false) {
        std::shared_ptr<Configuration> config = std::make_shared<Configuration>(isSmart, true);
        config->setDictPath("./be/dict/ik");
        IKSegmenter segmenter(config);

        lucene::util::SStringReader<char> reader;
        reader.init(s.data(), s.size(), false);
        segmenter.reset(&reader);

        Lexeme lexeme;
        while (segmenter.next(lexeme)) {
            tokenInfos.emplace_back(lexeme.getText(), lexeme.getType());
        }
    }

    /**
     * Helper function to check if a token with specific type exists
     */
    bool hasTokenWithType(const std::vector<TokenInfo>& tokens, const std::string& text,
                          Lexeme::Type type) {
        for (const auto& token : tokens) {
            if (token.text == text && token.type == type) {
                return true;
            }
        }
        return false;
    }

    /**
     * Helper function to check if a token exists (any type)
     */
    bool hasToken(const std::vector<TokenInfo>& tokens, const std::string& text) {
        for (const auto& token : tokens) {
            if (token.text == text) {
                return true;
            }
        }
        return false;
    }

    // Helper method to create a temporary dictionary file for testing
    std::string createTempDictFile(const std::string& content) {
        std::string temp_file = "/tmp/temp_dict_" + std::to_string(rand()) + ".dic";
        std::ofstream out(temp_file);
        out << content;
        out.close();
        return temp_file;
    }

    // Flag to indicate if the current test is an exception test
    bool is_exception_test = true;
};

// Test for Dictionary exception handling
TEST_F(IKTokenizerTest, TestDictionaryExceptionHandling) {
    // Test 1: Load non-existent path
    {
        Configuration cfg;
        cfg.setDictPath("/non_existent_path");
        cfg.setMainDictFile("main.dic");
        cfg.setQuantifierDictFile("quantifier.dic");
        cfg.setStopWordDictFile("stopword.dic");

        // Expect an exception
        ASSERT_THROW({ Dictionary::initial(cfg, false); }, CLuceneError);

        // Singleton should be nullptr, getSingleton() will throw an exception
        ASSERT_NO_THROW({ Dictionary::getSingleton(); });
    }

    // Test 2: Use reload method with invalid path
    {
        Dictionary* dict = Dictionary::getSingleton();
        // Set path to invalid path
        dict->getConfiguration()->setDictPath("/non_existent_path");

        ASSERT_THROW({ Dictionary::reload(); }, CLuceneError);
    }

    // Test 3: Initialize with valid dictionary path
    {
        Dictionary::getSingleton()->getConfiguration()->setDictPath("./be/dict/ik");

        // Expect no exception
        ASSERT_NO_THROW({ Dictionary::reload(); });

        // Singleton should be successfully created
        Dictionary* dict = nullptr;
        ASSERT_NO_THROW({ dict = Dictionary::getSingleton(); });
        ASSERT_NE(dict, nullptr);
    }
}

TEST_F(IKTokenizerTest, TestIKTokenizer) {
    std::vector<std::string> datas;

    // Test with max_word mode
    std::string Text1 = "ä¸­åäººæ°‘å…±å’Œå›½å›½æ­Œ";
    tokenize(Text1, datas, false);
    ASSERT_EQ(datas.size(), 10);
    datas.clear();

    // Test with smart mode
    tokenize(Text1, datas, true);
    ASSERT_EQ(datas.size(), 2);
    datas.clear();

    std::string Text2 = "äººæ°‘å¯ä»¥å¾—åˆ°æ›´å¤šå®æƒ ";
    tokenize(Text2, datas, false);
    ASSERT_EQ(datas.size(), 5);
    datas.clear();

    // Test with smart mode
    tokenize(Text2, datas, true);
    ASSERT_EQ(datas.size(), 5);
    datas.clear();

    std::string Text3 = "ä¸­å›½äººæ°‘é“¶è¡Œ";
    tokenize(Text3, datas, false);
    ASSERT_EQ(datas.size(), 8);
    datas.clear();

    // Test with smart mode
    tokenize(Text3, datas, true);
    ASSERT_EQ(datas.size(), 1);
    datas.clear();
}

TEST_F(IKTokenizerTest, TestIKRareTokenizer) {
    std::vector<std::string> datas;

    // Test with rare characters
    std::string Text = "è©ğªœ®é¾Ÿé¾™éºŸå‡¤å‡¤";
    tokenize(Text, datas, true);
    ASSERT_EQ(datas.size(), 4);
    std::vector<std::string> result = {"è©", "ğªœ®", "é¾Ÿé¾™éºŸå‡¤", "å‡¤"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(IKTokenizerTest, TestIKSmartModeTokenizer) {
    std::vector<std::string> datas;

    // Test smart mode tokenization
    std::string Text1 = "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦";
    tokenize(Text1, datas, true);
    ASSERT_EQ(datas.size(), 4);
    std::vector<std::string> result1 = {"æˆ‘", "æ¥åˆ°", "åŒ—äº¬", "æ¸…åå¤§å­¦"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result1[i]);
    }
    datas.clear();

    // Test another example with smart mode
    std::string Text2 = "ä¸­å›½çš„ç§‘æŠ€å‘å±•åœ¨ä¸–ç•Œä¸Šå¤„äºé¢†å…ˆ";
    tokenize(Text2, datas, true);
    ASSERT_EQ(datas.size(), 7);
    std::vector<std::string> result2 = {"ä¸­å›½", "çš„", "ç§‘æŠ€", "å‘å±•", "åœ¨ä¸–ç•Œä¸Š", "å¤„äº", "é¢†å…ˆ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result2[i]);
    }
    datas.clear();
}

TEST_F(IKTokenizerTest, TestIKMaxWordModeTokenizer) {
    std::vector<std::string> datas;

    // Test max word mode tokenization
    std::string Text1 = "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦";
    tokenize(Text1, datas, false);
    ASSERT_EQ(datas.size(), 6);
    std::vector<std::string> result1 = {"æˆ‘", "æ¥åˆ°", "åŒ—äº¬", "æ¸…åå¤§å­¦", "æ¸…å", "å¤§å­¦"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result1[i]);
    }
    datas.clear();

    // Test another example with max word mode
    std::string Text2 = "ä¸­å›½çš„ç§‘æŠ€å‘å±•åœ¨ä¸–ç•Œä¸Šå¤„äºé¢†å…ˆ";
    tokenize(Text2, datas, false);
    ASSERT_EQ(datas.size(), 11);
    std::vector<std::string> result2 = {"ä¸­å›½",   "çš„",   "ç§‘æŠ€", "å‘å±•", "åœ¨ä¸–ç•Œä¸Š", "åœ¨ä¸–",
                                        "ä¸–ç•Œä¸Š", "ä¸–ç•Œ", "ä¸Š",   "å¤„äº", "é¢†å…ˆ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result2[i]);
    }
    datas.clear();
}

TEST_F(IKTokenizerTest, TestEmptyInput) {
    std::vector<std::string> datas;
    // Test with empty input
    std::string emptyText = "";
    tokenize(emptyText, datas, true);
    ASSERT_EQ(datas.size(), 0);
}

TEST_F(IKTokenizerTest, TestSingleByteInput) {
    std::vector<std::string> datas;
    // Test with single byte input
    std::string singleByteText = "b";
    tokenize(singleByteText, datas, true);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "b");
}

TEST_F(IKTokenizerTest, TestLargeInput) {
    std::vector<std::string> datas;
    // Test with large input
    std::string largeText;
    for (int i = 0; i < 1000; i++) {
        largeText += "ä¸­å›½çš„ç§‘æŠ€å‘å±•åœ¨ä¸–ç•Œä¸Šå¤„äºé¢†å…ˆ";
    }
    tokenize(largeText, datas, true);
    ASSERT_EQ(datas.size(), 7000);
}

TEST_F(IKTokenizerTest, TestBufferExhaustCritical) {
    std::vector<std::string> datas;
    // Test with buffer exhaustion critical case
    std::string criticalText;
    for (int i = 0; i < 95; i++) {
        criticalText += "çš„";
    }
    tokenize(criticalText, datas, true);
    ASSERT_EQ(datas.size(), 95);
}

TEST_F(IKTokenizerTest, TestMixedLanguageInput) {
    std::vector<std::string> datas;
    // Test with mixed language input
    std::string mixedText =
            "Dorisæ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„MPPåˆ†æå‹æ•°æ®åº“ï¼Œå¯ä»¥å¤„ç†PBçº§åˆ«çš„æ•°æ®ï¼Œæ”¯æŒSQL92å’ŒSQL99ã€‚";
    tokenize(mixedText, datas, true);

    std::vector<std::string> expectedTokens = {
            "doris", "æ˜¯", "ä¸€ä¸ª", "ç°ä»£åŒ–", "çš„",   "mpp",  "åˆ†æ",  "å‹", "æ•°æ®åº“", "å¯ä»¥",
            "å¤„ç†",  "pb", "çº§",   "åˆ«çš„",   "æ•°æ®", "æ”¯æŒ", "sql92", "å’Œ", "sql99"};
    ASSERT_EQ(datas.size(), expectedTokens.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedTokens[i]);
    }
}

TEST_F(IKTokenizerTest, TestSpecialCharacters) {
    std::vector<std::string> datas;
    // Test with special characters
    std::string specialText = "ğŸ˜ŠğŸš€ğŸ‘æµ‹è¯•ç‰¹æ®Šç¬¦å·ï¼š@#Â¥%â€¦â€¦&*ï¼ˆï¼‰";
    tokenize(specialText, datas, true);
    ASSERT_EQ(datas.size(), 5);
    std::vector<std::string> expectedTokens = {"ğŸ˜Š", "ğŸš€", "ğŸ‘", "æµ‹è¯•", "ç‰¹æ®Šç¬¦å·"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedTokens[i]);
    }
}

TEST_F(IKTokenizerTest, TestBufferBoundaryWithSpace) {
    std::vector<std::string> datas;

    // Test with exact buffer boundary
    std::string exactText;
    int charCount = 4096 / 3;
    for (int i = 0; i < charCount; i++) {
        exactText += "ä¸­";
    }
    exactText += " ";

    tokenize(exactText, datas, true);
    ASSERT_EQ(datas.size(), charCount);
    datas.clear();

    // Test with buffer boundary overflow
    std::string overText;
    charCount = 4096 / 3 + 1;
    for (int i = 0; i < charCount; i++) {
        overText += "ä¸­";
    }
    overText += " ";

    tokenize(overText, datas, true);
    ASSERT_EQ(datas.size(), charCount);
    datas.clear();

    // Test with multiple spaces at buffer boundary
    std::string multiSpaceText;
    charCount = 4096 / 3 - 3;
    for (int i = 0; i < charCount; i++) {
        multiSpaceText += "ä¸­";
    }
    multiSpaceText += "   ";

    tokenize(multiSpaceText, datas, true);
    ASSERT_EQ(datas.size(), charCount);
    datas.clear();

    // Test with spaces around buffer boundary
    std::string spaceAroundBoundaryText;
    charCount = 4096 / 3 - 2;
    for (int i = 0; i < charCount / 2; i++) {
        spaceAroundBoundaryText += "ä¸­";
    }
    spaceAroundBoundaryText += " ";
    for (int i = 0; i < charCount / 2; i++) {
        spaceAroundBoundaryText += "ä¸­";
    }
    spaceAroundBoundaryText += "  ";

    tokenize(spaceAroundBoundaryText, datas, true);
    ASSERT_EQ(datas.size(), charCount - 1);
}

TEST_F(IKTokenizerTest, TestChineseCharacterAtBufferBoundary) {
    std::vector<std::string> datas;

    std::string boundaryText;
    // Test with a complete Chinese character cut at the first byte
    int completeChars = 4096 / 3;
    for (int i = 0; i < completeChars; i++) {
        boundaryText += "ä¸­";
    }

    boundaryText += "å›½";

    tokenize(boundaryText, datas, true);
    ASSERT_EQ(datas.size(), completeChars + 1);
    ASSERT_EQ(datas[datas.size() - 1], "å›½");
    datas.clear();
    boundaryText.clear();
    // Test with a complete Chinese character cut at the second byte
    boundaryText += "  ";

    for (int i = 0; i < completeChars; i++) {
        boundaryText += "ä¸­";
    }

    boundaryText += "å›½";

    tokenize(boundaryText, datas, true);
    ASSERT_EQ(datas.size(), completeChars);
    ASSERT_EQ(datas[datas.size() - 1], "ä¸­å›½");

    datas.clear();
    boundaryText.clear();
}

TEST_F(IKTokenizerTest, TestLongTextCompareWithJava) {
    std::vector<std::string> datas;

    // Test with long text and compare results with Java implementation
    std::string longText =
            "éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œç­‰æŠ€æœ¯å·²ç»åœ¨å„ä¸ªé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›"
            "åº”ç”¨ã€‚"
            "ä»è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†åˆ°è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼å’Œå·¥ä½œæ–¹å¼ã€‚"
            "åœ¨åŒ»ç–—é¢†åŸŸï¼ŒAIè¾…åŠ©è¯Šæ–­ç³»ç»Ÿå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«ç–¾ç—…ï¼›åœ¨é‡‘èé¢†åŸŸï¼Œæ™ºèƒ½ç®—æ³•å¯ä»¥é¢„æµ‹å¸‚"
            "åœºè¶‹åŠ¿å’Œé£é™©ï¼›"
            "åœ¨æ•™è‚²é¢†åŸŸï¼Œä¸ªæ€§åŒ–å­¦ä¹ å¹³å°å¯ä»¥æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ æƒ…å†µæä¾›å®šåˆ¶åŒ–çš„æ•™å­¦å†…å®¹ã€‚"
            "ç„¶è€Œï¼Œéšç€AIæŠ€æœ¯çš„æ™®åŠï¼Œä¹Ÿå¸¦æ¥äº†ä¸€ç³»åˆ—çš„ä¼¦ç†å’Œéšç§é—®é¢˜ã€‚å¦‚ä½•ç¡®ä¿AIç³»ç»Ÿçš„å…¬å¹³æ€§å’Œé€æ˜åº¦"
            "ï¼Œ"
            "å¦‚ä½•ä¿æŠ¤ç”¨æˆ·æ•°æ®çš„å®‰å…¨ï¼Œå¦‚ä½•é˜²æ­¢AIè¢«æ»¥ç”¨ï¼Œè¿™äº›éƒ½æ˜¯æˆ‘ä»¬éœ€è¦æ€è€ƒçš„é—®é¢˜ã€‚"
            "æ­¤å¤–ï¼ŒAIçš„å‘å±•ä¹Ÿå¯èƒ½å¯¹å°±ä¸šå¸‚åœºäº§ç”Ÿå½±å“ï¼Œä¸€äº›ä¼ ç»Ÿå·¥ä½œå¯èƒ½ä¼šè¢«è‡ªåŠ¨åŒ–ç³»ç»Ÿå–ä»£ï¼Œ"
            "ä½†åŒæ—¶ä¹Ÿä¼šåˆ›é€ å‡ºæ–°çš„å·¥ä½œå²—ä½å’Œæœºä¼šã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç§¯æé€‚åº”è¿™ä¸€å˜åŒ–ï¼Œ"
            "æå‡è‡ªå·±çš„æŠ€èƒ½å’ŒçŸ¥è¯†ï¼Œä»¥ä¾¿åœ¨AIæ—¶ä»£ä¿æŒç«äº‰åŠ›ã€‚"
            "æ€»çš„æ¥è¯´ï¼Œäººå·¥æ™ºèƒ½æ˜¯ä¸€æŠŠåŒåˆƒå‰‘ï¼Œå®ƒæ—¢å¸¦æ¥äº†å·¨å¤§çš„æœºé‡ï¼Œä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚"
            "æˆ‘ä»¬éœ€è¦ç†æ€§çœ‹å¾…AIçš„å‘å±•ï¼Œæ—¢è¦å……åˆ†åˆ©ç”¨å®ƒçš„ä¼˜åŠ¿ï¼Œä¹Ÿè¦è­¦æƒ•å¯èƒ½çš„é£é™©ï¼Œ"
            "å…±åŒæ¨åŠ¨AIæŠ€æœ¯å‘ç€æ›´åŠ å¥åº·ã€å¯æŒç»­çš„æ–¹å‘å‘å±•ã€‚";

    // Repeat 4 times to create a long text
    int i = 0;
    while (i < 4) {
        longText += longText;
        i++;
    }
    // Test with smart mode
    tokenize(longText, datas, true);

    ASSERT_EQ(datas.size(), 3312);

    // Compare first 20 tokens with Java implementation
    std::vector<std::string> javaFirst20Results = {
            "éšç€",     "äººå·¥æ™ºèƒ½æŠ€æœ¯", "çš„",   "å¿«é€Ÿ",     "å‘å±•", "æ·±åº¦", "å­¦ä¹ ",
            "æœºå™¨",     "å­¦ä¹ ",         "å’Œ",   "ç¥ç»ç½‘ç»œ", "ç­‰",   "æŠ€æœ¯", "å·²ç»åœ¨",
            "å„ä¸ªé¢†åŸŸ", "å¾—",           "åˆ°äº†", "å¹¿æ³›åº”ç”¨", "ä»",   "è¯­éŸ³"};
    for (size_t i = 0; i < 20; i++) {
        ASSERT_EQ(datas[i], javaFirst20Results[i]);
    }

    // Compare last 20 tokens with Java implementation
    std::vector<std::string> javaLast20Results = {
            "å‘å±•", "æ–¹å‘", "çš„",   "æŒç»­", "å¯",   "å¥åº·", "æ›´åŠ ", "å‘ç€", "æŠ€æœ¯", "ai",
            "æ¨åŠ¨", "å…±åŒ", "é£é™©", "çš„",   "å¯èƒ½", "è­¦æƒ•", "ä¹Ÿè¦", "ä¼˜åŠ¿", "çš„",   "å®ƒ"};
    for (size_t i = 0; i < 20; i++) {
        ASSERT_EQ(datas[datas.size() - i - 1], javaLast20Results[i]);
    }

    // Test with max_word mode
    datas.clear();
    javaFirst20Results = {"éšç€",     "äººå·¥æ™ºèƒ½æŠ€æœ¯", "äººå·¥æ™ºèƒ½", "äººå·¥", "æ™ºèƒ½", "æŠ€æœ¯",  "çš„",
                          "å¿«é€Ÿ",     "å‘å±•",         "æ·±åº¦",     "å­¦ä¹ ", "æœºå™¨", "å­¦ä¹ ",  "å’Œ",
                          "ç¥ç»ç½‘ç»œ", "ç¥ç»",         "ç½‘ç»œ",     "ç­‰",   "æŠ€æœ¯", "å·²ç»åœ¨"};
    javaLast20Results = {"å‘å±•", "æ–¹å‘", "çš„",   "æŒç»­", "å¯",   "å¥åº·", "æ›´åŠ ",
                         "å‘ç€", "æŠ€æœ¯", "ai",   "æ¨åŠ¨", "å…±åŒ", "é£é™©", "çš„",
                         "å¯èƒ½", "è­¦æƒ•", "ä¹Ÿè¦", "ä¼˜åŠ¿", "çš„",   "ç”¨å®ƒ"};

    tokenize(longText, datas, false);
    ASSERT_EQ(datas.size(), 4336);

    // Compare first 20 tokens with Java implementation
    for (size_t i = 0; i < 20; i++) {
        ASSERT_EQ(datas[i], javaFirst20Results[i]);
    }

    // Compare last 20 tokens with Java implementation
    for (size_t i = 0; i < 20; i++) {
        ASSERT_EQ(datas[datas.size() - i - 1], javaLast20Results[i]);
    }
}

TEST_F(IKTokenizerTest, TestFullWidthCharacters) {
    std::vector<std::string> datas;

    // test full width numbers
    std::string fullWidthNumbersText = "ï¼” ï¼“ ï¼’";
    tokenize(fullWidthNumbersText, datas, true);
    std::vector<std::string> expectedNumbers = {"4", "3", "2"}; // half width numbers
    ASSERT_EQ(datas.size(), expectedNumbers.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedNumbers[i]);
    }
    datas.clear();

    fullWidthNumbersText = "ï¼”ï¼“ï¼’";
    tokenize(fullWidthNumbersText, datas, false);
    expectedNumbers = {"432"};
    ASSERT_EQ(datas.size(), expectedNumbers.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedNumbers[i]);
    }
    datas.clear();

    // test full width currency symbol
    std::string currencyText = "ï¿¥";
    tokenize(currencyText, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ï¿¥");
    datas.clear();

    // test full width symbol in word
    std::string mixedText = "Highï¼†Low";
    tokenize(mixedText, datas, false);
    std::vector<std::string> expectedMixed = {"high&low", "high", "low"};
    ASSERT_EQ(datas.size(), expectedMixed.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedMixed[i]);
    }
    datas.clear();

    // test special separator
    std::string specialSeparatorText = "1ï½¥2";
    tokenize(specialSeparatorText, datas, false);
    std::vector<std::string> expectedSeparator = {"1", "ï½¥", "2"};
    ASSERT_EQ(datas.size(), expectedSeparator.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedSeparator[i]);
    }
    datas.clear();

    // test special character
    std::string specialCharText = "ï¨‘";
    tokenize(specialCharText, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ï¨‘");
    datas.clear();
}

TEST_F(IKTokenizerTest, TestEmojiAndSpecialCharacters) {
    std::vector<std::string> datas;

    // test emoji
    std::string emojiText = "ğŸ¼";
    tokenize(emojiText, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ğŸ¼");
    datas.clear();

    std::string emojiText2 = "ğŸ¢";
    tokenize(emojiText2, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ğŸ¢");
    datas.clear();

    // test special latin character
    std::string specialLatinText1 = "abcÅŸabc";
    tokenize(specialLatinText1, datas, false);
    ASSERT_EQ(datas.size(), 2);
    ASSERT_EQ(datas[0], "abc");
    ASSERT_EQ(datas[1], "abc");
    datas.clear();

    std::string specialLatinText2 = "abcÄ«abc";
    tokenize(specialLatinText2, datas, false);
    ASSERT_EQ(datas.size(), 2);
    ASSERT_EQ(datas[0], "abc");
    ASSERT_EQ(datas[1], "abc");
    datas.clear();

    std::string specialLatinText3 = "celebrityâ€¦get";
    tokenize(specialLatinText3, datas, false);
    std::vector<std::string> expectedEllipsis = {"celebrity", "get"};
    ASSERT_EQ(datas.size(), expectedEllipsis.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedEllipsis[i]);
    }
    datas.clear();

    // test mixed alphabet word
    std::string mixedAlphabetText1 = "HulyaiÑ€ole";
    tokenize(mixedAlphabetText1, datas, false);
    ASSERT_EQ(datas.size(), 2);
    ASSERT_EQ(datas[0], "hulyai");
    ASSERT_EQ(datas[1], "ole");
    datas.clear();

    std::string mixedAlphabetText2 = "Nisa AÅŸgabat";
    tokenize(mixedAlphabetText2, datas, false);
    std::vector<std::string> expectedName = {"nisa", "gabat"};
    ASSERT_EQ(datas.size(), expectedName.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedName[i]);
    }
    datas.clear();

    // test special connector
    std::string specialConnectorText = "alÙ€ameer";
    tokenize(specialConnectorText, datas, false);
    ASSERT_EQ(datas.size(), 2);
    ASSERT_EQ(datas[0], "al");
    ASSERT_EQ(datas[1], "ameer");
    datas.clear();

    // test rare unicode character
    std::string rareUnicodeText1 = "ğ“š";
    tokenize(rareUnicodeText1, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ğ“š");
    datas.clear();

    std::string rareUnicodeText2 = "ğ‘ª±";
    tokenize(rareUnicodeText2, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ğ‘ª±");
    datas.clear();

    std::string rareUnicodeText3 = "ğ´—";
    tokenize(rareUnicodeText3, datas, false);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "ğ´—");
    datas.clear();
}

TEST_F(IKTokenizerTest, TestCNQuantifierSegmenter) {
    std::vector<TokenInfo> tokenInfos;

    // Test case: "2023å¹´äººæ‰" - core test for the fix
    std::string testText = "2023å¹´äººæ‰";
    getTokensWithType(testText, tokenInfos, false);

    // Verify basic tokens exist
    ASSERT_TRUE(hasToken(tokenInfos, "2023")) << "Should contain token '2023'";
    ASSERT_TRUE(hasToken(tokenInfos, "å¹´")) << "Should contain token 'å¹´'";
    ASSERT_TRUE(hasToken(tokenInfos, "äººæ‰")) << "Should contain token 'äººæ‰'";

    // Core assertion: "äºº" should NOT be segmented as COUNT type separately
    ASSERT_FALSE(hasTokenWithType(tokenInfos, "äºº", Lexeme::Type::Count))
            << "'äºº' should not be segmented as COUNT type when not preceded by number";

    // "å¹´" should be COUNT type
    ASSERT_TRUE(hasTokenWithType(tokenInfos, "å¹´", Lexeme::Type::Count))
            << "'å¹´' should be COUNT type";
}

// Test the exception handling capabilities of the IKTokenizer and AnalyzeContext
TEST_F(IKTokenizerTest, TestExceptionHandling) {
    // Common mock reader class for testing exception handling
    class MockReader : public lucene::util::Reader {
    public:
        enum class ExceptionType { NONE, RUNTIME_ERROR, LENGTH_ERROR, OUT_OF_MEMORY };

        MockReader(ExceptionType type = ExceptionType::NONE, const std::string& errorMsg = "")
                : exceptionType(type), message(errorMsg), returnSize(0) {}

        ~MockReader() override {}

        int32_t read(const void** start, int32_t min, int32_t max) override {
            throwIfNeeded();
            return 0;
        }

        int32_t readCopy(void* start, int32_t off, int32_t len) override {
            throwIfNeeded();

            int32_t safeSize = std::min(returnSize, len);

            if (safeSize > 0) {
                memset(start, 'A', safeSize);
            }

            return safeSize;
        }

        int64_t skip(int64_t ntoskip) override {
            throwIfNeeded();
            return 0;
        }

        int64_t position() override { return 0; }

        size_t size() override { return std::numeric_limits<size_t>::max(); }

        void setReturnSize(int32_t size) { returnSize = size; }

    private:
        ExceptionType exceptionType;
        std::string message;
        int32_t returnSize;

        void throwIfNeeded() {
            switch (exceptionType) {
            case ExceptionType::RUNTIME_ERROR:
                throw std::runtime_error(message.empty() ? "Simulated runtime error" : message);
            case ExceptionType::LENGTH_ERROR:
                throw std::length_error(message.empty() ? "basic_string::_M_create" : message);
            case ExceptionType::OUT_OF_MEMORY:
                throw std::bad_alloc();
            case ExceptionType::NONE:
            default:
                break;
            }
        }
    };

    // PART 1: Test IKTokenizer::reset exception handling
    {
        // Set up analyzer and tokenizer
        IKAnalyzer analyzer;
        analyzer.initDict("./be/dict/ik");

        // Initialize with a valid reader
        lucene::util::SStringReader<char> validReader;
        validReader.init("Test text", 9, false);
        std::unique_ptr<IKTokenizer> tokenizer;
        tokenizer.reset((IKTokenizer*)analyzer.tokenStream(L"", &validReader));

        // Test case 1: Reader throwing runtime error
        MockReader runtimeErrorReader(MockReader::ExceptionType::RUNTIME_ERROR);
        // This may throw different exceptions depending on implementation details
        ASSERT_THROW({ tokenizer->reset(&runtimeErrorReader); }, CLuceneError);

        // Test case 2: Reader throwing length error
        MockReader lengthErrorReader(MockReader::ExceptionType::LENGTH_ERROR);
        ASSERT_THROW({ tokenizer->reset(&lengthErrorReader); }, CLuceneError);

        // Test case 3: Recovery after exception
        lucene::util::SStringReader<char> recoveryReader;
        recoveryReader.init("Recovery text", 13, false);
        ASSERT_NO_THROW({ tokenizer->reset(&recoveryReader); });

        // Verify tokenizer works after recovery
        Token t;
        std::vector<std::string> tokens;
        while (tokenizer->next(&t)) {
            std::string term(t.termBuffer<char>(), t.termLength<char>());
            tokens.emplace_back(term);
        }
        ASSERT_EQ(tokens.size(), 2);
        ASSERT_EQ(tokens[0], "recovery");
        ASSERT_EQ(tokens[1], "text");
    }

    // PART 2: Test AnalyzeContext::fillBuffer exception handling
    {
        // Create AnalyzeContext
        std::shared_ptr<Configuration> config = std::make_shared<Configuration>();
        vectorized::Arena arena {};
        AnalyzeContext context(arena, config);

        // Test case 1: Reader throwing length error
        MockReader lengthErrorReader(MockReader::ExceptionType::LENGTH_ERROR);
        ASSERT_THROW({ context.fillBuffer(&lengthErrorReader); }, CLuceneError);

        // Test case 2: Reader throwing runtime error
        MockReader runtimeErrorReader(MockReader::ExceptionType::RUNTIME_ERROR);
        ASSERT_THROW({ context.fillBuffer(&runtimeErrorReader); }, CLuceneError);

        // Test case 3: Reader simulating memory allocation failure
        MockReader largeDataReader;
        largeDataReader.setReturnSize(std::numeric_limits<int32_t>::max() - 1);
        try {
            context.fillBuffer(&largeDataReader);
            // If no exception is thrown, this is acceptable too
        } catch (const CLuceneError& e) {
            // Verify the exception is properly converted
            ASSERT_TRUE(std::string(e.what()).find("buffer filling") != std::string::npos);
        }

        // Test case 4: Reader returning empty data
        MockReader emptyReader;
        emptyReader.setReturnSize(0);
        ASSERT_EQ(context.fillBuffer(&emptyReader), 0);
    }
}

} // namespace doris::segment_v2
