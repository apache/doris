// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

#include "olap/rowset/segment_v2/inverted_index/analyzer/icu/icu_analyzer.h"

#include <gtest/gtest.h>

#include <memory>
#include <string>
#include <vector>

using namespace lucene::analysis;

namespace doris::segment_v2 {

class ICUTokenizerTest : public ::testing::Test {
protected:
    void tokenize(const std::string& s, std::vector<std::string>& datas) {
        try {
            ICUAnalyzer analyzer;
            analyzer.initDict("./be/dict/icu");
            analyzer.set_lowercase(false);

            lucene::util::SStringReader<char> reader;
            reader.init(s.data(), s.size(), false);

            std::unique_ptr<ICUTokenizer> tokenizer;
            tokenizer.reset((ICUTokenizer*)analyzer.tokenStream(L"", &reader));

            Token t;
            while (tokenizer->next(&t)) {
                std::string term(t.termBuffer<char>(), t.termLength<char>());
                datas.emplace_back(term);
            }
        } catch (CLuceneError& e) {
            std::cout << e.what() << std::endl;
            throw e;
        }
    }
};

TEST_F(ICUTokenizerTest, TestICUTokenizer) {
    std::vector<std::string> datas;

    // Chinese text
    std::string chineseText =
            "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘ä»¬ä¸€èµ·å»å…¬å›­æ•£æ­¥å§ã€‚äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚è¿™æœ¬ä¹¦çš„å†…å®¹éå¸¸æœ‰"
            "è¶£ï¼Œæˆ‘æ¨èç»™ä½ ã€‚";
    tokenize(chineseText, datas);
    ASSERT_EQ(datas.size(), 27);
    datas.clear();

    // English text
    std::string englishText =
            "The quick brown fox jumps over the lazy dog. Artificial intelligence is transforming "
            "various industries. Reading books can significantly enhance your knowledge.";
    tokenize(englishText, datas);
    ASSERT_EQ(datas.size(), 22);
    datas.clear();

    // Vietnamese text
    std::string vietnameseText =
            "HÃ´m nay thá»i tiáº¿t tháº­t Ä‘áº¹p, chÃºng ta cÃ¹ng Ä‘i dáº¡o cÃ´ng viÃªn nhÃ©. TrÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘ang "
            "thay Ä‘á»•i cÃ¡ch sá»‘ng cá»§a chÃºng ta. Cuá»‘n sÃ¡ch nÃ y ráº¥t thÃº vá»‹, tÃ´i muá»‘n giá»›i thiá»‡u cho "
            "báº¡n.";
    tokenize(vietnameseText, datas);
    ASSERT_EQ(datas.size(), 38);
    datas.clear();

    // Portuguese text
    std::string portugueseText =
            "O tempo estÃ¡ Ã³timo hoje, vamos dar um passeio no parque. A inteligÃªncia artificial "
            "estÃ¡ transformando nossas vidas. Este livro Ã© muito interessante, eu recomendo para "
            "vocÃª.";
    tokenize(portugueseText, datas);
    ASSERT_EQ(datas.size(), 27);
    datas.clear();

    // Indonesian text
    std::string indonesianText =
            "Hari ini cuaca sangat bagus, mari kita jalan-jalan ke taman. Kecerdasan buatan sedang "
            "mengubah cara hidup kita. Buku ini sangat menarik, mari kita rekomendasikan.";
    tokenize(indonesianText, datas);
    ASSERT_EQ(datas.size(), 25);
    datas.clear();

    // Spanish text
    std::string spanishText =
            "Hoy hace muy buen tiempo, vamos a pasear por el parque. La inteligencia artificial "
            "estÃ¡ cambiando nuestras vidas. Este libro es muy interesante, te lo recomiendo.";
    tokenize(spanishText, datas);
    ASSERT_EQ(datas.size(), 26);
    datas.clear();

    // Thai text
    std::string thaiText =
            "à¸§à¸±à¸™à¸™à¸µà¹‰à¸­à¸²à¸à¸²à¸¨à¸”à¸µà¸¡à¸²à¸ "
            "à¹€à¸£à¸²à¹„à¸›à¹€à¸”à¸´à¸™à¹€à¸¥à¹ˆà¸™à¸—à¸µà¹ˆà¸ªà¸§à¸™à¸ªà¸²à¸˜à¸²à¸£à¸“à¸°à¸à¸±à¸™à¹€à¸–à¸­à¸°à¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œà¸à¸³à¸¥à¸±à¸‡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸§à¸´à¸–à¸µà¸Šà¸µà¸§à¸´à¸•à¸‚à¸­à¸‡à¹€à¸£à¸²à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­à¹€à¸¥à¹ˆà¸¡à¸™à¸µà¹‰à¸™à¹ˆà¸²à¸ªà¸™à¹ƒà¸ˆà¸¡à¸²à¸ "
            "à¸‰à¸±à¸™à¸­à¸¢à¸²à¸à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¸„à¸¸à¸“à¸­à¹ˆà¸²à¸™";
    tokenize(thaiText, datas);
    ASSERT_EQ(datas.size(), 34);
    datas.clear();

    // Hindi text
    std::string hindiText =
            "à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ, à¤šà¤²à¥‹ à¤ªà¤¾à¤°à¥à¤• à¤®à¥‡à¤‚ à¤Ÿà¤¹à¤²à¤¨à¥‡ à¤šà¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤¹à¤®à¤¾à¤°à¥‡ à¤œà¥€à¤µà¤¨ à¤•à¥‹ à¤¬à¤¦à¤² à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤ à¤¯à¤¹ "
            "à¤•à¤¿à¤¤à¤¾à¤¬ à¤¬à¤¹à¥à¤¤ à¤¦à¤¿à¤²à¤šà¤¸à¥à¤ª à¤¹à¥ˆ, à¤®à¥ˆà¤‚ à¤‡à¤¸à¥‡ à¤†à¤ªà¤•à¥‹ à¤¸à¥à¤à¤¾à¤¤à¤¾ à¤¹à¥‚à¤‚à¥¤";
    tokenize(hindiText, datas);
    ASSERT_EQ(datas.size(), 29);
    datas.clear();
}

TEST_F(ICUTokenizerTest, TestICUTokenizerEmptyText) {
    std::vector<std::string> datas;
    std::string emptyText;
    tokenize(emptyText, datas);
    ASSERT_EQ(datas.size(), 0);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerSingleWord) {
    std::vector<std::string> datas;

    // Chinese word
    std::string chineseText = "å¤©æ°”";
    tokenize(chineseText, datas);
    ASSERT_EQ(datas.size(), 1);
    datas.clear();

    // English word
    std::string englishText = "weather";
    tokenize(englishText, datas);
    ASSERT_EQ(datas.size(), 1);
    datas.clear();

    // Arabic word
    std::string arabicText = "Ø§Ù„Ø°ÙƒØ§Ø¡";
    tokenize(arabicText, datas);
    ASSERT_EQ(datas.size(), 1);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerMultipleSpaces) {
    std::vector<std::string> datas;
    std::string multipleSpacesText = "The    quick    brown   fox";
    tokenize(multipleSpacesText, datas);
    ASSERT_EQ(datas.size(), 4);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerPunctuation) {
    std::vector<std::string> datas;
    std::string textWithPunctuation = "Hello, world! How's it going?";
    tokenize(textWithPunctuation, datas);
    ASSERT_EQ(datas.size(), 5);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerMixedLanguage) {
    std::vector<std::string> datas;
    std::string mixedText = "Hello, ä»Šå¤©å¤©æ°”çœŸå¥½!";
    tokenize(mixedText, datas);
    ASSERT_EQ(datas.size(), 4);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerUnicode) {
    std::vector<std::string> datas;
    std::string unicodeText = "ä½ å¥½ï¼Œä¸–ç•Œ! ğŸ˜ŠğŸŒ";
    tokenize(unicodeText, datas);
    ASSERT_EQ(datas.size(), 4);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerNumericText) {
    std::vector<std::string> datas;
    std::string numericText = "The price is 100 dollars.";
    tokenize(numericText, datas);
    ASSERT_EQ(datas.size(), 5);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerLongText) {
    std::vector<std::string> datas;
    std::string longText =
            "Artificial intelligence is rapidly changing various industries around the world. "
            "From healthcare to finance, it is transforming the way we work, live, and interact "
            "with technology.";
    tokenize(longText, datas);
    ASSERT_EQ(datas.size(), 26);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerSpecialCharacters) {
    std::vector<std::string> datas;
    std::string specialCharsText = "@#$%^&*()_+{}[]|:;\"'<>,.?/~`";
    tokenize(specialCharsText, datas);
    ASSERT_EQ(datas.size(), 0);
}

TEST_F(ICUTokenizerTest, TestICUTokenizerLongWords) {
    std::vector<std::string> datas;
    std::string longWordText = "hippopotomonstrosesquipedaliophobia";
    tokenize(longWordText, datas);
    ASSERT_EQ(datas.size(), 1);
}

TEST_F(ICUTokenizerTest, TestICUArmenian) {
    std::vector<std::string> datas;
    std::string longWordText =
            "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« 13 Õ´Õ«Õ¬Õ«Õ¸Õ¶ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ (4,600` Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶ Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´) Õ£Ö€Õ¾Õ¥Õ¬ Õ¥Õ¶ Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ« "
            "Õ¯Õ¸Õ²Õ´Õ«Ö Õ¸Ö‚ Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡ Õ¢Õ¸Õ¬Õ¸Ö€ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬ ÖÕ¡Õ¶Õ¯Õ¡Ö Õ´Õ¡Ö€Õ¤ Õ¸Õ¾ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ¢Õ¡ÖÕ¥Õ¬ "
            "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« Õ¯Õ¡ÕµÖ„Õ¨Ö‰";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {
            "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«",   "13",    "Õ´Õ«Õ¬Õ«Õ¸Õ¶",     "Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨",  "4,600",  "Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶",
            "Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´", "Õ£Ö€Õ¾Õ¥Õ¬", "Õ¥Õ¶",         "Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ«", "Õ¯Õ¸Õ²Õ´Õ«Ö", "Õ¸Ö‚",
            "Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡",       "Õ¢Õ¸Õ¬Õ¸Ö€", "Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨", "Õ¯Õ¡Ö€Õ¸Õ²",       "Õ§",      "Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬",
            "ÖÕ¡Õ¶Õ¯Õ¡Ö",        "Õ´Õ¡Ö€Õ¤",  "Õ¸Õ¾",         "Õ¯Õ¡Ö€Õ¸Õ²",       "Õ§",      "Õ¢Õ¡ÖÕ¥Õ¬",
            "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«",   "Õ¯Õ¡ÕµÖ„Õ¨"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUAmharic) {
    std::vector<std::string> datas;
    std::string longWordText = "á‹ŠáŠªá”á‹µá‹« á‹¨á‰£áˆˆ á‰¥á‹™ á‰‹áŠ•á‰‹ á‹¨á‰°áˆŸáˆ‹ á‰µáŠ­áŠ­áˆˆáŠ›áŠ“ áŠáŒ» áˆ˜á‹áŒˆá‰  á‹•á‹á‰€á‰µ (áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«) áŠá‹á¢ áˆ›áŠ•áŠ›á‹áˆ";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"á‹ŠáŠªá”á‹µá‹«", "á‹¨á‰£áˆˆ",  "á‰¥á‹™",   "á‰‹áŠ•á‰‹",       "á‹¨á‰°áˆŸáˆ‹", "á‰µáŠ­áŠ­áˆˆáŠ›áŠ“",
                                       "áŠáŒ»",    "áˆ˜á‹áŒˆá‰ ", "á‹•á‹á‰€á‰µ", "áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«", "áŠá‹",   "áˆ›áŠ•áŠ›á‹áˆ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUArabic) {
    std::vector<std::string> datas;
    std::string longWordText =
            "Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙŠØ³Ù…Ù‰ \"Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…: Ù‚ØµØ© ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§\" "
            "(Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Truth in Numbers: The Wikipedia Story)ØŒ Ø³ÙŠØªÙ… Ø¥Ø·Ù„Ø§Ù‚Ù‡ ÙÙŠ 2008.";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {
            "Ø§Ù„ÙÙŠÙ„Ù…",   "Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ",  "Ø§Ù„Ø£ÙˆÙ„",     "Ø¹Ù†",          "ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§", "ÙŠØ³Ù…Ù‰", "Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©",
            "Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…", "Ù‚ØµØ©",       "ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§", "Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", "Truth",     "in",   "Numbers",
            "The",      "Wikipedia", "Story",     "Ø³ÙŠØªÙ…",        "Ø¥Ø·Ù„Ø§Ù‚Ù‡",    "ÙÙŠ",   "2008"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUAramaic) {
    std::vector<std::string> datas;
    std::string longWordText =
            "Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ (ÜÜ¢Ü“Ü ÜÜ: Wikipedia) Ü—Ü˜ ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ ÜšÜÜªÜ¬Ü Ü•ÜÜ¢Ü›ÜªÜ¢Ü› Ü’Ü Ü«Ü¢ÌˆÜ Ü£Ü“ÜÜÌˆÜÜ‚ Ü«Ü¡Ü— ÜÜ¬Ü Ü¡Ü¢ "
            "Ü¡ÌˆÜ Ü¬Ü Ü•\"Ü˜ÜÜ©Ü\" Ü˜\"ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ\"Ü€";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {
            "Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ", "ÜÜ¢Ü“Ü ÜÜ", "Wikipedia", "Ü—Ü˜",  "ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ", "ÜšÜÜªÜ¬Ü",
            "Ü•ÜÜ¢Ü›ÜªÜ¢Ü›",  "Ü’Ü Ü«Ü¢ÌˆÜ",  "Ü£Ü“ÜÜÌˆÜ",     "Ü«Ü¡Ü—", "ÜÜ¬Ü",         "Ü¡Ü¢",
            "Ü¡ÌˆÜ Ü¬Ü",     "Ü•",      "Ü˜ÜÜ©Ü",      "Ü˜",   "ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUBengali) {
    std::vector<std::string> datas;
    std::string longWordText =
            "à¦à¦‡ à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦· à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾ à¦•à¦°à§‡ à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨ (à¦à¦•à¦Ÿà¦¿ à¦…à¦²à¦¾à¦­à¦œà¦¨à¦• à¦¸à¦‚à¦¸à§à¦¥à¦¾)à¥¤ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦° à¦¶à§à¦°à§ à§§à§« "
            "à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿, à§¨à§¦à§¦à§§ à¦¸à¦¾à¦²à§‡à¥¤ à¦à¦–à¦¨ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“ à¦¬à§‡à¦¶à§€ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤Ü€";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"à¦à¦‡",         "à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦·", "à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾", "à¦•à¦°à§‡",   "à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾",
                                       "à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨",   "à¦à¦•à¦Ÿà¦¿",    "à¦…à¦²à¦¾à¦­à¦œà¦¨à¦•",  "à¦¸à¦‚à¦¸à§à¦¥à¦¾", "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦°",
                                       "à¦¶à§à¦°à§",         "à§§à§«",      "à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿",  "à§¨à§¦à§¦à§§",  "à¦¸à¦¾à¦²à§‡",
                                       "à¦à¦–à¦¨",        "à¦ªà¦°à§à¦¯à¦¨à§à¦¤",   "à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“",  "à¦¬à§‡à¦¶à§€",  "à¦­à¦¾à¦·à¦¾à¦¯à¦¼",
                                       "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾", "à¦°à¦¯à¦¼à§‡à¦›à§‡"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUFarsi) {
    std::vector<std::string> datas;
    std::string longWordText =
            "ÙˆÛŒÚ©ÛŒ Ù¾Ø¯ÛŒØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Û²Ûµ Ø¯ÛŒ Û±Û³Û·Û¹ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ú©Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù” ØªØ®ØµØµÛŒ Ù†ÙˆÙ¾Ø¯ÛŒØ§ Ù†ÙˆØ´ØªÙ‡ "
            "Ø´Ø¯.";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"ÙˆÛŒÚ©ÛŒ",     "Ù¾Ø¯ÛŒØ§ÛŒ", "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ", "Ø¯Ø±",    "ØªØ§Ø±ÛŒØ®", "Û²Ûµ",
                                       "Ø¯ÛŒ",       "Û±Û³Û·Û¹",  "Ø¨Ù‡",      "ØµÙˆØ±Øª",  "Ù…Ú©Ù…Ù„ÛŒ", "Ø¨Ø±Ø§ÛŒ",
                                       "Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù”", "ØªØ®ØµØµÛŒ", "Ù†ÙˆÙ¾Ø¯ÛŒØ§",  "Ù†ÙˆØ´ØªÙ‡", "Ø´Ø¯"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUGreek) {
    std::vector<std::string> datas;
    std::string longWordText =
            "Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î± Î±Ï€ÏŒ ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚ Î¼Îµ Ï„Î¿ Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ wiki, ÎºÎ¬Ï„Î¹ Ï€Î¿Ï… ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ "
            "Î¬ÏÎ¸ÏÎ± Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½ Î® Î½Î± Î±Î»Î»Î¬Î¾Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿Î½ ÎºÎ±Î¸Î­Î½Î±.";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹", "ÏƒÎµ",         "ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î±", "Î±Ï€ÏŒ",   "ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚",
                                       "Î¼Îµ",       "Ï„Î¿",         "Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ",  "wiki",  "ÎºÎ¬Ï„Î¹",
                                       "Ï€Î¿Ï…",      "ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹",   "ÏŒÏ„Î¹",        "Î¬ÏÎ¸ÏÎ±", "Î¼Ï€Î¿ÏÎµÎ¯",
                                       "Î½Î±",       "Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½", "Î®",          "Î½Î±",    "Î±Î»Î»Î¬Î¾Î¿Ï…Î½",
                                       "Î±Ï€ÏŒ",      "Ï„Î¿Î½",        "ÎºÎ±Î¸Î­Î½Î±"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUKhmer) {
    std::vector<std::string> datas;
    std::string longWordText = "á•áŸ’á‘áŸ‡áŸáŸ’á€á¹á˜áŸáŸ’á€áŸƒá”á¸á”á½á“ááŸ’á“á„á“áŸáŸ‡";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"á•áŸ’á‘áŸ‡", "áŸáŸ’á€á¹á˜áŸáŸ’á€áŸƒ", "á”á¸", "á”á½á“", "ááŸ’á“á„", "á“áŸáŸ‡"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICULao) {
    std::vector<std::string> datas;
    std::string longWordText = "àºàº§à»ˆàº²àº”àº­àº àºàº²àºªàº²àº¥àº²àº§";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"àºàº§à»ˆàº²", "àº”àº­àº", "àºàº²àºªàº²", "àº¥àº²àº§"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUMyanmar) {
    std::vector<std::string> datas;
    std::string longWordText = "á€á€€á€ºá€á€„á€ºá€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸á€…á€±á€•á€¼á€®á€¸";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"á€á€€á€ºá€á€„á€º", "á€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸", "á€…á€±", "á€•á€¼á€®á€¸"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUThai) {
    std::vector<std::string> datas;
    std::string longWordText = "à¸à¸²à¸£à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸•à¹‰à¸­à¸‡à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸²à¸‡à¸²à¸™à¸”à¸µ. à¹à¸¥à¹‰à¸§à¹€à¸˜à¸­à¸ˆà¸°à¹„à¸›à¹„à¸«à¸™? à¹‘à¹’à¹“à¹”";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"à¸à¸²à¸£", "à¸—à¸µà¹ˆ",   "à¹„à¸”à¹‰",  "à¸•à¹‰à¸­à¸‡", "à¹à¸ªà¸”à¸‡", "à¸§à¹ˆà¸²",  "à¸‡à¸²à¸™",
                                       "à¸”à¸µ",   "à¹à¸¥à¹‰à¸§", "à¹€à¸˜à¸­", "à¸ˆà¸°",  "à¹„à¸›",   "à¹„à¸«à¸™", "à¹‘à¹’à¹“à¹”"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUTibetan) {
    std::vector<std::string> datas;
    std::string longWordText = "à½¦à¾£à½¼à½“à¼‹à½˜à½›à½¼à½‘à¼‹à½‘à½„à¼‹à½£à½¦à¼‹à½ à½‘à½²à½¦à¼‹à½–à½¼à½‘à¼‹à½¡à½²à½‚à¼‹à½˜à½²à¼‹à½‰à½˜à½¦à¼‹à½‚à½¼à½„à¼‹à½ à½•à½ºà½£à¼‹à½‘à½´à¼‹à½‚à½à½¼à½„à¼‹à½–à½¢à¼‹à½§à¼‹à½…à½„à¼‹à½‘à½‚à½ºà¼‹à½˜à½šà½“à¼‹à½˜à½†à½²à½¦à¼‹à½¦à½¼à¼ à¼";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"à½¦à¾£à½¼à½“", "à½˜à½›à½¼à½‘", "à½‘à½„", "à½£à½¦",  "à½ à½‘à½²à½¦", "à½–à½¼à½‘",  "à½¡à½²à½‚",
                                       "à½˜à½²",  "à½‰à½˜à½¦", "à½‚à½¼à½„", "à½ à½•à½ºà½£", "à½‘à½´",   "à½‚à½à½¼à½„", "à½–à½¢",
                                       "à½§",  "à½…à½„",  "à½‘à½‚à½º", "à½˜à½šà½“", "à½˜à½†à½²à½¦", "à½¦à½¼"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUChinese) {
    std::vector<std::string> datas;
    std::string longWordText = "æˆ‘æ˜¯ä¸­å›½äººã€‚ ï¼‘ï¼’ï¼“ï¼” ï¼´ï½…ï½“ï½”ï½“ ";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"æˆ‘æ˜¯", "ä¸­å›½äºº", "ï¼‘ï¼’ï¼“ï¼”", "ï¼´ï½…ï½“ï½”ï½“"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUHebrew) {
    {
        std::vector<std::string> datas;
        std::string longWordText = "×“× ×§× ×¨ ×ª×§×£ ××ª ×”×“×•\"×—";
        tokenize(longWordText, datas);
        std::vector<std::string> result = {"×“× ×§× ×¨", "×ª×§×£", "××ª", "×”×“×•\"×—"};
        for (size_t i = 0; i < datas.size(); i++) {
            ASSERT_EQ(datas[i], result[i]);
        }
    }
    {
        std::vector<std::string> datas;
        std::string longWordText = "×—×‘×¨×ª ×‘×ª ×©×œ ××•×“×™'×¡";
        tokenize(longWordText, datas);
        std::vector<std::string> result = {"×—×‘×¨×ª", "×‘×ª", "×©×œ", "××•×“×™'×¡"};
        for (size_t i = 0; i < datas.size(); i++) {
            ASSERT_EQ(datas[i], result[i]);
        }
    }
}

TEST_F(ICUTokenizerTest, TestICUEmpty) {
    std::vector<std::string> datas;
    std::string longWordText = " . ";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICULUCENE1545) {
    std::vector<std::string> datas;
    std::string longWordText = "moÍ¤chte";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"moÍ¤chte"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUAlphanumericSA) {
    std::vector<std::string> datas;
    std::string longWordText = "B2B 2B";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"B2B", "2B"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUDelimitersSA) {
    std::vector<std::string> datas;
    std::string longWordText = "some-dashed-phrase dogs,chase,cats ac/dc";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"some",  "dashed", "phrase", "dogs",
                                       "chase", "cats",   "ac",     "dc"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUApostrophesSA) {
    std::vector<std::string> datas;
    std::string longWordText = "O'Reilly you're she's Jim's don't O'Reilly's";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"O'Reilly", "you're", "she's",
                                       "Jim's",    "don't",  "O'Reilly's"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUNumericSA) {
    std::vector<std::string> datas;
    std::string longWordText = "21.35 R2D2 C3PO 216.239.63.104 216.239.63.104";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"21.35", "R2D2", "C3PO", "216.239.63.104", "216.239.63.104"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUTextWithNumbersSA) {
    std::vector<std::string> datas;
    std::string longWordText = "David has 5000 bones";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"David", "has", "5000", "bones"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUVariousTextSA) {
    std::vector<std::string> datas;
    std::string longWordText =
            "C embedded developers wanted foo bar FOO BAR foo      bar .  FOO <> BAR \"QUOTED\" "
            "word";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"C",   "embedded", "developers", "wanted", "foo",
                                       "bar", "FOO",      "BAR",        "foo",    "bar",
                                       "FOO", "BAR",      "QUOTED",     "word"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUKoreanSA) {
    std::vector<std::string> datas;
    std::string longWordText = "ì•ˆë…•í•˜ì„¸ìš” í•œê¸€ì…ë‹ˆë‹¤";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"ì•ˆë…•í•˜ì„¸ìš”", "í•œê¸€ì…ë‹ˆë‹¤"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUReusableTokenStream) {
    std::vector<std::string> datas;
    std::string longWordText = "à½¦à¾£à½¼à½“à¼‹à½˜à½›à½¼à½‘à¼‹à½‘à½„à¼‹à½£à½¦à¼‹à½ à½‘à½²à½¦à¼‹à½–à½¼à½‘à¼‹à½¡à½²à½‚à¼‹à½˜à½²à¼‹à½‰à½˜à½¦à¼‹à½‚à½¼à½„à¼‹à½ à½•à½ºà½£à¼‹à½‘à½´à¼‹à½‚à½à½¼à½„à¼‹à½–à½¢à¼‹à½§à¼‹à½…à½„à¼‹à½‘à½‚à½ºà¼‹à½˜à½šà½“à¼‹à½˜à½†à½²à½¦à¼‹à½¦à½¼à¼ à¼";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"à½¦à¾£à½¼à½“", "à½˜à½›à½¼à½‘", "à½‘à½„", "à½£à½¦",  "à½ à½‘à½²à½¦", "à½–à½¼à½‘",  "à½¡à½²à½‚",
                                       "à½˜à½²",  "à½‰à½˜à½¦", "à½‚à½¼à½„", "à½ à½•à½ºà½£", "à½‘à½´",   "à½‚à½à½¼à½„", "à½–à½¢",
                                       "à½§",  "à½…à½„",  "à½‘à½‚à½º", "à½˜à½šà½“", "à½˜à½†à½²à½¦", "à½¦à½¼"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUOffsets) {
    std::vector<std::string> datas;
    std::string longWordText = "David has 5000 bones";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"David", "has", "5000", "bones"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUKorean) {
    std::vector<std::string> datas;
    std::string longWordText = "í›ˆë¯¼ì •ìŒ";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"í›ˆë¯¼ì •ìŒ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUJapanese) {
    std::vector<std::string> datas;
    std::string longWordText = "ä»®åé£ã„ ã‚«ã‚¿ã‚«ãƒŠ";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"ä»®åé£ã„", "ã‚«ã‚¿ã‚«ãƒŠ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUEmoji) {
    std::vector<std::string> datas;
    std::string longWordText =
            "ğŸ’© ğŸ’©ğŸ’© ğŸ‘©â€â¤ï¸â€ğŸ‘© ğŸ‘¨ğŸ¼â€âš•ï¸ ğŸ‡ºğŸ‡¸ğŸ‡ºğŸ‡¸ #ï¸âƒ£ 3ï¸âƒ£ "
            "ğŸ´";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {
            "ğŸ’©", "ğŸ’©", "ğŸ’©", "ğŸ‘©â€â¤ï¸â€ğŸ‘©", "ğŸ‘¨ğŸ¼â€âš•ï¸", "ğŸ‡ºğŸ‡¸", "ğŸ‡ºğŸ‡¸",
            "#ï¸âƒ£",  "3ï¸âƒ£",  "ğŸ´"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUEmojiTokenization) {
    std::vector<std::string> datas;
    std::string longWordText = "pooğŸ’©poo ğŸ’©ä¸­åœ‹ğŸ’©";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"poo", "ğŸ’©", "poo", "ğŸ’©", "ä¸­åœ‹", "ğŸ’©"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(ICUTokenizerTest, TestICUScriptExtensions) {
    std::vector<std::string> datas;
    std::string longWordText = "ğ‘…—à¥¦ ğ‘…—à¤¾ ğ‘…—áª¾";
    tokenize(longWordText, datas);
    std::vector<std::string> result = {"ğ‘…—à¥¦", "ğ‘…—à¤¾", "ğ‘…—áª¾"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

} // namespace doris::segment_v2
