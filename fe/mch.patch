diff --git a/.asf.yaml b/.asf.yaml
index cbce60f0a..4e11b2140 100644
--- a/.asf.yaml
+++ b/.asf.yaml
@@ -19,24 +19,30 @@ github:
   description: Apache Doris is an easy-to-use, high performance and unified analytics database. 
   homepage: https://doris.apache.org
   labels:
-    - data-warehousing
-    - mpp
-    - olap
-    - dbms
-    - database
-    - sql
     - big-data
     - real-time
-    - analytics
-    - distributed-database
-    - iceberg
-    - hudi
+    - realtime-analytics
+    - query-engine
+    - olap
+    - ad-hoc
+    - mpp
     - datalake
+    - lakehouse
+    - etl
+    - elt
+    - sql
+    - snowflake
+    - redshift
+    - bigquery
     - hive
+    - iceberg
+    - hudi
+    - delta-lake
+    - spark
     - hadoop
-    - tpch
-    - ssb
+    - dbt
     - vectorized
+    - database
 
   enabled_merge_buttons:
     squash:  true
diff --git a/.clang-format-ignore b/.clang-format-ignore
index 9275d0e00..b92688b82 100644
--- a/.clang-format-ignore
+++ b/.clang-format-ignore
@@ -7,3 +7,7 @@ be/src/util/sse2neon.h
 be/src/util/mustache/mustache.h
 be/src/util/mustache/mustache.cc
 be/src/util/utf8_check.cpp
+be/src/util/cityhash102/city.h
+be/src/util/cityhash102/city.cc
+be/src/util/cityhash102/citycrc.h
+be/src/util/cityhash102/config.h
diff --git a/.licenserc.yaml b/.licenserc.yaml
index d458e4526..8e02ad6e8 100644
--- a/.licenserc.yaml
+++ b/.licenserc.yaml
@@ -52,6 +52,7 @@ header:
     - "be/src/util/jsonb_document.h"
     - "be/src/util/jsonb_error.h"
     - "be/src/util/jsonb_parser.h"
+    - "be/src/util/jsonb_parser_simd.h"
     - "be/src/util/jsonb_stream.h"
     - "be/src/util/jsonb_updater.h"
     - "be/src/util/jsonb_utils.h"
@@ -63,6 +64,7 @@ header:
     - "be/src/util/sse2neo.h"
     - "be/src/util/sse2neon.h"
     - "be/src/util/utf8_check.cpp"
+    - "be/src/util/cityhash102"
     - "build-support/run_clang_format.py"
     - "regression-test/data"
     - "docs/.vuepress/public/css/animate.min.css"
diff --git a/README.md b/README.md
index 88c5114b1..b7a33dc9f 100644
--- a/README.md
+++ b/README.md
@@ -39,6 +39,7 @@ All this makes Apache Doris an ideal tool for scenarios including report analysi
 
 
 üéâ Version 1.2.0 released now! It is fully evolved release and all users are encouraged to upgrade to this release. Check out the üîó[Release Notes](https://doris.apache.org/docs/releasenotes/release-1.2.0) here. 
+üéâ Version 1.1.5 released now. It is a LTS(Long-term Suopport) release based on version 1.1. Check out the üîó[Release Notes](https://doris.apache.org/docs/dev/releasenotes/release-1.1.5) here. 
 
 üëÄ Have a look at the üîó[Official Website](https://doris.apache.org/) for a comprehensive list of Apache Doris's core features, blogs and user cases.
 
@@ -138,7 +139,7 @@ In terms of optimizers, Doris uses a combination of CBO and RBO. RBO supports co
 
 **Apache Doris has graduated from Apache incubator successfully and become a Top-Level Project in June 2022**. 
 
-Currently, the Apache Doris community has gathered more than 350 contributors from nearly 100 companies in different industries, and the number of active contributors is close to 100 per month.
+Currently, the Apache Doris community has gathered more than 400 contributors from nearly 200 companies in different industries, and the number of active contributors is close to 100 per month.
 
 
 [![Monthly Active Contributors](https://contributor-overtime-api.apiseven.com/contributors-svg?chart=contributorMonthlyActivity&repo=apache/doris)](https://www.apiseven.com/en/contributor-graph?chart=contributorMonthlyActivity&repo=apache/doris)
@@ -149,7 +150,7 @@ We deeply appreciate üîó[community contributors](https://github.com/apache/dori
 
 ## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Users
 
-Apache Doris now has a wide user base in China and around the world, and as of today, **Apache Doris is used in production environments in over 1000 companies worldwide.** More than 80% of the top 50 Internet companies in China in terms of market capitalization or valuation have been using Apache Doris for a long time, including Baidu, Meituan, Xiaomi, Jingdong, Bytedance, Tencent, NetEase, Kwai, Weibo, and Ke Holdings. It is also widely used in some traditional industries such as finance, energy, manufacturing, and telecommunications.
+Apache Doris now has a wide user base in China and around the world, and as of today, **Apache Doris is used in production environments in thousands of companies worldwide.** More than 80% of the top 50 Internet companies in China in terms of market capitalization or valuation have been using Apache Doris for a long time, including Baidu, Meituan, Xiaomi, Jingdong, Bytedance, Tencent, NetEase, Kwai, Sina, 360, Mihoyo, and Ke Holdings. It is also widely used in some traditional industries such as finance, energy, manufacturing, and telecommunications.
 
 The users of Apache Doris: üîó[https://doris.apache.org/users](https://doris.apache.org/users)
 
diff --git a/be/CMakeLists.txt b/be/CMakeLists.txt
index c85f87e8a..dfa097223 100644
--- a/be/CMakeLists.txt
+++ b/be/CMakeLists.txt
@@ -510,6 +510,8 @@ if ("${CMAKE_BUILD_TARGET_ARCH}" STREQUAL "x86" OR "${CMAKE_BUILD_TARGET_ARCH}"
     if (USE_AVX2)
         set(CXX_COMMON_FLAGS "${CXX_COMMON_FLAGS} -mavx2")
     endif()
+    # set -mlzcnt for leading zero count used by simdjson
+    set(CXX_COMMON_FLAGS "${CXX_COMMON_FLAGS} -msse4.2")
 endif()
 set(CXX_COMMON_FLAGS "${CXX_COMMON_FLAGS} -Wno-attributes -DS2_USE_GFLAGS -DS2_USE_GLOG")
 
diff --git a/be/src/common/config.h b/be/src/common/config.h
index debcdb67d..056ea2ab7 100644
--- a/be/src/common/config.h
+++ b/be/src/common/config.h
@@ -448,6 +448,8 @@ CONF_Bool(enable_quadratic_probing, "false");
 
 // for pprof
 CONF_String(pprof_profile_dir, "${DORIS_HOME}/log");
+// for jeprofile in jemalloc
+CONF_mString(jeprofile_dir, "${DORIS_HOME}/log");
 
 // to forward compatibility, will be removed later
 CONF_mBool(enable_token_check, "true");
@@ -826,6 +828,7 @@ CONF_Int32(s3_transfer_executor_pool_size, "2");
 CONF_Bool(enable_time_lut, "true");
 CONF_Bool(enable_simdjson_reader, "false");
 
+CONF_mBool(enable_query_like_bloom_filter, "true");
 // number of s3 scanner thread pool size
 CONF_Int32(doris_remote_scanner_thread_pool_thread_num, "16");
 // number of s3 scanner thread pool queue size
@@ -867,6 +870,10 @@ CONF_Bool(enable_fuzzy_mode, "false");
 
 CONF_Int32(pipeline_executor_size, "0");
 
+// Temp config. True to use optimization for bitmap_index apply predicate except leaf node of the and node.
+// Will remove after fully test.
+CONF_Bool(enable_index_apply_preds_except_leafnode_of_andnode, "false");
+
 #ifdef BE_TEST
 // test s3
 CONF_String(test_s3_resource, "resource");
diff --git a/be/src/exec/olap_common.h b/be/src/exec/olap_common.h
index 605ab9dda..8f1950276 100644
--- a/be/src/exec/olap_common.h
+++ b/be/src/exec/olap_common.h
@@ -96,6 +96,10 @@ public:
 
     Status add_range(SQLFilterOp op, CppType value);
 
+    Status add_compound_value(SQLFilterOp op, CppType value);
+
+    bool is_in_compound_value_range() const;
+
     bool is_fixed_value_range() const;
 
     bool is_scope_value_range() const;
@@ -239,6 +243,31 @@ public:
         }
     }
 
+    void to_condition_in_compound(std::vector<TCondition>& filters) {
+        for (const auto& value : _compound_values) {
+            TCondition condition;
+            condition.__set_column_name(_column_name);
+            if (value.first == FILTER_LARGER) {
+                condition.__set_condition_op(">>");
+            } else if (value.first == FILTER_LARGER_OR_EQUAL) {
+                condition.__set_condition_op(">=");
+            } else if (value.first == FILTER_LESS) {
+                condition.__set_condition_op("<<");
+            } else if (value.first == FILTER_LESS_OR_EQUAL) {
+                condition.__set_condition_op("<=");
+            } else if (value.first == FILTER_IN) {
+                condition.__set_condition_op("*=");
+            } else if (value.first == FILTER_NOT_IN) {
+                condition.__set_condition_op("!*=");
+            }
+            condition.condition_values.push_back(
+                    cast_to_string<primitive_type, CppType>(value.second, _scale));
+            if (condition.condition_values.size() != 0) {
+                filters.push_back(condition);
+            }
+        }
+    }
+
     void set_whole_value_range() {
         _fixed_values.clear();
         _low_value = TYPE_MIN;
@@ -286,6 +315,11 @@ public:
         range.add_range(op, *value);
     }
 
+    static void add_compound_value_range(ColumnValueRange<primitive_type>& range, SQLFilterOp op,
+                                         CppType* value) {
+        range.add_compound_value(op, *value);
+    }
+
     static ColumnValueRange<primitive_type> create_empty_column_value_range() {
         return ColumnValueRange<primitive_type>::create_empty_column_value_range("");
     }
@@ -335,6 +369,9 @@ private:
                                                   primitive_type == PrimitiveType::TYPE_BOOLEAN ||
                                                   primitive_type == PrimitiveType::TYPE_DATETIME ||
                                                   primitive_type == PrimitiveType::TYPE_DATETIMEV2;
+
+    // range value except leaf node of and node in compound expr tree
+    std::set<std::pair<SQLFilterOp, CppType>> _compound_values;
 };
 
 class OlapScanKeys {
@@ -476,6 +513,18 @@ Status ColumnValueRange<primitive_type>::add_fixed_value(const CppType& value) {
     return Status::OK();
 }
 
+template <PrimitiveType primitive_type>
+Status ColumnValueRange<primitive_type>::add_compound_value(SQLFilterOp op, CppType value) {
+    std::pair<SQLFilterOp, CppType> val_with_op(op, value);
+    _compound_values.insert(val_with_op);
+    _contain_null = false;
+
+    _high_value = TYPE_MIN;
+    _low_value = TYPE_MAX;
+
+    return Status::OK();
+}
+
 template <PrimitiveType primitive_type>
 void ColumnValueRange<primitive_type>::remove_fixed_value(const CppType& value) {
     _fixed_values.erase(value);
@@ -486,6 +535,11 @@ bool ColumnValueRange<primitive_type>::is_fixed_value_range() const {
     return _fixed_values.size() != 0;
 }
 
+template <PrimitiveType primitive_type>
+bool ColumnValueRange<primitive_type>::is_in_compound_value_range() const {
+    return _compound_values.size() != 0;
+}
+
 template <PrimitiveType primitive_type>
 bool ColumnValueRange<primitive_type>::is_scope_value_range() const {
     return _high_value > _low_value;
@@ -497,7 +551,7 @@ bool ColumnValueRange<primitive_type>::is_empty_value_range() const {
         return true;
     }
 
-    return !is_fixed_value_range() && !is_scope_value_range() && !contain_null();
+    return (!is_fixed_value_range() && !is_scope_value_range() && !contain_null());
 }
 
 template <PrimitiveType primitive_type>
diff --git a/be/src/http/CMakeLists.txt b/be/src/http/CMakeLists.txt
index a23d67939..a2e1c3eb4 100644
--- a/be/src/http/CMakeLists.txt
+++ b/be/src/http/CMakeLists.txt
@@ -35,6 +35,7 @@ add_library(Webserver STATIC
   http_client.cpp
   action/download_action.cpp
   action/monitor_action.cpp
+  action/pad_rowset_action.cpp
   action/health_action.cpp
   action/tablet_migration_action.cpp
   action/tablets_info_action.cpp
@@ -54,4 +55,5 @@ add_library(Webserver STATIC
   action/reset_rpc_channel_action.cpp
   action/check_tablet_segment_action.cpp
   action/version_action.cpp
+  action/jeprofile_actions.cpp
 )
diff --git a/be/src/http/action/jeprofile_actions.cpp b/be/src/http/action/jeprofile_actions.cpp
new file mode 100644
index 000000000..27c3d0796
--- /dev/null
+++ b/be/src/http/action/jeprofile_actions.cpp
@@ -0,0 +1,81 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "http/action/jeprofile_actions.h"
+
+#include <jemalloc/jemalloc.h>
+
+#include <ctime>
+#include <fstream>
+#include <mutex>
+#include <sstream>
+
+#include "common/config.h"
+#include "common/object_pool.h"
+#include "http/ev_http_server.h"
+#include "http/http_channel.h"
+#include "http/http_handler.h"
+#include "http/http_headers.h"
+#include "http/http_request.h"
+#include "util/file_utils.h"
+
+namespace doris {
+
+static std::mutex kJeprofileActionMutex;
+class JeHeapAction : public HttpHandler {
+public:
+    JeHeapAction() = default;
+    virtual ~JeHeapAction() = default;
+
+    virtual void handle(HttpRequest* req) override;
+};
+
+void JeHeapAction::handle(HttpRequest* req) {
+    std::lock_guard<std::mutex> lock(kJeprofileActionMutex);
+#ifndef USE_JEMALLOC
+    std::string str = "jemalloc heap dump is not available without setting USE_JEMALLOC";
+    HttpChannel::send_reply(req, str);
+#else
+    std::stringstream tmp_jeprof_file_name;
+    std::time_t now = std::time(nullptr);
+    // Build a temporary file name that is hopefully unique.
+    tmp_jeprof_file_name << config::jeprofile_dir << "/jeheap_dump." << now << "." << getpid()
+                         << "." << rand() << ".heap";
+    const std::string& tmp_file_name_str = tmp_jeprof_file_name.str();
+    const char* file_name_ptr = tmp_file_name_str.c_str();
+    int result = je_mallctl("prof.dump", nullptr, nullptr, &file_name_ptr, sizeof(const char*));
+    std::stringstream response;
+    if (result == 0) {
+        response << "Jemalloc heap dump success, dump file path: " << tmp_jeprof_file_name.str()
+                 << "\n";
+    } else {
+        response << "Jemalloc heap dump failed, je_mallctl return: " << result << "\n";
+    }
+    HttpChannel::send_reply(req, response.str());
+#endif
+}
+
+Status JeprofileActions::setup(doris::ExecEnv* exec_env, doris::EvHttpServer* http_server,
+                               doris::ObjectPool& pool) {
+    if (!config::jeprofile_dir.empty()) {
+        FileUtils::create_dir(config::jeprofile_dir);
+    }
+    http_server->register_handler(HttpMethod::GET, "/jeheap/dump", pool.add(new JeHeapAction()));
+    return Status::OK();
+}
+
+} // namespace doris
diff --git a/be/src/http/action/jeprofile_actions.h b/be/src/http/action/jeprofile_actions.h
new file mode 100644
index 000000000..2ebeb3c9f
--- /dev/null
+++ b/be/src/http/action/jeprofile_actions.h
@@ -0,0 +1,31 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_JEPROFILE_ACTIONS_H
+#define DORIS_JEPROFILE_ACTIONS_H
+#include "common/status.h"
+namespace doris {
+class EvHttpServer;
+class ExecEnv;
+class ObjectPool;
+class JeprofileActions {
+public:
+    static Status setup(ExecEnv* exec_env, EvHttpServer* http_server, ObjectPool& pool);
+};
+
+} // namespace doris
+#endif //DORIS_JEPROFILE_ACTIONS_H
diff --git a/be/src/http/action/pad_rowset_action.cpp b/be/src/http/action/pad_rowset_action.cpp
new file mode 100644
index 000000000..df2721f50
--- /dev/null
+++ b/be/src/http/action/pad_rowset_action.cpp
@@ -0,0 +1,105 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "http/action/pad_rowset_action.h"
+
+#include <memory>
+#include <mutex>
+
+#include "http/http_channel.h"
+#include "olap/olap_common.h"
+#include "olap/rowset/beta_rowset_writer.h"
+#include "olap/rowset/rowset.h"
+#include "olap/storage_engine.h"
+
+namespace doris {
+
+const std::string TABLET_ID = "tablet_id";
+const std::string START_VERSION = "start_version";
+const std::string END_VERSION = "end_version";
+
+Status check_one_param(const std::string& param_val, const std::string& param_name) {
+    if (param_val.empty()) {
+        return Status::InternalError("paramater {} not specified in url", param_name);
+    }
+    return Status::OK();
+}
+
+void PadRowsetAction::handle(HttpRequest* req) {
+    LOG(INFO) << "accept one request " << req->debug_string();
+    Status status = _handle(req);
+    std::string result = status.to_json();
+    LOG(INFO) << "handle request result:" << result;
+    if (status.ok()) {
+        HttpChannel::send_reply(req, HttpStatus::OK, result);
+    } else {
+        HttpChannel::send_reply(req, HttpStatus::INTERNAL_SERVER_ERROR, result);
+    }
+}
+
+Status PadRowsetAction::check_param(HttpRequest* req) {
+    RETURN_IF_ERROR(check_one_param(req->param(TABLET_ID), TABLET_ID));
+    RETURN_IF_ERROR(check_one_param(req->param(START_VERSION), START_VERSION));
+    RETURN_IF_ERROR(check_one_param(req->param(END_VERSION), END_VERSION));
+    return Status::OK();
+}
+
+Status PadRowsetAction::_handle(HttpRequest* req) {
+    RETURN_IF_ERROR(check_param(req));
+
+    const std::string& tablet_id_str = req->param(TABLET_ID);
+    const std::string& start_version_str = req->param(START_VERSION);
+    const std::string& end_version_str = req->param(END_VERSION);
+
+    // valid str format
+    int64_t tablet_id = std::atol(tablet_id_str.c_str());
+    int32_t start_version = std::atoi(start_version_str.c_str());
+    int32_t end_version = std::atoi(end_version_str.c_str());
+    if (start_version < 0 || end_version < 0 || end_version < start_version) {
+        return Status::InternalError("Invalid input version");
+    }
+
+    auto tablet = StorageEngine::instance()->tablet_manager()->get_tablet(tablet_id);
+    if (nullptr == tablet) {
+        return Status::InternalError("Unknown tablet id {}", tablet_id);
+    }
+    return _pad_rowset(tablet, Version(start_version, end_version));
+}
+
+Status PadRowsetAction::_pad_rowset(TabletSharedPtr tablet, const Version& version) {
+    if (tablet->check_version_exist(version)) {
+        return Status::InternalError("Input version {} exists", version.to_string());
+    }
+
+    std::unique_ptr<RowsetWriter> writer;
+    RETURN_IF_ERROR(tablet->create_rowset_writer(version, VISIBLE, NONOVERLAPPING,
+                                                 tablet->tablet_schema(), -1, -1, &writer));
+    auto rowset = writer->build();
+    rowset->make_visible(version);
+
+    std::vector<RowsetSharedPtr> to_add {rowset};
+    std::vector<RowsetSharedPtr> to_delete;
+    {
+        std::unique_lock wlock(tablet->get_header_lock());
+        tablet->modify_rowsets(to_add, to_delete);
+        tablet->save_meta();
+    }
+
+    return Status::OK();
+}
+
+} // namespace doris
diff --git a/be/src/http/action/pad_rowset_action.h b/be/src/http/action/pad_rowset_action.h
new file mode 100644
index 000000000..f6036dc9f
--- /dev/null
+++ b/be/src/http/action/pad_rowset_action.h
@@ -0,0 +1,44 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#pragma once
+
+#include "common/status.h"
+#include "http/http_handler.h"
+#include "http/http_request.h"
+#include "olap/tablet.h"
+
+namespace doris {
+
+class PadRowsetAction : public HttpHandler {
+public:
+    PadRowsetAction() = default;
+
+    ~PadRowsetAction() override = default;
+
+    void handle(HttpRequest* req) override;
+
+private:
+    Status _handle(HttpRequest* req);
+    Status check_param(HttpRequest* req);
+
+#ifdef BE_TEST
+public:
+#endif
+    Status _pad_rowset(TabletSharedPtr tablet, const Version& version);
+};
+} // end namespace doris
\ No newline at end of file
diff --git a/be/src/io/CMakeLists.txt b/be/src/io/CMakeLists.txt
index a8ea5ff23..900026002 100644
--- a/be/src/io/CMakeLists.txt
+++ b/be/src/io/CMakeLists.txt
@@ -33,6 +33,7 @@ set(IO_FILES
     local_file_writer.cpp
     s3_reader.cpp
     s3_writer.cpp
+    fs/file_reader_options.cpp
     fs/file_system_map.cpp
     fs/local_file_reader.cpp
     fs/local_file_system.cpp
@@ -44,6 +45,7 @@ set(IO_FILES
     fs/hdfs_file_reader.cpp
     fs/broker_file_system.cpp
     fs/broker_file_reader.cpp
+    fs/remote_file_system.cpp
     fs/stream_load_pipe.cpp
     cache/dummy_file_cache.cpp
     cache/file_cache.cpp
diff --git a/be/src/io/cache/file_cache_manager.cpp b/be/src/io/cache/file_cache_manager.cpp
index a49dfcd24..59a12b0ea 100644
--- a/be/src/io/cache/file_cache_manager.cpp
+++ b/be/src/io/cache/file_cache_manager.cpp
@@ -216,12 +216,13 @@ void FileCacheManager::gc_file_caches() {
 
 FileCachePtr FileCacheManager::new_file_cache(const std::string& cache_dir, int64_t alive_time_sec,
                                               io::FileReaderSPtr remote_file_reader,
-                                              const std::string& file_cache_type) {
-    if (file_cache_type == "whole_file_cache") {
+                                              io::FileCacheType cache_type) {
+    switch (cache_type) {
+    case io::FileCacheType::SUB_FILE_CACHE:
         return std::make_unique<WholeFileCache>(cache_dir, alive_time_sec, remote_file_reader);
-    } else if (file_cache_type == "sub_file_cache") {
+    case io::FileCacheType::WHOLE_FILE_CACHE:
         return std::make_unique<SubFileCache>(cache_dir, alive_time_sec, remote_file_reader);
-    } else {
+    default:
         return nullptr;
     }
 }
diff --git a/be/src/io/cache/file_cache_manager.h b/be/src/io/cache/file_cache_manager.h
index 9332b324f..b8200d964 100644
--- a/be/src/io/cache/file_cache_manager.h
+++ b/be/src/io/cache/file_cache_manager.h
@@ -24,6 +24,7 @@
 #include "common/config.h"
 #include "common/status.h"
 #include "io/cache/file_cache.h"
+#include "io/fs/file_reader_options.h"
 
 namespace doris {
 namespace io {
@@ -59,7 +60,7 @@ public:
 
     FileCachePtr new_file_cache(const std::string& cache_dir, int64_t alive_time_sec,
                                 io::FileReaderSPtr remote_file_reader,
-                                const std::string& file_cache_type);
+                                io::FileCacheType cache_type);
 
     bool exist(const std::string& cache_path);
 
diff --git a/be/src/io/file_factory.cpp b/be/src/io/file_factory.cpp
index 2c8b56ea5..d44a2add8 100644
--- a/be/src/io/file_factory.cpp
+++ b/be/src/io/file_factory.cpp
@@ -170,7 +170,7 @@ Status FileFactory::create_file_reader(RuntimeProfile* /*profile*/,
     }
     case TFileType::FILE_BROKER: {
         RETURN_IF_ERROR(create_broker_reader(system_properties.broker_addresses[0],
-                                             system_properties.properties, file_description.path,
+                                             system_properties.properties, file_description,
                                              &file_system_ptr, file_reader));
         break;
     }
@@ -232,12 +232,12 @@ Status FileFactory::create_s3_reader(const std::map<std::string, std::string>& p
 
 Status FileFactory::create_broker_reader(const TNetworkAddress& broker_addr,
                                          const std::map<std::string, std::string>& prop,
-                                         const std::string& path,
+                                         const FileDescription& file_description,
                                          io::FileSystem** broker_file_system,
                                          io::FileReaderSPtr* reader) {
-    *broker_file_system = new io::BrokerFileSystem(broker_addr, prop);
+    *broker_file_system = new io::BrokerFileSystem(broker_addr, prop, file_description.file_size);
     RETURN_IF_ERROR((dynamic_cast<io::BrokerFileSystem*>(*broker_file_system))->connect());
-    RETURN_IF_ERROR((*broker_file_system)->open_file(path, reader));
+    RETURN_IF_ERROR((*broker_file_system)->open_file(file_description.path, reader));
     return Status::OK();
 }
 } // namespace doris
diff --git a/be/src/io/file_factory.h b/be/src/io/file_factory.h
index 69d115881..d59aca61b 100644
--- a/be/src/io/file_factory.h
+++ b/be/src/io/file_factory.h
@@ -95,7 +95,8 @@ public:
 
     static Status create_broker_reader(const TNetworkAddress& broker_addr,
                                        const std::map<std::string, std::string>& prop,
-                                       const std::string& path, io::FileSystem** hdfs_file_system,
+                                       const FileDescription& file_description,
+                                       io::FileSystem** hdfs_file_system,
                                        io::FileReaderSPtr* reader);
 
     static TFileType::type convert_storage_type(TStorageBackendType::type type) {
diff --git a/be/src/io/fs/broker_file_system.cpp b/be/src/io/fs/broker_file_system.cpp
index c65a0319e..af6cdcefd 100644
--- a/be/src/io/fs/broker_file_system.cpp
+++ b/be/src/io/fs/broker_file_system.cpp
@@ -57,10 +57,12 @@ inline const std::string& client_id(const TNetworkAddress& addr) {
 #endif
 
 BrokerFileSystem::BrokerFileSystem(const TNetworkAddress& broker_addr,
-                                   const std::map<std::string, std::string>& broker_prop)
+                                   const std::map<std::string, std::string>& broker_prop,
+                                   size_t file_size)
         : RemoteFileSystem("", "", FileSystemType::BROKER),
           _broker_addr(broker_addr),
-          _broker_prop(broker_prop) {}
+          _broker_prop(broker_prop),
+          _file_size(file_size) {}
 
 Status BrokerFileSystem::connect() {
     Status status = Status::OK();
@@ -110,13 +112,12 @@ Status BrokerFileSystem::open_file(const Path& path, FileReaderSPtr* reader) {
     // TODO(cmy): The file size is no longer got from openReader() method.
     // But leave the code here for compatibility.
     // This will be removed later.
-    size_t file_size = 0;
     TBrokerFD fd;
     if (response->__isset.size) {
-        file_size = response->size;
+        _file_size = response->size;
     }
     fd = response->fd;
-    *reader = std::make_shared<BrokerFileReader>(_broker_addr, path, file_size, fd, this);
+    *reader = std::make_shared<BrokerFileReader>(_broker_addr, path, _file_size, fd, this);
     return Status::OK();
 }
 
@@ -196,43 +197,7 @@ Status BrokerFileSystem::exists(const Path& path, bool* res) const {
 }
 
 Status BrokerFileSystem::file_size(const Path& path, size_t* file_size) const {
-    CHECK_BROKER_CLIENT(_client);
-    TBrokerOpenReaderRequest request;
-    request.__set_version(TBrokerVersion::VERSION_ONE);
-    request.__set_path(path);
-    request.__set_startOffset(0);
-    request.__set_clientId(client_id(_broker_addr));
-    request.__set_properties(_broker_prop);
-
-    TBrokerOpenReaderResponse* response = new TBrokerOpenReaderResponse();
-    Defer del_reponse {[&] { delete response; }};
-    try {
-        Status status;
-        try {
-            (*_client)->openReader(*response, request);
-        } catch (apache::thrift::transport::TTransportException& e) {
-            std::this_thread::sleep_for(std::chrono::seconds(1));
-            RETURN_IF_ERROR((*_client).reopen());
-            (*_client)->openReader(*response, request);
-        }
-    } catch (apache::thrift::TException& e) {
-        std::stringstream ss;
-        ss << "Open broker reader failed, broker: " << _broker_addr << " failed: " << e.what();
-        return Status::RpcError(ss.str());
-    }
-
-    if (response->opStatus.statusCode != TBrokerOperationStatusCode::OK) {
-        std::stringstream ss;
-        ss << "Open broker reader failed, broker: " << _broker_addr
-           << " failed: " << response->opStatus.message;
-        return Status::RpcError(ss.str());
-    }
-    // TODO(cmy): The file size is no longer got from openReader() method.
-    // But leave the code here for compatibility.
-    // This will be removed later.
-    if (response->__isset.size) {
-        *file_size = response->size;
-    }
+    *file_size = _file_size;
     return Status::OK();
 }
 
diff --git a/be/src/io/fs/broker_file_system.h b/be/src/io/fs/broker_file_system.h
index 5b6cce33b..fc478cddb 100644
--- a/be/src/io/fs/broker_file_system.h
+++ b/be/src/io/fs/broker_file_system.h
@@ -25,7 +25,8 @@ namespace io {
 class BrokerFileSystem final : public RemoteFileSystem {
 public:
     BrokerFileSystem(const TNetworkAddress& broker_addr,
-                     const std::map<std::string, std::string>& broker_prop);
+                     const std::map<std::string, std::string>& broker_prop, size_t file_size);
+
     ~BrokerFileSystem() override = default;
 
     Status create_file(const Path& /*path*/, FileWriterPtr* /*writer*/) override {
@@ -67,6 +68,7 @@ public:
 private:
     const TNetworkAddress& _broker_addr;
     const std::map<std::string, std::string>& _broker_prop;
+    size_t _file_size;
 
     std::shared_ptr<BrokerServiceConnection> _client;
 };
diff --git a/be/src/io/fs/file_reader_options.cpp b/be/src/io/fs/file_reader_options.cpp
new file mode 100644
index 000000000..00534d8c4
--- /dev/null
+++ b/be/src/io/fs/file_reader_options.cpp
@@ -0,0 +1,36 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "io/fs/file_reader_options.h"
+
+namespace doris {
+namespace io {
+
+FileCacheType cache_type_from_string(const std::string& type) {
+    if (type == "sub_file_cache") {
+        return FileCacheType::SUB_FILE_CACHE;
+    } else if (type == "whole_file_cache") {
+        return FileCacheType::WHOLE_FILE_CACHE;
+    } else if (type == "file_block_cache") {
+        return FileCacheType::FILE_BLOCK_CACHE;
+    } else {
+        return FileCacheType::NO_CACHE;
+    }
+}
+
+} // namespace io
+} // namespace doris
diff --git a/be/src/io/fs/file_reader_options.h b/be/src/io/fs/file_reader_options.h
new file mode 100644
index 000000000..c4c006170
--- /dev/null
+++ b/be/src/io/fs/file_reader_options.h
@@ -0,0 +1,72 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#pragma once
+
+#include <string>
+
+#include "common/status.h"
+
+namespace doris {
+namespace io {
+
+enum class FileCacheType : uint8_t {
+    NO_CACHE,
+    SUB_FILE_CACHE,
+    WHOLE_FILE_CACHE,
+    FILE_BLOCK_CACHE,
+};
+
+FileCacheType cache_type_from_string(const std::string& type);
+
+// CachePathPolicy it to define which cache path should be used
+// for the local cache of the given file(path).
+// The dervied class should implement get_cache_path() method
+class CachePathPolicy {
+public:
+    // path: the path of file which will be cached
+    // return value: the cache path of the given file.
+    virtual std::string get_cache_path(const std::string& path) const { return ""; }
+};
+
+class NoCachePathPolicy : public CachePathPolicy {
+public:
+    NoCachePathPolicy() = default;
+    std::string get_cache_path(const std::string& path) const override { return path; }
+};
+
+class SegmentCachePathPolicy : public CachePathPolicy {
+public:
+    SegmentCachePathPolicy() = default;
+    std::string get_cache_path(const std::string& path) const override {
+        // the segment file path is {rowset_dir}/{schema_hash}/{rowset_id}_{seg_num}.dat
+        // cache path is: {rowset_dir}/{schema_hash}/{rowset_id}_{seg_num}/
+        return path.substr(0, path.size() - 4) + "/";
+    }
+};
+
+class FileReaderOptions {
+public:
+    FileReaderOptions(FileCacheType cache_type_, const CachePathPolicy& path_policy_)
+            : cache_type(cache_type_), path_policy(path_policy_) {}
+
+    FileCacheType cache_type;
+    CachePathPolicy path_policy;
+};
+
+} // namespace io
+} // namespace doris
diff --git a/be/src/io/fs/file_system.h b/be/src/io/fs/file_system.h
index 2dde2a64e..735e25793 100644
--- a/be/src/io/fs/file_system.h
+++ b/be/src/io/fs/file_system.h
@@ -22,6 +22,7 @@
 #include "common/status.h"
 #include "gutil/macros.h"
 #include "io/fs/file_reader.h"
+#include "io/fs/file_reader_options.h"
 #include "io/fs/file_writer.h"
 #include "io/fs/path.h"
 
@@ -52,6 +53,9 @@ public:
 
     virtual Status create_file(const Path& path, FileWriterPtr* writer) = 0;
 
+    virtual Status open_file(const Path& path, const FileReaderOptions& reader_options,
+                             FileReaderSPtr* reader) = 0;
+
     virtual Status open_file(const Path& path, FileReaderSPtr* reader) = 0;
 
     virtual Status delete_file(const Path& path) = 0;
diff --git a/be/src/io/fs/local_file_system.h b/be/src/io/fs/local_file_system.h
index 1477d0aa9..92c6944ab 100644
--- a/be/src/io/fs/local_file_system.h
+++ b/be/src/io/fs/local_file_system.h
@@ -30,6 +30,11 @@ public:
 
     Status create_file(const Path& path, FileWriterPtr* writer) override;
 
+    Status open_file(const Path& path, const FileReaderOptions& reader_options,
+                     FileReaderSPtr* reader) override {
+        return open_file(path, reader);
+    }
+
     Status open_file(const Path& path, FileReaderSPtr* reader) override;
 
     Status delete_file(const Path& path) override;
diff --git a/be/src/io/fs/remote_file_system.cpp b/be/src/io/fs/remote_file_system.cpp
new file mode 100644
index 000000000..871c32578
--- /dev/null
+++ b/be/src/io/fs/remote_file_system.cpp
@@ -0,0 +1,59 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "io/fs/remote_file_system.h"
+
+#include "gutil/strings/stringpiece.h"
+#include "io/cache/file_cache_manager.h"
+#include "io/fs/file_reader_options.h"
+
+namespace doris {
+namespace io {
+
+Status RemoteFileSystem::open_file(const Path& path, const FileReaderOptions& reader_options,
+                                   FileReaderSPtr* reader) {
+    FileReaderSPtr raw_reader;
+    RETURN_IF_ERROR(open_file(path, &raw_reader));
+    switch (reader_options.cache_type) {
+    case io::FileCacheType::NO_CACHE: {
+        *reader = raw_reader;
+        break;
+    }
+    case io::FileCacheType::SUB_FILE_CACHE:
+    case io::FileCacheType::WHOLE_FILE_CACHE: {
+        StringPiece str(path.native());
+        std::string cache_path = reader_options.path_policy.get_cache_path(str.as_string());
+        io::FileCachePtr cache_reader = FileCacheManager::instance()->new_file_cache(
+                cache_path, config::file_cache_alive_time_sec, raw_reader,
+                reader_options.cache_type);
+        FileCacheManager::instance()->add_file_cache(cache_path, cache_reader);
+        *reader = cache_reader;
+        break;
+    }
+    case io::FileCacheType::FILE_BLOCK_CACHE: {
+        return Status::NotSupported("add file block cache reader");
+    }
+    default: {
+        // TODO: add file block cache reader
+        return Status::InternalError("Unknown cache type: {}", reader_options.cache_type);
+    }
+    }
+    return Status::OK();
+}
+
+} // namespace io
+} // namespace doris
diff --git a/be/src/io/fs/remote_file_system.h b/be/src/io/fs/remote_file_system.h
index 390da5ca0..2218ce40c 100644
--- a/be/src/io/fs/remote_file_system.h
+++ b/be/src/io/fs/remote_file_system.h
@@ -35,6 +35,13 @@ public:
                                 const std::vector<Path>& dest_paths) = 0;
 
     virtual Status connect() = 0;
+
+    Status open_file(const Path& path, const FileReaderOptions& reader_options,
+                     FileReaderSPtr* reader) override;
+
+    Status open_file(const Path& path, FileReaderSPtr* reader) override {
+        return Status::NotSupported("implemented in derived classes");
+    }
 };
 
 } // namespace io
diff --git a/be/src/olap/CMakeLists.txt b/be/src/olap/CMakeLists.txt
index fd1387aca..7f542e971 100644
--- a/be/src/olap/CMakeLists.txt
+++ b/be/src/olap/CMakeLists.txt
@@ -40,6 +40,7 @@ add_library(Olap STATIC
     file_helper.cpp
     hll.cpp
     inverted_index_parser.cpp
+    itoken_extractor.cpp
     like_column_predicate.cpp
     key_coder.cpp
     lru_cache.cpp
@@ -93,6 +94,7 @@ add_library(Olap STATIC
     rowset/segment_v2/empty_segment_iterator.cpp
     rowset/segment_v2/segment_writer.cpp
     rowset/segment_v2/block_split_bloom_filter.cpp
+    rowset/segment_v2/ngram_bloom_filter.cpp
     rowset/segment_v2/bloom_filter_index_reader.cpp
     rowset/segment_v2/bloom_filter_index_writer.cpp
     rowset/segment_v2/bloom_filter.cpp
diff --git a/be/src/olap/column_predicate.h b/be/src/olap/column_predicate.h
index 1d3a56110..293ac742e 100644
--- a/be/src/olap/column_predicate.h
+++ b/be/src/olap/column_predicate.h
@@ -31,6 +31,10 @@ namespace doris {
 
 class Schema;
 
+struct PredicateParams {
+    std::string value;
+};
+
 enum class PredicateType {
     UNKNOWN = 0,
     EQ = 1,
@@ -113,7 +117,9 @@ struct PredicateTypeTraits {
 class ColumnPredicate {
 public:
     explicit ColumnPredicate(uint32_t column_id, bool opposite = false)
-            : _column_id(column_id), _opposite(opposite) {}
+            : _column_id(column_id), _opposite(opposite) {
+        _predicate_params = std::make_shared<PredicateParams>();
+    }
 
     virtual ~ColumnPredicate() = default;
 
@@ -156,6 +162,15 @@ public:
                                   bool* flags) const {
         DCHECK(false) << "should not reach here";
     }
+
+    virtual std::string get_search_str() const {
+        DCHECK(false) << "should not reach here";
+        return "";
+    }
+
+    virtual void set_page_ng_bf(std::unique_ptr<segment_v2::BloomFilter>) {
+        DCHECK(false) << "should not reach here";
+    }
     uint32_t column_id() const { return _column_id; }
 
     virtual std::string debug_string() const {
@@ -163,12 +178,43 @@ public:
                ", opposite=" + (_opposite ? "true" : "false");
     }
 
+    std::shared_ptr<PredicateParams> predicate_params() { return _predicate_params; }
+    const std::string pred_type_string(PredicateType type) {
+        switch (type) {
+        case PredicateType::EQ:
+            return "eq";
+        case PredicateType::NE:
+            return "ne";
+        case PredicateType::LT:
+            return "lt";
+        case PredicateType::LE:
+            return "le";
+        case PredicateType::GT:
+            return "gt";
+        case PredicateType::GE:
+            return "ge";
+        case PredicateType::IN_LIST:
+            return "in_list";
+        case PredicateType::NOT_IN_LIST:
+            return "not_in_list";
+        case PredicateType::IS_NULL:
+            return "is_null";
+        case PredicateType::IS_NOT_NULL:
+            return "is_not_null";
+        case PredicateType::BF:
+            return "bf";
+        default:
+            return "unknown";
+        }
+    }
+
 protected:
     virtual std::string _debug_string() const = 0;
 
     uint32_t _column_id;
     // TODO: the value is only in delete condition, better be template value
     bool _opposite;
+    std::shared_ptr<PredicateParams> _predicate_params;
 };
 
 } //namespace doris
diff --git a/be/src/olap/iterators.h b/be/src/olap/iterators.h
index 241942f71..105d8a291 100644
--- a/be/src/olap/iterators.h
+++ b/be/src/olap/iterators.h
@@ -25,6 +25,7 @@
 #include "olap/olap_common.h"
 #include "olap/tablet_schema.h"
 #include "vec/core/block.h"
+#include "vec/exprs/vexpr.h"
 
 namespace doris {
 
@@ -81,6 +82,7 @@ public:
     // reader's column predicate, nullptr if not existed
     // used to fiter rows in row block
     std::vector<ColumnPredicate*> column_predicates;
+    std::vector<ColumnPredicate*> column_predicates_except_leafnode_of_andnode;
     std::unordered_map<int32_t, std::shared_ptr<AndBlockColumnPredicate>> col_id_to_predicates;
     std::unordered_map<int32_t, std::vector<const ColumnPredicate*>> col_id_to_del_predicates;
     TPushAggOp::type push_down_agg_type_opt = TPushAggOp::NONE;
@@ -96,8 +98,8 @@ public:
     bool read_orderby_key_reverse = false;
     // columns for orderby keys
     std::vector<uint32_t>* read_orderby_key_columns = nullptr;
-
     IOContext io_ctx;
+    vectorized::VExpr* remaining_vconjunct_root = nullptr;
 };
 
 class RowwiseIterator {
diff --git a/be/src/olap/itoken_extractor.cpp b/be/src/olap/itoken_extractor.cpp
new file mode 100644
index 000000000..90ad29d29
--- /dev/null
+++ b/be/src/olap/itoken_extractor.cpp
@@ -0,0 +1,77 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "itoken_extractor.h"
+
+#include "util/simd/vstring_function.h"
+
+namespace doris {
+
+bool NgramTokenExtractor::next_in_string(const char* data, size_t length, size_t* __restrict pos,
+                                         size_t* __restrict token_start,
+                                         size_t* __restrict token_length) const {
+    *token_start = *pos;
+    *token_length = 0;
+    size_t code_points = 0;
+    for (; code_points < n && *token_start + *token_length < length; ++code_points) {
+        size_t sz = get_utf8_byte_length(static_cast<uint8_t>(data[*token_start + *token_length]));
+        *token_length += sz;
+    }
+    *pos += get_utf8_byte_length(static_cast<uint8_t>(data[*pos]));
+    return code_points == n;
+}
+
+bool NgramTokenExtractor::next_in_string_like(const char* data, size_t length, size_t* pos,
+                                              std::string& token) const {
+    token.clear();
+
+    size_t code_points = 0;
+    bool escaped = false;
+    for (size_t i = *pos; i < length;) {
+        if (escaped && (data[i] == '%' || data[i] == '_' || data[i] == '\\')) {
+            token += data[i];
+            ++code_points;
+            escaped = false;
+            ++i;
+        } else if (!escaped && (data[i] == '%' || data[i] == '_')) {
+            /// This token is too small, go to the next.
+            token.clear();
+            code_points = 0;
+            escaped = false;
+            *pos = ++i;
+        } else if (!escaped && data[i] == '\\') {
+            escaped = true;
+            ++i;
+        } else {
+            const size_t sz = get_utf8_byte_length(static_cast<uint8_t>(data[i]));
+            for (size_t j = 0; j < sz; ++j) {
+                token += data[i + j];
+            }
+            i += sz;
+            ++code_points;
+            escaped = false;
+        }
+
+        if (code_points == n) {
+            *pos += get_utf8_byte_length(static_cast<uint8_t>(data[*pos]));
+            return true;
+        }
+    }
+
+    return false;
+}
+} // namespace doris
diff --git a/be/src/olap/itoken_extractor.h b/be/src/olap/itoken_extractor.h
new file mode 100644
index 000000000..d7004d9c8
--- /dev/null
+++ b/be/src/olap/itoken_extractor.h
@@ -0,0 +1,98 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_ITOKEN_EXTRACTOR_H
+#define DORIS_ITOKEN_EXTRACTOR_H
+
+#include <stddef.h>
+
+#include <string>
+
+#include "olap/rowset/segment_v2/bloom_filter.h"
+
+namespace doris {
+
+/// Interface for string parsers.
+struct ITokenExtractor {
+    virtual ~ITokenExtractor() = default;
+
+    /// Fast inplace implementation for regular use.
+    /// Gets string (data ptr and len) and start position for extracting next token (state of extractor).
+    /// Returns false if parsing is finished, otherwise returns true.
+    virtual bool next_in_string(const char* data, size_t length, size_t* __restrict pos,
+                                size_t* __restrict token_start,
+                                size_t* __restrict token_length) const = 0;
+
+    /// Special implementation for creating bloom filter for LIKE function.
+    /// It skips unescaped `%` and `_` and supports escaping symbols, but it is less lightweight.
+    virtual bool next_in_string_like(const char* data, size_t length, size_t* pos,
+                                     std::string& out) const = 0;
+
+    virtual void string_to_bloom_filter(const char* data, size_t length,
+                                        segment_v2::BloomFilter& bloom_filter) const = 0;
+
+    virtual bool string_like_to_bloom_filter(const char* data, size_t length,
+                                             segment_v2::BloomFilter& bloom_filter) const = 0;
+};
+
+template <typename Derived>
+class ITokenExtractorHelper : public ITokenExtractor {
+public:
+    void string_to_bloom_filter(const char* data, size_t length,
+                                segment_v2::BloomFilter& bloom_filter) const override {
+        size_t cur = 0;
+        size_t token_start = 0;
+        size_t token_len = 0;
+
+        while (cur < length && static_cast<const Derived*>(this)->next_in_string(
+                                       data, length, &cur, &token_start, &token_len))
+            bloom_filter.add_bytes(data + token_start, token_len);
+    }
+
+    bool string_like_to_bloom_filter(const char* data, size_t length,
+                                     segment_v2::BloomFilter& bloom_filter) const override {
+        size_t cur = 0;
+        bool added = false;
+        std::string token;
+        while (cur < length &&
+               static_cast<const Derived*>(this)->next_in_string_like(data, length, &cur, token)) {
+            bloom_filter.add_bytes(token.data(), token.size());
+            added = true;
+        }
+
+        return added;
+    }
+};
+
+/// Parser extracting all ngrams from string.
+struct NgramTokenExtractor final : public ITokenExtractorHelper<NgramTokenExtractor> {
+public:
+    explicit NgramTokenExtractor(size_t n_) : n(n_) {}
+
+    bool next_in_string(const char* data, size_t length, size_t* __restrict pos,
+                        size_t* __restrict token_start,
+                        size_t* __restrict token_length) const override;
+
+    bool next_in_string_like(const char* data, size_t length, size_t* pos,
+                             std::string& token) const override;
+
+private:
+    size_t n;
+};
+} // namespace doris
+
+#endif //DORIS_ITOKEN_EXTRACTOR_H
diff --git a/be/src/olap/like_column_predicate.h b/be/src/olap/like_column_predicate.h
index e376fe4df..2be798010 100644
--- a/be/src/olap/like_column_predicate.h
+++ b/be/src/olap/like_column_predicate.h
@@ -47,6 +47,22 @@ public:
     void evaluate_and_vec(const vectorized::IColumn& column, uint16_t size,
                           bool* flags) const override;
 
+    std::string get_search_str() const override {
+        return std::string(reinterpret_cast<char*>(pattern.ptr), pattern.len);
+    }
+    bool is_opposite() const { return _opposite; }
+
+    void set_page_ng_bf(std::unique_ptr<segment_v2::BloomFilter> src) override {
+        _page_ng_bf = std::move(src);
+    }
+    bool evaluate_and(const BloomFilter* bf) const override {
+        if (_page_ng_bf) {
+            return bf->contains(*_page_ng_bf);
+        }
+        return true;
+    }
+    bool can_do_bloom_filter() const override { return true; }
+
 private:
     template <bool is_and>
     void _evaluate_vec(const vectorized::IColumn& column, uint16_t size, bool* flags) const {
@@ -130,9 +146,11 @@ private:
 
     StateType* _state;
 
-    // A separate scratch region is required for every concurrent caller of the Hyperscan API.
-    // So here _like_state is separate for each instance of LikeColumnPredicate.
+    // A separate scratch region is required for every concurrent caller of the
+    // Hyperscan API. So here _like_state is separate for each instance of
+    // LikeColumnPredicate.
     vectorized::LikeSearchState _like_state;
+    std::unique_ptr<segment_v2::BloomFilter> _page_ng_bf; // for ngram-bf index
 };
 
-} //namespace doris
+} // namespace doris
diff --git a/be/src/olap/reader.cpp b/be/src/olap/reader.cpp
index 7a0df6c66..7d823ee04 100644
--- a/be/src/olap/reader.cpp
+++ b/be/src/olap/reader.cpp
@@ -22,6 +22,11 @@
 #include "common/status.h"
 #include "exprs/create_predicate_function.h"
 #include "exprs/hybrid_set.h"
+#include "gen_cpp/segment_v2.pb.h"
+#include "olap/bloom_filter_predicate.h"
+#include "olap/comparison_predicate.h"
+#include "olap/in_list_predicate.h"
+#include "olap/itoken_extractor.h"
 #include "olap/like_column_predicate.h"
 #include "olap/olap_common.h"
 #include "olap/predicate_creator.h"
@@ -85,6 +90,9 @@ TabletReader::~TabletReader() {
     for (auto pred : _value_col_predicates) {
         delete pred;
     }
+    for (auto pred : _col_preds_except_leafnode_of_andnode) {
+        delete pred;
+    }
 }
 
 Status TabletReader::init(const ReaderParams& read_params) {
@@ -200,6 +208,7 @@ Status TabletReader::_capture_rs_readers(const ReaderParams& read_params,
     _reader_context.read_orderby_key_columns =
             _orderby_key_columns.size() > 0 ? &_orderby_key_columns : nullptr;
     _reader_context.predicates = &_col_predicates;
+    _reader_context.predicates_except_leafnode_of_andnode = &_col_preds_except_leafnode_of_andnode;
     _reader_context.value_predicates = &_value_col_predicates;
     _reader_context.lower_bound_keys = &_keys_param.start_keys;
     _reader_context.is_lower_keys_included = &_is_lower_keys_included;
@@ -216,6 +225,7 @@ Status TabletReader::_capture_rs_readers(const ReaderParams& read_params,
     _reader_context.enable_unique_key_merge_on_write = tablet()->enable_unique_key_merge_on_write();
     _reader_context.record_rowids = read_params.record_rowids;
     _reader_context.is_key_column_group = read_params.is_key_column_group;
+    _reader_context.remaining_vconjunct_root = read_params.remaining_vconjunct_root;
 
     *valid_rs_readers = *rs_readers;
 
@@ -234,6 +244,7 @@ Status TabletReader::_init_params(const ReaderParams& read_params) {
     _reader_context.runtime_state = read_params.runtime_state;
 
     _init_conditions_param(read_params);
+    _init_conditions_param_except_leafnode_of_andnode(read_params);
 
     Status res = _init_delete_condition(read_params);
     if (!res.ok()) {
@@ -463,8 +474,52 @@ void TabletReader::_init_conditions_param(const ReaderParams& read_params) {
     }
 
     // Function filter push down to storage engine
+    auto is_like_predicate = [](ColumnPredicate* _pred) {
+        if (dynamic_cast<LikeColumnPredicate<false>*>(_pred) ||
+            dynamic_cast<LikeColumnPredicate<true>*>(_pred)) {
+            return true;
+        }
+
+        return false;
+    };
+
     for (const auto& filter : read_params.function_filters) {
         _col_predicates.emplace_back(_parse_to_predicate(filter));
+        auto* pred = _col_predicates.back();
+        const auto& col = _tablet->tablet_schema()->column(pred->column_id());
+        auto is_like = is_like_predicate(pred);
+        auto* tablet_index = _tablet->tablet_schema()->get_ngram_bf_index(col.unique_id());
+
+        if (is_like && tablet_index && config::enable_query_like_bloom_filter) {
+            std::unique_ptr<segment_v2::BloomFilter> ng_bf;
+            std::string pattern = pred->get_search_str();
+            auto gram_bf_size = tablet_index->get_gram_bf_size();
+            auto gram_size = tablet_index->get_gram_size();
+
+            segment_v2::BloomFilter::create(segment_v2::NGRAM_BLOOM_FILTER, &ng_bf, gram_bf_size);
+            NgramTokenExtractor _token_extractor(gram_size);
+
+            if (_token_extractor.string_like_to_bloom_filter(pattern.data(), pattern.length(),
+                                                             *ng_bf)) {
+                pred->set_page_ng_bf(std::move(ng_bf));
+            }
+        }
+    }
+}
+
+void TabletReader::_init_conditions_param_except_leafnode_of_andnode(
+        const ReaderParams& read_params) {
+    for (const auto& condition : read_params.conditions_except_leafnode_of_andnode) {
+        TCondition tmp_cond = condition;
+        auto condition_col_uid = _tablet_schema->column(tmp_cond.column_name).unique_id();
+        tmp_cond.__set_column_unique_id(condition_col_uid);
+        ColumnPredicate* predicate =
+                parse_to_predicate(_tablet_schema, tmp_cond, _predicate_mem_pool.get());
+        if (predicate != nullptr) {
+            auto predicate_params = predicate->predicate_params();
+            predicate_params->value = condition.condition_values[0];
+            _col_preds_except_leafnode_of_andnode.push_back(predicate);
+        }
     }
 }
 
diff --git a/be/src/olap/reader.h b/be/src/olap/reader.h
index 3c4ba1a0d..ccc32024d 100644
--- a/be/src/olap/reader.h
+++ b/be/src/olap/reader.h
@@ -39,6 +39,7 @@ class RuntimeState;
 namespace vectorized {
 class VCollectIterator;
 class Block;
+class VExpr;
 } // namespace vectorized
 
 class TabletReader {
@@ -75,6 +76,7 @@ public:
         std::vector<std::pair<string, std::shared_ptr<BloomFilterFuncBase>>> bloom_filters;
         std::vector<std::pair<string, std::shared_ptr<BitmapFilterFuncBase>>> bitmap_filters;
         std::vector<std::pair<string, std::shared_ptr<HybridSetBase>>> in_filters;
+        std::vector<TCondition> conditions_except_leafnode_of_andnode;
         std::vector<FunctionFilter> function_filters;
         std::vector<RowsetMetaSharedPtr> delete_predicates;
 
@@ -90,6 +92,7 @@ public:
         std::vector<uint32_t>* origin_return_columns = nullptr;
         std::unordered_set<uint32_t>* tablet_columns_convert_to_null_set = nullptr;
         TPushAggOp::type push_down_agg_type_opt = TPushAggOp::NONE;
+        vectorized::VExpr* remaining_vconjunct_root = nullptr;
 
         // used for compaction to record row ids
         bool record_rowids = false;
@@ -164,6 +167,8 @@ protected:
 
     void _init_conditions_param(const ReaderParams& read_params);
 
+    void _init_conditions_param_except_leafnode_of_andnode(const ReaderParams& read_params);
+
     ColumnPredicate* _parse_to_predicate(
             const std::pair<std::string, std::shared_ptr<BloomFilterFuncBase>>& bloom_filter);
 
@@ -198,6 +203,7 @@ protected:
     std::vector<bool> _is_lower_keys_included;
     std::vector<bool> _is_upper_keys_included;
     std::vector<ColumnPredicate*> _col_predicates;
+    std::vector<ColumnPredicate*> _col_preds_except_leafnode_of_andnode;
     std::vector<ColumnPredicate*> _value_col_predicates;
     DeleteHandler _delete_handler;
 
diff --git a/be/src/olap/rowset/beta_rowset.cpp b/be/src/olap/rowset/beta_rowset.cpp
index a6a0d2987..c9e304d38 100644
--- a/be/src/olap/rowset/beta_rowset.cpp
+++ b/be/src/olap/rowset/beta_rowset.cpp
@@ -47,12 +47,6 @@ std::string BetaRowset::segment_file_path(int segment_id) {
     return segment_file_path(_rowset_dir, rowset_id(), segment_id);
 }
 
-std::string BetaRowset::segment_cache_path(const std::string& rowset_dir, const RowsetId& rowset_id,
-                                           int segment_id) {
-    // {root_path}/data/{shard_id}/{tablet_id}/{schema_hash}/{rowset_id}_{seg_num}
-    return fmt::format("{}/{}_{}", rowset_dir, rowset_id.to_string(), segment_id);
-}
-
 std::string BetaRowset::segment_cache_path(int segment_id) {
     // {root_path}/data/{shard_id}/{tablet_id}/{schema_hash}/{rowset_id}_{seg_num}
     return fmt::format("{}/{}_{}", _tablet_path, rowset_id().to_string(), segment_id);
@@ -139,10 +133,8 @@ Status BetaRowset::load_segments(std::vector<segment_v2::SegmentSharedPtr>* segm
     }
     for (int seg_id = 0; seg_id < num_segments(); ++seg_id) {
         auto seg_path = segment_file_path(seg_id);
-        auto cache_path = segment_cache_path(seg_id);
         std::shared_ptr<segment_v2::Segment> segment;
-        auto s = segment_v2::Segment::open(fs, seg_path, cache_path, seg_id, rowset_id(), _schema,
-                                           &segment);
+        auto s = segment_v2::Segment::open(fs, seg_path, seg_id, rowset_id(), _schema, &segment);
         if (!s.ok()) {
             LOG(WARNING) << "failed to open segment. " << seg_path << " under rowset "
                          << unique_id() << " : " << s.to_string();
@@ -163,10 +155,8 @@ Status BetaRowset::load_segments(int64_t seg_id_begin, int64_t seg_id_end,
             return Status::Error<INIT_FAILED>();
         }
         auto seg_path = segment_file_path(seg_id);
-        auto cache_path = segment_cache_path(seg_id);
         std::shared_ptr<segment_v2::Segment> segment;
-        auto s = segment_v2::Segment::open(fs, seg_path, cache_path, seg_id, rowset_id(), _schema,
-                                           &segment);
+        auto s = segment_v2::Segment::open(fs, seg_path, seg_id, rowset_id(), _schema, &segment);
         if (!s.ok()) {
             LOG(WARNING) << "failed to open segment. " << seg_path << " under rowset "
                          << unique_id() << " : " << s.to_string();
@@ -335,10 +325,8 @@ bool BetaRowset::check_current_rowset_segment() {
     }
     for (int seg_id = 0; seg_id < num_segments(); ++seg_id) {
         auto seg_path = segment_file_path(seg_id);
-        auto cache_path = segment_cache_path(seg_id);
         std::shared_ptr<segment_v2::Segment> segment;
-        auto s = segment_v2::Segment::open(fs, seg_path, cache_path, seg_id, rowset_id(), _schema,
-                                           &segment);
+        auto s = segment_v2::Segment::open(fs, seg_path, seg_id, rowset_id(), _schema, &segment);
         if (!s.ok()) {
             LOG(WARNING) << "segment can not be opened. file=" << seg_path;
             return false;
diff --git a/be/src/olap/rowset/beta_rowset.h b/be/src/olap/rowset/beta_rowset.h
index a1167d2c2..5ac860eaa 100644
--- a/be/src/olap/rowset/beta_rowset.h
+++ b/be/src/olap/rowset/beta_rowset.h
@@ -46,9 +46,6 @@ public:
 
     std::string segment_cache_path(int segment_id);
 
-    static std::string segment_cache_path(const std::string& rowset_dir, const RowsetId& rowset_id,
-                                          int segment_id);
-
     static bool is_segment_cache_dir(const std::string& cache_dir);
 
     static std::string segment_file_path(const std::string& rowset_dir, const RowsetId& rowset_id,
diff --git a/be/src/olap/rowset/beta_rowset_reader.cpp b/be/src/olap/rowset/beta_rowset_reader.cpp
index b716942bc..9daab16ec 100644
--- a/be/src/olap/rowset/beta_rowset_reader.cpp
+++ b/be/src/olap/rowset/beta_rowset_reader.cpp
@@ -57,6 +57,7 @@ Status BetaRowsetReader::get_segment_iterators(RowsetReaderContext* read_context
     // convert RowsetReaderContext to StorageReadOptions
     _read_options.stats = _stats;
     _read_options.push_down_agg_type_opt = _context->push_down_agg_type_opt;
+    _read_options.remaining_vconjunct_root = _context->remaining_vconjunct_root;
     if (read_context->lower_bound_keys != nullptr) {
         for (int i = 0; i < read_context->lower_bound_keys->size(); ++i) {
             _read_options.key_ranges.emplace_back(&read_context->lower_bound_keys->at(i),
@@ -104,6 +105,14 @@ Status BetaRowsetReader::get_segment_iterators(RowsetReaderContext* read_context
                     single_column_block_predicate);
         }
     }
+
+    if (read_context->predicates_except_leafnode_of_andnode != nullptr) {
+        _read_options.column_predicates_except_leafnode_of_andnode.insert(
+                _read_options.column_predicates_except_leafnode_of_andnode.end(),
+                read_context->predicates_except_leafnode_of_andnode->begin(),
+                read_context->predicates_except_leafnode_of_andnode->end());
+    }
+
     // Take a delete-bitmap for each segment, the bitmap contains all deletes
     // until the max read version, which is read_context->version.second
     if (read_context->delete_bitmap != nullptr) {
diff --git a/be/src/olap/rowset/beta_rowset_writer.cpp b/be/src/olap/rowset/beta_rowset_writer.cpp
index e6ee1833b..4544dc7bb 100644
--- a/be/src/olap/rowset/beta_rowset_writer.cpp
+++ b/be/src/olap/rowset/beta_rowset_writer.cpp
@@ -391,10 +391,8 @@ Status BetaRowsetWriter::_load_noncompacted_segments(
     for (int seg_id = _segcompacted_point; seg_id < num; ++seg_id) {
         auto seg_path =
                 BetaRowset::segment_file_path(_context.rowset_dir, _context.rowset_id, seg_id);
-        auto cache_path =
-                BetaRowset::segment_cache_path(_context.rowset_dir, _context.rowset_id, seg_id);
         std::shared_ptr<segment_v2::Segment> segment;
-        auto s = segment_v2::Segment::open(fs, seg_path, cache_path, seg_id, rowset_id(),
+        auto s = segment_v2::Segment::open(fs, seg_path, seg_id, rowset_id(),
                                            _context.tablet_schema, &segment);
         if (!s.ok()) {
             LOG(WARNING) << "failed to open segment. " << seg_path << ":" << s.to_string();
diff --git a/be/src/olap/rowset/rowset_reader_context.h b/be/src/olap/rowset/rowset_reader_context.h
index 804a64a9a..354a5d9df 100644
--- a/be/src/olap/rowset/rowset_reader_context.h
+++ b/be/src/olap/rowset/rowset_reader_context.h
@@ -21,6 +21,7 @@
 #include "olap/column_predicate.h"
 #include "olap/olap_common.h"
 #include "runtime/runtime_state.h"
+#include "vec/exprs/vexpr.h"
 
 namespace doris {
 
@@ -45,6 +46,7 @@ struct RowsetReaderContext {
     // column name -> column predicate
     // adding column_name for predicate to make use of column selectivity
     const std::vector<ColumnPredicate*>* predicates = nullptr;
+    const std::vector<ColumnPredicate*>* predicates_except_leafnode_of_andnode = nullptr;
     // value column predicate in UNIQUE table
     const std::vector<ColumnPredicate*>* value_predicates = nullptr;
     const std::vector<RowCursor>* lower_bound_keys = nullptr;
@@ -54,6 +56,7 @@ struct RowsetReaderContext {
     const DeleteHandler* delete_handler = nullptr;
     OlapReaderStatistics* stats = nullptr;
     RuntimeState* runtime_state = nullptr;
+    vectorized::VExpr* remaining_vconjunct_root = nullptr;
     bool use_page_cache = false;
     int sequence_id_idx = -1;
     int batch_size = 1024;
diff --git a/be/src/olap/rowset/segment_v2/block_split_bloom_filter.h b/be/src/olap/rowset/segment_v2/block_split_bloom_filter.h
index 9d082f716..93591adc1 100644
--- a/be/src/olap/rowset/segment_v2/block_split_bloom_filter.h
+++ b/be/src/olap/rowset/segment_v2/block_split_bloom_filter.h
@@ -32,6 +32,7 @@ public:
     void add_hash(uint64_t hash) override;
 
     bool test_hash(uint64_t hash) const override;
+    bool contains(const BloomFilter&) const override { return true; }
 
 private:
     // Bytes in a tiny Bloom filter block.
diff --git a/be/src/olap/rowset/segment_v2/bloom_filter.cpp b/be/src/olap/rowset/segment_v2/bloom_filter.cpp
index cbb5181ac..667cc9b59 100644
--- a/be/src/olap/rowset/segment_v2/bloom_filter.cpp
+++ b/be/src/olap/rowset/segment_v2/bloom_filter.cpp
@@ -21,14 +21,18 @@
 #include "gen_cpp/segment_v2.pb.h"
 #include "gutil/strings/substitute.h"
 #include "olap/rowset/segment_v2/block_split_bloom_filter.h"
+#include "olap/rowset/segment_v2/ngram_bloom_filter.h"
 #include "olap/utils.h"
 
 namespace doris {
 namespace segment_v2 {
 
-Status BloomFilter::create(BloomFilterAlgorithmPB algorithm, std::unique_ptr<BloomFilter>* bf) {
+Status BloomFilter::create(BloomFilterAlgorithmPB algorithm, std::unique_ptr<BloomFilter>* bf,
+                           size_t bf_size) {
     if (algorithm == BLOCK_BLOOM_FILTER) {
         bf->reset(new BlockSplitBloomFilter());
+    } else if (algorithm == NGRAM_BLOOM_FILTER) {
+        bf->reset(new NGramBloomFilter(bf_size));
     } else {
         return Status::InternalError("invalid bloom filter algorithm:{}", algorithm);
     }
diff --git a/be/src/olap/rowset/segment_v2/bloom_filter.h b/be/src/olap/rowset/segment_v2/bloom_filter.h
index 8796f4c6d..139ff3aea 100644
--- a/be/src/olap/rowset/segment_v2/bloom_filter.h
+++ b/be/src/olap/rowset/segment_v2/bloom_filter.h
@@ -51,17 +51,26 @@ public:
     static const uint32_t MAXIMUM_BYTES = 128 * 1024 * 1024;
 
     // Factory function for BloomFilter
-    static Status create(BloomFilterAlgorithmPB algorithm, std::unique_ptr<BloomFilter>* bf);
+    static Status create(BloomFilterAlgorithmPB algorithm, std::unique_ptr<BloomFilter>* bf,
+                         size_t bf_size = 0);
 
     BloomFilter() : _data(nullptr), _num_bytes(0), _size(0), _has_null(nullptr) {}
 
-    virtual ~BloomFilter() { delete[] _data; }
+    virtual ~BloomFilter() {
+        if (_data) {
+            delete[] _data;
+        }
+    }
+
+    virtual bool is_ngram_bf() const { return false; }
 
     // for write
     Status init(uint64_t n, double fpp, HashStrategyPB strategy) {
         return this->init(optimal_bit_num(n, fpp) / 8, strategy);
     }
 
+    Status init(uint64_t filter_size) { return init(filter_size, HASH_MURMUR3_X64_64); }
+
     Status init(uint64_t filter_size, HashStrategyPB strategy) {
         if (strategy == HASH_MURMUR3_X64_64) {
             _hash_func = murmur_hash3_x64_64;
@@ -81,7 +90,7 @@ public:
 
     // for read
     // use deep copy to acquire the data
-    Status init(const char* buf, uint32_t size, HashStrategyPB strategy) {
+    virtual Status init(const char* buf, uint32_t size, HashStrategyPB strategy) {
         DCHECK(size > 1);
         if (strategy == HASH_MURMUR3_X64_64) {
             _hash_func = murmur_hash3_x64_64;
@@ -108,7 +117,7 @@ public:
         return hash_code;
     }
 
-    void add_bytes(const char* buf, uint32_t size) {
+    virtual void add_bytes(const char* buf, uint32_t size) {
         if (buf == nullptr) {
             *_has_null = true;
             return;
@@ -125,15 +134,19 @@ public:
         return test_hash(code);
     }
 
-    char* data() const { return _data; }
+    /// Checks if this contains everything from another bloom filter.
+    /// Bloom filters must have equal size and seed.
+    virtual bool contains(const BloomFilter& bf_) const = 0;
+
+    virtual char* data() const { return _data; }
 
     uint32_t num_bytes() const { return _num_bytes; }
 
-    uint32_t size() const { return _size; }
+    virtual uint32_t size() const { return _size; }
 
     void set_has_null(bool has_null) { *_has_null = has_null; }
 
-    bool has_null() const { return *_has_null; }
+    virtual bool has_null() const { return *_has_null; }
 
     virtual void add_hash(uint64_t hash) = 0;
     virtual bool test_hash(uint64_t hash) const = 0;
diff --git a/be/src/olap/rowset/segment_v2/bloom_filter_index_reader.cpp b/be/src/olap/rowset/segment_v2/bloom_filter_index_reader.cpp
index f3779d032..1f6a73377 100644
--- a/be/src/olap/rowset/segment_v2/bloom_filter_index_reader.cpp
+++ b/be/src/olap/rowset/segment_v2/bloom_filter_index_reader.cpp
@@ -50,8 +50,8 @@ Status BloomFilterIndexIterator::read_bloom_filter(rowid_t ordinal,
     RETURN_IF_ERROR(_bloom_filter_iter.next_batch(&num_read, &column_block_view));
     DCHECK(num_to_read == num_read);
     // construct bloom filter
-    BloomFilter::create(_reader->_bloom_filter_index_meta->algorithm(), bf);
     const Slice* value_ptr = reinterpret_cast<const Slice*>(block.data());
+    BloomFilter::create(_reader->_bloom_filter_index_meta->algorithm(), bf, value_ptr->size);
     RETURN_IF_ERROR((*bf)->init(value_ptr->data, value_ptr->size,
                                 _reader->_bloom_filter_index_meta->hash_strategy()));
     _pool->clear();
diff --git a/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.cpp b/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.cpp
index 5542a6068..567cf8d79 100644
--- a/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.cpp
+++ b/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.cpp
@@ -170,6 +170,63 @@ private:
 
 } // namespace
 
+NGramBloomFilterIndexWriterImpl::NGramBloomFilterIndexWriterImpl(
+        const BloomFilterOptions& bf_options, uint8_t gram_size, uint16_t bf_size)
+        : _bf_options(bf_options),
+          _gram_size(gram_size),
+          _bf_size(bf_size),
+          _bf_buffer_size(0),
+          _token_extractor(gram_size) {
+    BloomFilter::create(NGRAM_BLOOM_FILTER, &_bf, bf_size);
+}
+
+void NGramBloomFilterIndexWriterImpl::add_values(const void* values, size_t count) {
+    const Slice* src = reinterpret_cast<const Slice*>(values);
+    for (int i = 0; i < count; ++i, ++src) {
+        if (src->size < _gram_size) {
+            continue;
+        }
+        _token_extractor.string_to_bloom_filter(src->data, src->size, *_bf);
+    }
+}
+
+Status NGramBloomFilterIndexWriterImpl::flush() {
+    _bf_buffer_size += _bf->size();
+    _bfs.emplace_back(std::move(_bf));
+    // init new one
+    RETURN_IF_ERROR(BloomFilter::create(NGRAM_BLOOM_FILTER, &_bf, _bf_size));
+    return Status::OK();
+}
+
+Status NGramBloomFilterIndexWriterImpl::finish(io::FileWriter* file_writer,
+                                               ColumnIndexMetaPB* index_meta) {
+    index_meta->set_type(BLOOM_FILTER_INDEX);
+    BloomFilterIndexPB* meta = index_meta->mutable_bloom_filter_index();
+    meta->set_hash_strategy(CITY_HASH_64);
+    meta->set_algorithm(NGRAM_BLOOM_FILTER);
+
+    // write bloom filters
+    const TypeInfo* bf_typeinfo = get_scalar_type_info(OLAP_FIELD_TYPE_VARCHAR);
+    IndexedColumnWriterOptions options;
+    options.write_ordinal_index = true;
+    options.write_value_index = false;
+    options.encoding = PLAIN_ENCODING;
+    IndexedColumnWriter bf_writer(options, bf_typeinfo, file_writer);
+    RETURN_IF_ERROR(bf_writer.init());
+    for (auto& bf : _bfs) {
+        Slice data(bf->data(), bf->size());
+        bf_writer.add(&data);
+    }
+    RETURN_IF_ERROR(bf_writer.finish(meta->mutable_bloom_filter()));
+    return Status::OK();
+}
+
+uint64_t NGramBloomFilterIndexWriterImpl::size() {
+    uint64_t total_size = _bf_buffer_size;
+    total_size += _pool.total_allocated_bytes();
+    return total_size;
+}
+
 // TODO currently we don't support bloom filter index for tinyint/hll/float/double
 Status BloomFilterIndexWriter::create(const BloomFilterOptions& bf_options,
                                       const TypeInfo* type_info,
@@ -203,5 +260,23 @@ Status BloomFilterIndexWriter::create(const BloomFilterOptions& bf_options,
     return Status::OK();
 }
 
+Status NGramBloomFilterIndexWriterImpl::create(const BloomFilterOptions& bf_options,
+                                               const TypeInfo* typeinfo, uint8_t gram_size,
+                                               uint16_t gram_bf_size,
+                                               std::unique_ptr<BloomFilterIndexWriter>* res) {
+    FieldType type = typeinfo->type();
+    switch (type) {
+    case OLAP_FIELD_TYPE_CHAR:
+    case OLAP_FIELD_TYPE_VARCHAR:
+    case OLAP_FIELD_TYPE_STRING:
+        res->reset(new NGramBloomFilterIndexWriterImpl(bf_options, gram_size, gram_bf_size));
+        break;
+    default:
+        return Status::NotSupported("unsupported type for ngram bloom filter index:{}",
+                                    std::to_string(type));
+    }
+    return Status::OK();
+}
+
 } // namespace segment_v2
 } // namespace doris
diff --git a/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.h b/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.h
index 8b9a945e1..9cc02f4f6 100644
--- a/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.h
+++ b/be/src/olap/rowset/segment_v2/bloom_filter_index_writer.h
@@ -23,6 +23,8 @@
 #include "common/status.h"
 #include "gen_cpp/segment_v2.pb.h"
 #include "gutil/macros.h"
+#include "olap/itoken_extractor.h"
+#include "runtime/mem_pool.h"
 
 namespace doris {
 
@@ -38,7 +40,7 @@ struct BloomFilterOptions;
 
 class BloomFilterIndexWriter {
 public:
-    static Status create(const BloomFilterOptions& bf_options, const TypeInfo* type_info,
+    static Status create(const BloomFilterOptions& bf_options, const TypeInfo* typeinfo,
                          std::unique_ptr<BloomFilterIndexWriter>* res);
 
     BloomFilterIndexWriter() = default;
@@ -58,5 +60,30 @@ private:
     DISALLOW_COPY_AND_ASSIGN(BloomFilterIndexWriter);
 };
 
+class NGramBloomFilterIndexWriterImpl : public BloomFilterIndexWriter {
+public:
+    static Status create(const BloomFilterOptions& bf_options, const TypeInfo* typeinfo,
+                         uint8_t gram_size, uint16_t gram_bf_size,
+                         std::unique_ptr<BloomFilterIndexWriter>* res);
+
+    NGramBloomFilterIndexWriterImpl(const BloomFilterOptions& bf_options, uint8_t gram_size,
+                                    uint16_t bf_size);
+    void add_values(const void* values, size_t count) override;
+    void add_nulls(uint32_t) override {}
+    Status flush() override;
+    Status finish(io::FileWriter* file_writer, ColumnIndexMetaPB* index_meta) override;
+    uint64_t size() override;
+
+private:
+    BloomFilterOptions _bf_options;
+    uint8_t _gram_size;
+    uint16_t _bf_size;
+    MemPool _pool;
+    uint64_t _bf_buffer_size;
+    NgramTokenExtractor _token_extractor;
+    std::unique_ptr<BloomFilter> _bf;
+    std::vector<std::unique_ptr<BloomFilter>> _bfs;
+};
+
 } // namespace segment_v2
 } // namespace doris
diff --git a/be/src/olap/rowset/segment_v2/column_writer.cpp b/be/src/olap/rowset/segment_v2/column_writer.cpp
index 146a29443..df820b141 100644
--- a/be/src/olap/rowset/segment_v2/column_writer.cpp
+++ b/be/src/olap/rowset/segment_v2/column_writer.cpp
@@ -299,6 +299,7 @@ Status ScalarColumnWriter::init() {
         RETURN_IF_ERROR(
                 BitmapIndexWriter::create(get_field()->type_info(), &_bitmap_index_builder));
     }
+
     if (_opts.inverted_index) {
         RETURN_IF_ERROR(InvertedIndexColumnWriter::create(
                 get_field(), &_inverted_index_builder, _opts.meta->unique_id(),
@@ -307,8 +308,14 @@ Status ScalarColumnWriter::init() {
                 _file_writer->fs()));
     }
     if (_opts.need_bloom_filter) {
-        RETURN_IF_ERROR(BloomFilterIndexWriter::create(
-                BloomFilterOptions(), get_field()->type_info(), &_bloom_filter_index_builder));
+        if (_opts.is_ngram_bf_index) {
+            RETURN_IF_ERROR(NGramBloomFilterIndexWriterImpl::create(
+                    BloomFilterOptions(), get_field()->type_info(), _opts.gram_size,
+                    _opts.gram_bf_size, &_bloom_filter_index_builder));
+        } else {
+            RETURN_IF_ERROR(BloomFilterIndexWriter::create(
+                    BloomFilterOptions(), get_field()->type_info(), &_bloom_filter_index_builder));
+        }
     }
     return Status::OK();
 }
diff --git a/be/src/olap/rowset/segment_v2/column_writer.h b/be/src/olap/rowset/segment_v2/column_writer.h
index 748b8d725..5ea7ae654 100644
--- a/be/src/olap/rowset/segment_v2/column_writer.h
+++ b/be/src/olap/rowset/segment_v2/column_writer.h
@@ -51,6 +51,9 @@ struct ColumnWriterOptions {
     bool need_zone_map = false;
     bool need_bitmap_index = false;
     bool need_bloom_filter = false;
+    bool is_ngram_bf_index = false;
+    uint8_t gram_size;
+    uint16_t gram_bf_size;
     std::vector<const TabletIndex*> indexes;
     const TabletIndex* inverted_index = nullptr;
     std::string to_string() const {
diff --git a/be/src/olap/rowset/segment_v2/ngram_bloom_filter.cpp b/be/src/olap/rowset/segment_v2/ngram_bloom_filter.cpp
new file mode 100644
index 000000000..a761c5a6e
--- /dev/null
+++ b/be/src/olap/rowset/segment_v2/ngram_bloom_filter.cpp
@@ -0,0 +1,74 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "olap/rowset/segment_v2/ngram_bloom_filter.h"
+
+#include "util/cityhash102/city.h"
+#include "util/debug_util.h"
+
+namespace doris {
+namespace segment_v2 {
+
+static constexpr uint64_t SEED_GEN = 217728422;
+
+NGramBloomFilter::NGramBloomFilter(size_t size)
+        : _size(size),
+          words((size + sizeof(UnderType) - 1) / sizeof(UnderType)),
+          filter(words, 0) {}
+
+// for read
+Status NGramBloomFilter::init(const char* buf, uint32_t size, HashStrategyPB strategy) {
+    if (size == 0) {
+        return Status::InvalidArgument(strings::Substitute("invalid size:$0", size));
+    }
+    DCHECK(_size == size);
+
+    if (strategy != CITY_HASH_64) {
+        return Status::InvalidArgument(strings::Substitute("invalid strategy:$0", strategy));
+    }
+    words = (_size + sizeof(UnderType) - 1) / sizeof(UnderType);
+    filter.reserve(words);
+    const UnderType* from = reinterpret_cast<const UnderType*>(buf);
+    for (size_t i = 0; i < words; ++i) {
+        filter[i] = from[i];
+    }
+
+    return Status::OK();
+}
+
+void NGramBloomFilter::add_bytes(const char* data, uint32_t len) {
+    size_t hash1 = CityHash_v1_0_2::CityHash64WithSeed(data, len, 0);
+    size_t hash2 = CityHash_v1_0_2::CityHash64WithSeed(data, len, SEED_GEN);
+
+    for (size_t i = 0; i < HASH_FUNCTIONS; ++i) {
+        size_t pos = (hash1 + i * hash2 + i * i) % (8 * _size);
+        filter[pos / (8 * sizeof(UnderType))] |= (1ULL << (pos % (8 * sizeof(UnderType))));
+    }
+}
+
+bool NGramBloomFilter::contains(const BloomFilter& bf_) const {
+    const NGramBloomFilter& bf = static_cast<const NGramBloomFilter&>(bf_);
+    for (size_t i = 0; i < words; ++i) {
+        if ((filter[i] & bf.filter[i]) != bf.filter[i]) {
+            return false;
+        }
+    }
+    return true;
+}
+
+} // namespace segment_v2
+} // namespace doris
diff --git a/be/src/olap/rowset/segment_v2/ngram_bloom_filter.h b/be/src/olap/rowset/segment_v2/ngram_bloom_filter.h
new file mode 100644
index 000000000..c7684a796
--- /dev/null
+++ b/be/src/olap/rowset/segment_v2/ngram_bloom_filter.h
@@ -0,0 +1,50 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#pragma once
+
+#include "olap/rowset/segment_v2/bloom_filter.h"
+
+namespace doris {
+namespace segment_v2 {
+
+class NGramBloomFilter : public BloomFilter {
+public:
+    // Fixed hash function number
+    static const size_t HASH_FUNCTIONS = 2;
+    using UnderType = uint64_t;
+    NGramBloomFilter(size_t size);
+    void add_bytes(const char* data, uint32_t len) override;
+    bool contains(const BloomFilter& bf_) const override;
+    Status init(const char* buf, uint32_t size, HashStrategyPB strategy) override;
+    char* data() const override {
+        return reinterpret_cast<char*>(const_cast<uint64_t*>(filter.data()));
+    }
+    uint32_t size() const override { return _size; }
+    void add_hash(uint64_t) override {}
+    bool test_hash(uint64_t hash) const override { return true; }
+    bool has_null() const override { return true; }
+    bool is_ngram_bf() const override { return true; }
+
+private:
+    size_t _size;
+    size_t words;
+    std::vector<uint64_t> filter;
+};
+
+} // namespace segment_v2
+} // namespace doris
diff --git a/be/src/olap/rowset/segment_v2/segment.cpp b/be/src/olap/rowset/segment_v2/segment.cpp
index 6651e1879..cc489ef85 100644
--- a/be/src/olap/rowset/segment_v2/segment.cpp
+++ b/be/src/olap/rowset/segment_v2/segment.cpp
@@ -25,6 +25,7 @@
 #include "common/config.h"
 #include "common/logging.h" // LOG
 #include "io/cache/file_cache_manager.h"
+#include "io/fs/file_reader_options.h"
 #include "io/fs/file_system.h"
 #include "olap/iterators.h"
 #include "olap/rowset/segment_v2/empty_segment_iterator.h"
@@ -42,30 +43,26 @@ namespace segment_v2 {
 
 using io::FileCacheManager;
 
-Status Segment::open(io::FileSystemSPtr fs, const std::string& path, const std::string& cache_path,
-                     uint32_t segment_id, RowsetId rowset_id, TabletSchemaSPtr tablet_schema,
+Status Segment::open(io::FileSystemSPtr fs, const std::string& path, uint32_t segment_id,
+                     RowsetId rowset_id, TabletSchemaSPtr tablet_schema,
                      std::shared_ptr<Segment>* output) {
-    std::shared_ptr<Segment> segment(new Segment(segment_id, rowset_id, tablet_schema));
+    io::FileReaderOptions reader_options(io::cache_type_from_string(config::file_cache_type),
+                                         io::SegmentCachePathPolicy());
     io::FileReaderSPtr file_reader;
 #ifndef BE_TEST
-    RETURN_IF_ERROR(fs->open_file(path, &file_reader));
+    RETURN_IF_ERROR(fs->open_file(path, reader_options, &file_reader));
 #else
     // be ut use local file reader instead of remote file reader while use remote cache
     if (!config::file_cache_type.empty()) {
-        RETURN_IF_ERROR(io::global_local_filesystem()->open_file(path, &file_reader));
+        RETURN_IF_ERROR(
+                io::global_local_filesystem()->open_file(path, reader_options, &file_reader));
     } else {
-        RETURN_IF_ERROR(fs->open_file(path, &file_reader));
+        RETURN_IF_ERROR(fs->open_file(path, reader_options, &file_reader));
     }
 #endif
-    if (fs->type() != io::FileSystemType::LOCAL && !config::file_cache_type.empty()) {
-        io::FileCachePtr cache_reader = FileCacheManager::instance()->new_file_cache(
-                cache_path, config::file_cache_alive_time_sec, file_reader,
-                config::file_cache_type);
-        segment->_file_reader = cache_reader;
-        FileCacheManager::instance()->add_file_cache(cache_path, cache_reader);
-    } else {
-        segment->_file_reader = std::move(file_reader);
-    }
+
+    std::shared_ptr<Segment> segment(new Segment(segment_id, rowset_id, tablet_schema));
+    segment->_file_reader = std::move(file_reader);
     RETURN_IF_ERROR(segment->_open());
     *output = std::move(segment);
     return Status::OK();
diff --git a/be/src/olap/rowset/segment_v2/segment.h b/be/src/olap/rowset/segment_v2/segment.h
index b5e54dfa4..f2fa40bb4 100644
--- a/be/src/olap/rowset/segment_v2/segment.h
+++ b/be/src/olap/rowset/segment_v2/segment.h
@@ -61,9 +61,9 @@ using SegmentSharedPtr = std::shared_ptr<Segment>;
 // change finished, client should disable all cached Segment for old TabletSchema.
 class Segment : public std::enable_shared_from_this<Segment> {
 public:
-    static Status open(io::FileSystemSPtr fs, const std::string& path,
-                       const std::string& cache_path, uint32_t segment_id, RowsetId rowset_id,
-                       TabletSchemaSPtr tablet_schema, std::shared_ptr<Segment>* output);
+    static Status open(io::FileSystemSPtr fs, const std::string& path, uint32_t segment_id,
+                       RowsetId rowset_id, TabletSchemaSPtr tablet_schema,
+                       std::shared_ptr<Segment>* output);
 
     ~Segment();
 
diff --git a/be/src/olap/rowset/segment_v2/segment_iterator.cpp b/be/src/olap/rowset/segment_v2/segment_iterator.cpp
index b01ab46dd..e3f1b1325 100644
--- a/be/src/olap/rowset/segment_v2/segment_iterator.cpp
+++ b/be/src/olap/rowset/segment_v2/segment_iterator.cpp
@@ -32,6 +32,8 @@
 #include "util/doris_metrics.h"
 #include "util/key_util.h"
 #include "util/simd/bits.h"
+#include "vec/data_types/data_type_number.h"
+#include "vec/exprs/vliteral.h"
 
 namespace doris {
 using namespace ErrorCode;
@@ -168,6 +170,12 @@ Status SegmentIterator::init(const StorageReadOptions& opts) {
     }
     // Read options will not change, so that just resize here
     _block_rowids.resize(_opts.block_row_max);
+    if (!opts.column_predicates_except_leafnode_of_andnode.empty()) {
+        _col_preds_except_leafnode_of_andnode = opts.column_predicates_except_leafnode_of_andnode;
+    }
+    _remaining_vconjunct_root = opts.remaining_vconjunct_root;
+
+    _column_predicate_info.reset(new ColumnPredicateInfo());
     return Status::OK();
 }
 
@@ -283,6 +291,17 @@ Status SegmentIterator::_get_row_ranges_by_column_conditions() {
     if (_row_bitmap.isEmpty()) {
         return Status::OK();
     }
+
+    if (config::enable_index_apply_preds_except_leafnode_of_andnode) {
+        RETURN_IF_ERROR(_apply_index_except_leafnode_of_andnode());
+        if (_can_filter_by_preds_except_leafnode_of_andnode()) {
+            auto res = _execute_predicates_except_leafnode_of_andnode(_remaining_vconjunct_root);
+            if (res.ok() && _pred_except_leafnode_of_andnode_evaluate_result.size() == 1) {
+                _row_bitmap &= _pred_except_leafnode_of_andnode_evaluate_result[0];
+            }
+        }
+    }
+
     RETURN_IF_ERROR(_apply_bitmap_index());
 
     if (!_row_bitmap.isEmpty() &&
@@ -317,6 +336,7 @@ Status SegmentIterator::_get_row_ranges_from_conditions(RowRanges* condition_row
                 _opts.col_id_to_predicates[cid].get(), &column_bf_row_ranges));
         RowRanges::ranges_intersection(bf_row_ranges, column_bf_row_ranges, &bf_row_ranges);
     }
+
     size_t pre_size = condition_row_ranges->count();
     RowRanges::ranges_intersection(*condition_row_ranges, bf_row_ranges, condition_row_ranges);
     _opts.stats->rows_bf_filtered += (pre_size - condition_row_ranges->count());
@@ -350,6 +370,7 @@ Status SegmentIterator::_get_row_ranges_from_conditions(RowRanges* condition_row
 Status SegmentIterator::_apply_bitmap_index() {
     SCOPED_RAW_TIMER(&_opts.stats->bitmap_index_filter_timer);
     size_t input_rows = _row_bitmap.cardinality();
+
     std::vector<ColumnPredicate*> remaining_predicates;
 
     for (auto pred : _col_predicates) {
@@ -371,6 +392,163 @@ Status SegmentIterator::_apply_bitmap_index() {
     return Status::OK();
 }
 
+bool SegmentIterator::_is_literal_node(const TExprNodeType::type& node_type) {
+    switch (node_type) {
+    case TExprNodeType::BOOL_LITERAL:
+    case TExprNodeType::INT_LITERAL:
+    case TExprNodeType::LARGE_INT_LITERAL:
+    case TExprNodeType::FLOAT_LITERAL:
+    case TExprNodeType::DECIMAL_LITERAL:
+    case TExprNodeType::STRING_LITERAL:
+    case TExprNodeType::DATE_LITERAL:
+        return true;
+    default:
+        return false;
+    }
+}
+
+Status SegmentIterator::_execute_predicates_except_leafnode_of_andnode(vectorized::VExpr* expr) {
+    if (expr == nullptr) {
+        return Status::OK();
+    }
+
+    auto children = expr->children();
+    for (int i = 0; i < children.size(); ++i) {
+        RETURN_IF_ERROR(_execute_predicates_except_leafnode_of_andnode(children[i]));
+    }
+
+    auto node_type = expr->node_type();
+    if (node_type == TExprNodeType::SLOT_REF) {
+        _column_predicate_info->column_name = expr->expr_name();
+    } else if (_is_literal_node(node_type)) {
+        auto v_literal_expr = dynamic_cast<doris::vectorized::VLiteral*>(expr);
+        _column_predicate_info->query_value = v_literal_expr->value();
+    } else if (node_type == TExprNodeType::BINARY_PRED) {
+        _column_predicate_info->query_op = expr->fn().name.function_name;
+        // get child condition result in compound condtions
+        auto pred_result_sign = _gen_predicate_sign(_column_predicate_info.get());
+        _column_predicate_info.reset(new ColumnPredicateInfo());
+        if (_rowid_result_for_index.count(pred_result_sign) > 0 &&
+            _rowid_result_for_index[pred_result_sign].first) {
+            auto apply_reuslt = _rowid_result_for_index[pred_result_sign].second;
+            _pred_except_leafnode_of_andnode_evaluate_result.push_back(apply_reuslt);
+        }
+    } else if (node_type == TExprNodeType::COMPOUND_PRED) {
+        auto function_name = expr->fn().name.function_name;
+        // execute logic function
+        RETURN_IF_ERROR(_execute_compound_fn(function_name));
+    }
+
+    return Status::OK();
+}
+
+Status SegmentIterator::_execute_compound_fn(const std::string& function_name) {
+    auto size = _pred_except_leafnode_of_andnode_evaluate_result.size();
+    if (function_name == "and") {
+        if (size < 2) {
+            return Status::InternalError("execute and logic compute error.");
+        }
+        _pred_except_leafnode_of_andnode_evaluate_result.at(size - 2) &=
+                _pred_except_leafnode_of_andnode_evaluate_result.at(size - 1);
+        _pred_except_leafnode_of_andnode_evaluate_result.pop_back();
+    } else if (function_name == "or") {
+        if (size < 2) {
+            return Status::InternalError("execute or logic compute error.");
+        }
+        _pred_except_leafnode_of_andnode_evaluate_result.at(size - 2) |=
+                _pred_except_leafnode_of_andnode_evaluate_result.at(size - 1);
+        _pred_except_leafnode_of_andnode_evaluate_result.pop_back();
+    } else if (function_name == "not") {
+        if (size < 1) {
+            return Status::InternalError("execute not logic compute error.");
+        }
+        roaring::Roaring tmp = _row_bitmap;
+        tmp -= _pred_except_leafnode_of_andnode_evaluate_result.at(size - 1);
+        _pred_except_leafnode_of_andnode_evaluate_result.at(size - 1) = tmp;
+    }
+    return Status::OK();
+}
+
+bool SegmentIterator::_can_filter_by_preds_except_leafnode_of_andnode() {
+    for (auto pred : _col_preds_except_leafnode_of_andnode) {
+        if (!_check_apply_by_bitmap_index(pred)) {
+            return false;
+        }
+    }
+    return true;
+}
+
+bool SegmentIterator::_check_apply_by_bitmap_index(ColumnPredicate* pred) {
+    int32_t unique_id = _schema.unique_id(pred->column_id());
+    if (_bitmap_index_iterators.count(unique_id) < 1 ||
+        _bitmap_index_iterators[unique_id] == nullptr) {
+        // no bitmap index for this column
+        return false;
+    }
+    return true;
+}
+
+Status SegmentIterator::_apply_bitmap_index_except_leafnode_of_andnode(
+        ColumnPredicate* pred, roaring::Roaring* output_result) {
+    int32_t unique_id = _schema.unique_id(pred->column_id());
+    RETURN_IF_ERROR(pred->evaluate(_bitmap_index_iterators[unique_id], _segment->num_rows(),
+                                   output_result));
+    return Status::OK();
+}
+
+Status SegmentIterator::_apply_index_except_leafnode_of_andnode() {
+    for (auto pred : _col_preds_except_leafnode_of_andnode) {
+        auto pred_type = pred->type();
+        bool is_support = pred_type == PredicateType::EQ || pred_type == PredicateType::NE ||
+                          pred_type == PredicateType::LT || pred_type == PredicateType::LE ||
+                          pred_type == PredicateType::GT || pred_type == PredicateType::GE;
+        if (!is_support) {
+            continue;
+        }
+
+        bool can_apply_by_bitmap_index = _check_apply_by_bitmap_index(pred);
+        roaring::Roaring bitmap = _row_bitmap;
+        Status res = Status::OK();
+        if (can_apply_by_bitmap_index) {
+            res = _apply_bitmap_index_except_leafnode_of_andnode(pred, &bitmap);
+        } else {
+            continue;
+        }
+
+        if (!res.ok()) {
+            LOG(WARNING) << "failed to evaluate index"
+                         << ", column predicate type: " << pred->pred_type_string(pred->type())
+                         << ", error msg: " << res.code_as_string();
+            return res;
+        }
+
+        std::string pred_result_sign = _gen_predicate_sign(pred);
+        _rowid_result_for_index.emplace(
+                std::make_pair(pred_result_sign, std::make_pair(true, bitmap)));
+    }
+
+    return Status::OK();
+}
+
+std::string SegmentIterator::_gen_predicate_sign(ColumnPredicate* predicate) {
+    std::string pred_result_sign;
+
+    auto column_desc = _schema.column(predicate->column_id());
+    auto pred_type = predicate->type();
+    auto predicate_params = predicate->predicate_params();
+    pred_result_sign = column_desc->name() + "_" + predicate->pred_type_string(pred_type) + "_" +
+                       predicate_params->value;
+
+    return pred_result_sign;
+}
+
+std::string SegmentIterator::_gen_predicate_sign(ColumnPredicateInfo* predicate_info) {
+    std::string pred_result_sign;
+    pred_result_sign = predicate_info->column_name + "_" + predicate_info->query_op + "_" +
+                       predicate_info->query_value;
+    return pred_result_sign;
+}
+
 Status SegmentIterator::_init_return_column_iterators() {
     if (_cur_rowid >= num_rows()) {
         return Status::OK();
@@ -861,6 +1039,10 @@ Status SegmentIterator::_read_columns_by_index(uint32_t nrows_read_limit, uint32
         } else {
             nrows_read += rows_to_read;
         }
+
+        auto next_range = RowRanges::create_single(range_from, range_to);
+        auto next_bm_in_row_bitmap = RowRanges::ranges_to_roaring(next_range);
+        _split_row_ranges.emplace_back(next_bm_in_row_bitmap);
         // if _opts.read_orderby_key_reverse is true, only read one range for fast reverse purpose
     } while (nrows_read < nrows_read_limit && !_opts.read_orderby_key_reverse);
     return Status::OK();
@@ -1006,6 +1188,7 @@ Status SegmentIterator::next_batch(vectorized::Block* block) {
         // read 100 rows to estimate average row size
         nrows_read_limit = 100;
     }
+    _split_row_ranges.clear();
     _read_columns_by_index(nrows_read_limit, _current_batch_rows_read,
                            _lazy_materialization_read || _opts.record_rowids);
 
@@ -1026,6 +1209,7 @@ Status SegmentIterator::next_batch(vectorized::Block* block) {
 
     if (!_is_need_vec_eval && !_is_need_short_eval) {
         _output_non_pred_columns(block);
+        _output_index_result_column(nullptr, 0, block);
     } else {
         _convert_dict_code_for_predicate_if_necessary();
         uint16_t selected_size = _current_batch_rows_read;
@@ -1082,6 +1266,7 @@ Status SegmentIterator::next_batch(vectorized::Block* block) {
             RETURN_IF_ERROR(_output_column_by_sel_idx(block, _first_read_column_ids, sel_rowid_idx,
                                                       selected_size));
         }
+        _output_index_result_column(sel_rowid_idx, selected_size, block);
     }
 
     // shrink char_type suffix zero data
@@ -1106,6 +1291,53 @@ Status SegmentIterator::next_batch(vectorized::Block* block) {
     return Status::OK();
 }
 
+void SegmentIterator::_output_index_result_column(uint16_t* sel_rowid_idx, uint16_t select_size,
+                                                  vectorized::Block* block) {
+    if (block->rows() == 0) {
+        return;
+    }
+
+    for (auto iter : _rowid_result_for_index) {
+        block->insert({vectorized::ColumnUInt8::create(),
+                       std::make_shared<vectorized::DataTypeUInt8>(), iter.first});
+        if (!iter.second.first) {
+            // predicate not in compound query
+            continue;
+        }
+        _build_index_result_column(sel_rowid_idx, select_size, block, iter.first,
+                                   iter.second.second);
+    }
+}
+
+void SegmentIterator::_build_index_result_column(uint16_t* sel_rowid_idx, uint16_t select_size,
+                                                 vectorized::Block* block,
+                                                 const std::string& pred_result_sign,
+                                                 const roaring::Roaring& index_result) {
+    auto index_result_column = vectorized::ColumnUInt8::create();
+    vectorized::ColumnUInt8::Container& vec_match_pred = index_result_column->get_data();
+    vec_match_pred.resize(block->rows());
+    size_t idx_in_block = 0;
+    size_t idx_in_row_range = 0;
+    size_t idx_in_selected = 0;
+    for (auto origin_row_range : _split_row_ranges) {
+        for (auto rowid : origin_row_range) {
+            if (sel_rowid_idx == nullptr || (idx_in_selected < select_size &&
+                                             idx_in_row_range == sel_rowid_idx[idx_in_selected])) {
+                if (index_result.contains(rowid)) {
+                    vec_match_pred[idx_in_block++] = true;
+                } else {
+                    vec_match_pred[idx_in_block++] = false;
+                }
+                idx_in_selected++;
+            }
+            idx_in_row_range++;
+        }
+    }
+    assert(block->rows() == vec_match_pred.size());
+    auto index_result_position = block->get_position_by_name(pred_result_sign);
+    block->replace_by_position(index_result_position, std::move(index_result_column));
+}
+
 void SegmentIterator::_convert_dict_code_for_predicate_if_necessary() {
     for (auto predicate : _short_cir_eval_predicate) {
         _convert_dict_code_for_predicate_if_necessary_impl(predicate);
diff --git a/be/src/olap/rowset/segment_v2/segment_iterator.h b/be/src/olap/rowset/segment_v2/segment_iterator.h
index 89900a863..d1ac74cff 100644
--- a/be/src/olap/rowset/segment_v2/segment_iterator.h
+++ b/be/src/olap/rowset/segment_v2/segment_iterator.h
@@ -30,6 +30,7 @@
 #include "olap/rowset/segment_v2/segment.h"
 #include "olap/schema.h"
 #include "util/file_cache.h"
+#include "vec/exprs/vexpr.h"
 
 namespace doris {
 
@@ -47,6 +48,13 @@ class BitmapIndexIterator;
 class BitmapIndexReader;
 class ColumnIterator;
 
+struct ColumnPredicateInfo {
+    ColumnPredicateInfo() = default;
+    std::string column_name;
+    std::string query_value;
+    std::string query_op;
+};
+
 class SegmentIterator : public RowwiseIterator {
 public:
     SegmentIterator(std::shared_ptr<Segment> segment, const Schema& _schema);
@@ -115,6 +123,15 @@ private:
     Status _get_row_ranges_from_conditions(RowRanges* condition_row_ranges);
     Status _apply_bitmap_index();
 
+    Status _apply_index_except_leafnode_of_andnode();
+    Status _apply_bitmap_index_except_leafnode_of_andnode(ColumnPredicate* pred,
+                                                          roaring::Roaring* output_result);
+
+    bool _can_filter_by_preds_except_leafnode_of_andnode();
+    Status _execute_predicates_except_leafnode_of_andnode(vectorized::VExpr* expr);
+    Status _execute_compound_fn(const std::string& function_name);
+    bool _is_literal_node(const TExprNodeType::type& node_type);
+
     void _init_lazy_materialization();
     void _vec_init_lazy_materialization();
     // TODO: Fix Me
@@ -166,6 +183,18 @@ private:
 
     void _update_max_row(const vectorized::Block* block);
 
+    bool _check_apply_by_bitmap_index(ColumnPredicate* pred);
+
+    std::string _gen_predicate_sign(ColumnPredicate* predicate);
+    std::string _gen_predicate_sign(ColumnPredicateInfo* predicate_info);
+
+    void _build_index_result_column(uint16_t* sel_rowid_idx, uint16_t select_size,
+                                    vectorized::Block* block, const std::string& pred_result_sign,
+                                    const roaring::Roaring& index_result);
+    void _output_index_result_column(uint16_t* sel_rowid_idx, uint16_t select_size,
+                                     vectorized::Block* block);
+
+private:
     class BitmapRangeIterator;
     class BackwardBitmapRangeIterator;
 
@@ -178,6 +207,9 @@ private:
     std::map<int32_t, BitmapIndexIterator*> _bitmap_index_iterators;
     // after init(), `_row_bitmap` contains all rowid to scan
     roaring::Roaring _row_bitmap;
+    // "column_name+operator+value-> <in_compound_query, rowid_result>
+    std::unordered_map<std::string, std::pair<bool, roaring::Roaring> > _rowid_result_for_index;
+    std::vector<roaring::Roaring> _split_row_ranges;
     // an iterator for `_row_bitmap` that can be used to extract row range to scan
     std::unique_ptr<BitmapRangeIterator> _range_iter;
     // the next rowid to read
@@ -221,6 +253,10 @@ private:
     StorageReadOptions _opts;
     // make a copy of `_opts.column_predicates` in order to make local changes
     std::vector<ColumnPredicate*> _col_predicates;
+    std::vector<ColumnPredicate*> _col_preds_except_leafnode_of_andnode;
+    doris::vectorized::VExpr* _remaining_vconjunct_root;
+    std::vector<roaring::Roaring> _pred_except_leafnode_of_andnode_evaluate_result;
+    std::unique_ptr<ColumnPredicateInfo> _column_predicate_info;
 
     // row schema of the key to seek
     // only used in `_get_row_ranges_by_keys`
diff --git a/be/src/olap/rowset/segment_v2/segment_writer.cpp b/be/src/olap/rowset/segment_v2/segment_writer.cpp
index 9776761aa..df7f26c34 100644
--- a/be/src/olap/rowset/segment_v2/segment_writer.cpp
+++ b/be/src/olap/rowset/segment_v2/segment_writer.cpp
@@ -112,6 +112,14 @@ Status SegmentWriter::init(const std::vector<uint32_t>& col_ids, bool has_key) {
         // and not support zone map for array type and jsonb type.
         opts.need_zone_map = column.is_key() || _tablet_schema->keys_type() != KeysType::AGG_KEYS;
         opts.need_bloom_filter = column.is_bf_column();
+        auto* tablet_index = _tablet_schema->get_ngram_bf_index(column.unique_id());
+        if (tablet_index) {
+            opts.need_bloom_filter = true;
+            opts.is_ngram_bf_index = true;
+            opts.gram_size = tablet_index->get_gram_size();
+            opts.gram_bf_size = tablet_index->get_gram_bf_size();
+        }
+
         opts.need_bitmap_index = column.has_bitmap_index();
         bool skip_inverted_index = false;
         if (_opts.rowset_ctx != nullptr) {
diff --git a/be/src/olap/tablet_meta.cpp b/be/src/olap/tablet_meta.cpp
index 99c752325..e3d4c6f3f 100644
--- a/be/src/olap/tablet_meta.cpp
+++ b/be/src/olap/tablet_meta.cpp
@@ -165,6 +165,13 @@ TabletMeta::TabletMeta(int64_t table_id, int64_t partition_id, int64_t tablet_id
                         column->set_has_bitmap_index(true);
                         break;
                     }
+                } else if (index.index_type == TIndexType::type::BLOOMFILTER ||
+                           index.index_type == TIndexType::type::NGRAM_BF) {
+                    DCHECK_EQ(index.columns.size(), 1);
+                    if (iequal(tcolumn.column_name, index.columns[0])) {
+                        column->set_is_bf_column(true);
+                        break;
+                    }
                 }
             }
         }
@@ -195,7 +202,11 @@ TabletMeta::TabletMeta(int64_t table_id, int64_t partition_id, int64_t tablet_id
             case TIndexType::BLOOMFILTER:
                 index_pb->set_index_type(IndexType::BLOOMFILTER);
                 break;
+            case TIndexType::NGRAM_BF:
+                index_pb->set_index_type(IndexType::NGRAM_BF);
+                break;
             }
+
             if (index.__isset.properties) {
                 auto properties = index_pb->mutable_properties();
                 for (auto kv : index.properties) {
@@ -284,6 +295,7 @@ void TabletMeta::init_column_from_tcolumn(uint32_t unique_id, const TColumn& tco
     if (tcolumn.__isset.is_bloom_filter_column) {
         column->set_is_bf_column(tcolumn.is_bloom_filter_column);
     }
+
     if (tcolumn.column_type.type == TPrimitiveType::ARRAY) {
         ColumnPB* children_column = column->add_children_columns();
         init_column_from_tcolumn(0, tcolumn.children_column[0], children_column);
diff --git a/be/src/olap/tablet_schema.cpp b/be/src/olap/tablet_schema.cpp
index d68480645..55f030cbb 100644
--- a/be/src/olap/tablet_schema.cpp
+++ b/be/src/olap/tablet_schema.cpp
@@ -402,6 +402,7 @@ void TabletColumn::init_from_pb(const ColumnPB& column) {
     if (column.has_visible()) {
         _visible = column.visible();
     }
+
     if (_type == FieldType::OLAP_FIELD_TYPE_ARRAY) {
         DCHECK(column.children_columns_size() == 1) << "ARRAY type has more than 1 children types.";
         TabletColumn child_column;
@@ -479,6 +480,9 @@ void TabletIndex::init_from_thrift(const TOlapTableIndex& index,
     case TIndexType::BLOOMFILTER:
         _index_type = IndexType::BLOOMFILTER;
         break;
+    case TIndexType::NGRAM_BF:
+        _index_type = IndexType::NGRAM_BF;
+        break;
     }
     if (index.__isset.properties) {
         for (auto kv : index.properties) {
@@ -811,6 +815,36 @@ const TabletIndex* TabletSchema::get_inverted_index(int32_t col_unique_id) const
     return nullptr;
 }
 
+bool TabletSchema::has_ngram_bf_index(int32_t col_unique_id) const {
+    // TODO use more efficient impl
+    for (size_t i = 0; i < _indexes.size(); i++) {
+        if (_indexes[i].index_type() == IndexType::NGRAM_BF) {
+            for (int32_t id : _indexes[i].col_unique_ids()) {
+                if (id == col_unique_id) {
+                    return true;
+                }
+            }
+        }
+    }
+
+    return false;
+}
+
+const TabletIndex* TabletSchema::get_ngram_bf_index(int32_t col_unique_id) const {
+    // TODO use more efficient impl
+    for (size_t i = 0; i < _indexes.size(); i++) {
+        if (_indexes[i].index_type() == IndexType::NGRAM_BF) {
+            for (int32_t id : _indexes[i].col_unique_ids()) {
+                if (id == col_unique_id) {
+                    return &(_indexes[i]);
+                }
+            }
+        }
+    }
+
+    return nullptr;
+}
+
 vectorized::Block TabletSchema::create_block(
         const std::vector<uint32_t>& return_columns,
         const std::unordered_set<uint32_t>* tablet_columns_need_convert_null) const {
diff --git a/be/src/olap/tablet_schema.h b/be/src/olap/tablet_schema.h
index 20b05d180..bbbbfc896 100644
--- a/be/src/olap/tablet_schema.h
+++ b/be/src/olap/tablet_schema.h
@@ -69,8 +69,11 @@ public:
                                                             std::string suffix) const;
     int precision() const { return _precision; }
     int frac() const { return _frac; }
-    bool visible() const { return _visible; }
-    // Add a sub column.
+    inline bool visible() const { return _visible; }
+
+    /**
+     * Add a sub column.
+     */
     void add_sub_column(TabletColumn& sub_column);
 
     uint32_t get_subtype_count() const { return _sub_column_count; }
@@ -129,6 +132,20 @@ public:
     const IndexType index_type() const { return _index_type; }
     const vector<int32_t>& col_unique_ids() const { return _col_unique_ids; }
     const std::map<string, string>& properties() const { return _properties; }
+    int32_t get_gram_size() const {
+        if (_properties.count("gram_size")) {
+            return std::stoi(_properties.at("gram_size"));
+        }
+
+        return 0;
+    }
+    int32_t get_gram_bf_size() const {
+        if (_properties.count("bf_size")) {
+            return std::stoi(_properties.at("bf_size"));
+        }
+
+        return 0;
+    }
 
 private:
     int64_t _index_id;
@@ -186,6 +203,8 @@ public:
     std::vector<const TabletIndex*> get_indexes_for_column(int32_t col_unique_id) const;
     bool has_inverted_index(int32_t col_unique_id) const;
     const TabletIndex* get_inverted_index(int32_t col_unique_id) const;
+    bool has_ngram_bf_index(int32_t col_unique_id) const;
+    const TabletIndex* get_ngram_bf_index(int32_t col_unique_id) const;
     void update_indexes_from_thrift(const std::vector<doris::TOlapTableIndex>& indexes);
 
     int32_t schema_version() const { return _schema_version; }
@@ -250,4 +269,4 @@ bool operator!=(const TabletSchema& a, const TabletSchema& b);
 
 using TabletSchemaSPtr = std::shared_ptr<TabletSchema>;
 
-} // namespace doris
\ No newline at end of file
+} // namespace doris
diff --git a/be/src/runtime/jsonb_value.h b/be/src/runtime/jsonb_value.h
index bdb5ba497..1e03518d9 100644
--- a/be/src/runtime/jsonb_value.h
+++ b/be/src/runtime/jsonb_value.h
@@ -22,7 +22,7 @@
 #include "util/cpu_info.h"
 #include "util/hash_util.hpp"
 #include "util/jsonb_error.h"
-#include "util/jsonb_parser.h"
+#include "util/jsonb_parser_simd.h"
 #include "util/jsonb_utils.h"
 #include "vec/common/string_ref.h"
 
@@ -38,7 +38,7 @@ struct JsonBinaryValue {
     // default nullprt and size 0 for invalid or NULL value
     const char* ptr = nullptr;
     size_t len = 0;
-    JsonbParser parser;
+    JsonbParserSIMD parser;
 
     JsonBinaryValue() : ptr(nullptr), len(0) {}
     JsonBinaryValue(char* ptr, int len) { from_json_string(const_cast<const char*>(ptr), len); }
diff --git a/be/src/service/http_service.cpp b/be/src/service/http_service.cpp
index b62e54e6b..b7acb1fa6 100644
--- a/be/src/service/http_service.cpp
+++ b/be/src/service/http_service.cpp
@@ -24,8 +24,10 @@
 #include "http/action/config_action.h"
 #include "http/action/download_action.h"
 #include "http/action/health_action.h"
+#include "http/action/jeprofile_actions.h"
 #include "http/action/meta_action.h"
 #include "http/action/metrics_action.h"
+#include "http/action/pad_rowset_action.h"
 #include "http/action/pprof_actions.h"
 #include "http/action/reload_tablet_action.h"
 #include "http/action/reset_rpc_channel_action.h"
@@ -117,6 +119,9 @@ Status HttpService::start() {
     // register pprof actions
     PprofActions::setup(_env, _ev_http_server.get(), _pool);
 
+    // register jeprof actions
+    JeprofileActions::setup(_env, _ev_http_server.get(), _pool);
+
     // register metrics
     {
         auto action = _pool.add(new MetricsAction(DorisMetrics::instance()->metric_registry()));
@@ -180,6 +185,9 @@ Status HttpService::start() {
     _ev_http_server->register_handler(HttpMethod::POST, "/api/check_tablet_segment_lost",
                                       check_tablet_segment_action);
 
+    PadRowsetAction* pad_rowset_action = _pool.add(new PadRowsetAction());
+    _ev_http_server->register_handler(HttpMethod::POST, "api/pad_rowset", pad_rowset_action);
+
     _ev_http_server->start();
     return Status::OK();
 }
diff --git a/be/src/util/CMakeLists.txt b/be/src/util/CMakeLists.txt
index 067d7442f..4b67e12ce 100644
--- a/be/src/util/CMakeLists.txt
+++ b/be/src/util/CMakeLists.txt
@@ -106,6 +106,7 @@ set(UTIL_FILES
   hdfs_util.cpp
   time_lut.cpp
   topn_counter.cpp
+  cityhash102/city.cc
   tuple_row_zorder_compare.cpp
   telemetry/telemetry.cpp
   telemetry/brpc_carrier.cpp
diff --git a/be/src/util/cityhash102/city.cc b/be/src/util/cityhash102/city.cc
new file mode 100644
index 000000000..6ad5c37c6
--- /dev/null
+++ b/be/src/util/cityhash102/city.cc
@@ -0,0 +1,481 @@
+// Copyright (c) 2011 Google, Inc.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+//
+// CityHash, by Geoff Pike and Jyrki Alakuijala
+//
+// This file provides CityHash64() and related functions.
+//
+// It's probably possible to create even faster hash functions by
+// writing a program that systematically explores some of the space of
+// possible hash functions, by using SIMD instructions, or by
+// compromising on hash quality.
+
+#include "config.h"
+#include "city.h"
+
+#include <algorithm>
+#include <string.h>  // for memcpy and memset
+
+using namespace std;
+
+
+#if !defined(WORDS_BIGENDIAN)
+
+#define uint32_in_expected_order(x) (x)
+#define uint64_in_expected_order(x) (x)
+
+#else
+
+#ifdef _MSC_VER
+#include <stdlib.h>
+#define bswap_32(x) _byteswap_ulong(x)
+#define bswap_64(x) _byteswap_uint64(x)
+
+#elif defined(__APPLE__)
+// Mac OS X / Darwin features
+#include <libkern/OSByteOrder.h>
+#define bswap_32(x) OSSwapInt32(x)
+#define bswap_64(x) OSSwapInt64(x)
+
+#else
+#include <byteswap.h>
+#endif
+
+#define uint32_in_expected_order(x) (bswap_32(x))
+#define uint64_in_expected_order(x) (bswap_64(x))
+
+#endif  // WORDS_BIGENDIAN
+
+#if !defined(LIKELY)
+#if HAVE_BUILTIN_EXPECT
+#define LIKELY(x) (__builtin_expect(!!(x), 1))
+#else
+#define LIKELY(x) (x)
+#endif
+#endif
+
+namespace CityHash_v1_0_2
+{
+
+static uint64 UNALIGNED_LOAD64(const char *p) {
+  uint64 result;
+  memcpy(&result, p, sizeof(result));
+  return result;
+}
+
+static uint32 UNALIGNED_LOAD32(const char *p) {
+  uint32 result;
+  memcpy(&result, p, sizeof(result));
+  return result;
+}
+
+static uint64 Fetch64(const char *p) {
+  return uint64_in_expected_order(UNALIGNED_LOAD64(p));
+}
+
+static uint32 Fetch32(const char *p) {
+  return uint32_in_expected_order(UNALIGNED_LOAD32(p));
+}
+
+// Some primes between 2^63 and 2^64 for various uses.
+static const uint64 k0 = 0xc3a5c85c97cb3127ULL;
+static const uint64 k1 = 0xb492b66fbe98f273ULL;
+static const uint64 k2 = 0x9ae16a3b2f90404fULL;
+static const uint64 k3 = 0xc949d7c7509e6557ULL;
+
+// Bitwise right rotate.  Normally this will compile to a single
+// instruction, especially if the shift is a manifest constant.
+static uint64 Rotate(uint64 val, int shift) {
+  // Avoid shifting by 64: doing so yields an undefined result.
+  return shift == 0 ? val : ((val >> shift) | (val << (64 - shift)));
+}
+
+// Equivalent to Rotate(), but requires the second arg to be non-zero.
+// On x86-64, and probably others, it's possible for this to compile
+// to a single instruction if both args are already in registers.
+static uint64 RotateByAtLeast1(uint64 val, int shift) {
+  return (val >> shift) | (val << (64 - shift));
+}
+
+static uint64 ShiftMix(uint64 val) {
+  return val ^ (val >> 47);
+}
+
+static uint64 HashLen16(uint64 u, uint64 v) {
+  return Hash128to64(uint128(u, v));
+}
+
+static uint64 HashLen0to16(const char *s, size_t len) {
+  if (len > 8) {
+    uint64 a = Fetch64(s);
+    uint64 b = Fetch64(s + len - 8);
+    return HashLen16(a, RotateByAtLeast1(b + len, len)) ^ b;
+  }
+  if (len >= 4) {
+    uint64 a = Fetch32(s);
+    return HashLen16(len + (a << 3), Fetch32(s + len - 4));
+  }
+  if (len > 0) {
+    uint8 a = s[0];
+    uint8 b = s[len >> 1];
+    uint8 c = s[len - 1];
+    uint32 y = static_cast<uint32>(a) + (static_cast<uint32>(b) << 8);
+    uint32 z = len + (static_cast<uint32>(c) << 2);
+    return ShiftMix(y * k2 ^ z * k3) * k2;
+  }
+  return k2;
+}
+
+// This probably works well for 16-byte strings as well, but it may be overkill
+// in that case.
+static uint64 HashLen17to32(const char *s, size_t len) {
+  uint64 a = Fetch64(s) * k1;
+  uint64 b = Fetch64(s + 8);
+  uint64 c = Fetch64(s + len - 8) * k2;
+  uint64 d = Fetch64(s + len - 16) * k0;
+  return HashLen16(Rotate(a - b, 43) + Rotate(c, 30) + d,
+                   a + Rotate(b ^ k3, 20) - c + len);
+}
+
+// Return a 16-byte hash for 48 bytes.  Quick and dirty.
+// Callers do best to use "random-looking" values for a and b.
+static pair<uint64, uint64> WeakHashLen32WithSeeds(
+    uint64 w, uint64 x, uint64 y, uint64 z, uint64 a, uint64 b) {
+  a += w;
+  b = Rotate(b + a + z, 21);
+  uint64 c = a;
+  a += x;
+  a += y;
+  b += Rotate(a, 44);
+  return make_pair(a + z, b + c);
+}
+
+// Return a 16-byte hash for s[0] ... s[31], a, and b.  Quick and dirty.
+static pair<uint64, uint64> WeakHashLen32WithSeeds(
+    const char* s, uint64 a, uint64 b) {
+  return WeakHashLen32WithSeeds(Fetch64(s),
+                                Fetch64(s + 8),
+                                Fetch64(s + 16),
+                                Fetch64(s + 24),
+                                a,
+                                b);
+}
+
+// Return an 8-byte hash for 33 to 64 bytes.
+static uint64 HashLen33to64(const char *s, size_t len) {
+  uint64 z = Fetch64(s + 24);
+  uint64 a = Fetch64(s) + (len + Fetch64(s + len - 16)) * k0;
+  uint64 b = Rotate(a + z, 52);
+  uint64 c = Rotate(a, 37);
+  a += Fetch64(s + 8);
+  c += Rotate(a, 7);
+  a += Fetch64(s + 16);
+  uint64 vf = a + z;
+  uint64 vs = b + Rotate(a, 31) + c;
+  a = Fetch64(s + 16) + Fetch64(s + len - 32);
+  z = Fetch64(s + len - 8);
+  b = Rotate(a + z, 52);
+  c = Rotate(a, 37);
+  a += Fetch64(s + len - 24);
+  c += Rotate(a, 7);
+  a += Fetch64(s + len - 16);
+  uint64 wf = a + z;
+  uint64 ws = b + Rotate(a, 31) + c;
+  uint64 r = ShiftMix((vf + ws) * k2 + (wf + vs) * k0);
+  return ShiftMix(r * k0 + vs) * k2;
+}
+
+uint64 CityHash64(const char *s, size_t len) {
+  if (len <= 32) {
+    if (len <= 16) {
+      return HashLen0to16(s, len);
+    } else {
+      return HashLen17to32(s, len);
+    }
+  } else if (len <= 64) {
+    return HashLen33to64(s, len);
+  }
+
+  // For strings over 64 bytes we hash the end first, and then as we
+  // loop we keep 56 bytes of state: v, w, x, y, and z.
+  uint64 x = Fetch64(s);
+  uint64 y = Fetch64(s + len - 16) ^ k1;
+  uint64 z = Fetch64(s + len - 56) ^ k0;
+  pair<uint64, uint64> v = WeakHashLen32WithSeeds(s + len - 64, len, y);
+  pair<uint64, uint64> w = WeakHashLen32WithSeeds(s + len - 32, len * k1, k0);
+  z += ShiftMix(v.second) * k1;
+  x = Rotate(z + x, 39) * k1;
+  y = Rotate(y, 33) * k1;
+
+  // Decrease len to the nearest multiple of 64, and operate on 64-byte chunks.
+  len = (len - 1) & ~static_cast<size_t>(63);
+  do {
+    x = Rotate(x + y + v.first + Fetch64(s + 16), 37) * k1;
+    y = Rotate(y + v.second + Fetch64(s + 48), 42) * k1;
+    x ^= w.second;
+    y ^= v.first;
+    z = Rotate(z ^ w.first, 33);
+    v = WeakHashLen32WithSeeds(s, v.second * k1, x + w.first);
+    w = WeakHashLen32WithSeeds(s + 32, z + w.second, y);
+    std::swap(z, x);
+    s += 64;
+    len -= 64;
+  } while (len != 0);
+  return HashLen16(HashLen16(v.first, w.first) + ShiftMix(y) * k1 + z,
+                   HashLen16(v.second, w.second) + x);
+}
+
+uint64 CityHash64WithSeed(const char *s, size_t len, uint64 seed) {
+  return CityHash64WithSeeds(s, len, k2, seed);
+}
+
+uint64 CityHash64WithSeeds(const char *s, size_t len,
+                           uint64 seed0, uint64 seed1) {
+  return HashLen16(CityHash64(s, len) - seed0, seed1);
+}
+
+// A subroutine for CityHash128().  Returns a decent 128-bit hash for strings
+// of any length representable in ssize_t.  Based on City and Murmur.
+static uint128 CityMurmur(const char *s, size_t len, uint128 seed) {
+  uint64 a = Uint128Low64(seed);
+  uint64 b = Uint128High64(seed);
+  uint64 c = 0;
+  uint64 d = 0;
+  ssize_t l = len - 16;
+  if (l <= 0) {  // len <= 16
+    a = ShiftMix(a * k1) * k1;
+    c = b * k1 + HashLen0to16(s, len);
+    d = ShiftMix(a + (len >= 8 ? Fetch64(s) : c));
+  } else {  // len > 16
+    c = HashLen16(Fetch64(s + len - 8) + k1, a);
+    d = HashLen16(b + len, c + Fetch64(s + len - 16));
+    a += d;
+    do {
+      a ^= ShiftMix(Fetch64(s) * k1) * k1;
+      a *= k1;
+      b ^= a;
+      c ^= ShiftMix(Fetch64(s + 8) * k1) * k1;
+      c *= k1;
+      d ^= c;
+      s += 16;
+      l -= 16;
+    } while (l > 0);
+  }
+  a = HashLen16(a, c);
+  b = HashLen16(d, b);
+  return uint128(a ^ b, HashLen16(b, a));
+}
+
+uint128 CityHash128WithSeed(const char *s, size_t len, uint128 seed) {
+  if (len < 128) {
+    return CityMurmur(s, len, seed);
+  }
+
+  // We expect len >= 128 to be the common case.  Keep 56 bytes of state:
+  // v, w, x, y, and z.
+  pair<uint64, uint64> v, w;
+  uint64 x = Uint128Low64(seed);
+  uint64 y = Uint128High64(seed);
+  uint64 z = len * k1;
+  v.first = Rotate(y ^ k1, 49) * k1 + Fetch64(s);
+  v.second = Rotate(v.first, 42) * k1 + Fetch64(s + 8);
+  w.first = Rotate(y + z, 35) * k1 + x;
+  w.second = Rotate(x + Fetch64(s + 88), 53) * k1;
+
+  // This is the same inner loop as CityHash64(), manually unrolled.
+  do {
+    x = Rotate(x + y + v.first + Fetch64(s + 16), 37) * k1;
+    y = Rotate(y + v.second + Fetch64(s + 48), 42) * k1;
+    x ^= w.second;
+    y ^= v.first;
+    z = Rotate(z ^ w.first, 33);
+    v = WeakHashLen32WithSeeds(s, v.second * k1, x + w.first);
+    w = WeakHashLen32WithSeeds(s + 32, z + w.second, y);
+    std::swap(z, x);
+    s += 64;
+    x = Rotate(x + y + v.first + Fetch64(s + 16), 37) * k1;
+    y = Rotate(y + v.second + Fetch64(s + 48), 42) * k1;
+    x ^= w.second;
+    y ^= v.first;
+    z = Rotate(z ^ w.first, 33);
+    v = WeakHashLen32WithSeeds(s, v.second * k1, x + w.first);
+    w = WeakHashLen32WithSeeds(s + 32, z + w.second, y);
+    std::swap(z, x);
+    s += 64;
+    len -= 128;
+  } while (LIKELY(len >= 128));
+  y += Rotate(w.first, 37) * k0 + z;
+  x += Rotate(v.first + z, 49) * k0;
+  // If 0 < len < 128, hash up to 4 chunks of 32 bytes each from the end of s.
+  for (size_t tail_done = 0; tail_done < len; ) {
+    tail_done += 32;
+    y = Rotate(y - x, 42) * k0 + v.second;
+    w.first += Fetch64(s + len - tail_done + 16);
+    x = Rotate(x, 49) * k0 + w.first;
+    w.first += v.first;
+    v = WeakHashLen32WithSeeds(s + len - tail_done, v.first, v.second);
+  }
+  // At this point our 48 bytes of state should contain more than
+  // enough information for a strong 128-bit hash.  We use two
+  // different 48-byte-to-8-byte hashes to get a 16-byte final result.
+  x = HashLen16(x, v.first);
+  y = HashLen16(y, w.first);
+  return uint128(HashLen16(x + v.second, w.second) + y,
+                 HashLen16(x + w.second, y + v.second));
+}
+
+uint128 CityHash128(const char *s, size_t len) {
+  if (len >= 16) {
+    return CityHash128WithSeed(s + 16,
+                               len - 16,
+                               uint128(Fetch64(s) ^ k3,
+                                       Fetch64(s + 8)));
+  } else if (len >= 8) {
+    return CityHash128WithSeed(NULL,
+                               0,
+                               uint128(Fetch64(s) ^ (len * k0),
+                                       Fetch64(s + len - 8) ^ k1));
+  } else {
+    return CityHash128WithSeed(s, len, uint128(k0, k1));
+  }
+}
+
+}
+
+#ifdef __SSE4_2__
+#include "citycrc.h"
+#include <nmmintrin.h>
+
+namespace CityHash_v1_0_2
+{
+
+// Requires len >= 240.
+static void CityHashCrc256Long(const char *s, size_t len,
+                               uint32 seed, uint64 *result) {
+  uint64 a = Fetch64(s + 56) + k0;
+  uint64 b = Fetch64(s + 96) + k0;
+  uint64 c = result[1] = HashLen16(b, len);
+  uint64 d = result[2] = Fetch64(s + 120) * k0 + len;
+  uint64 e = Fetch64(s + 184) + seed;
+  uint64 f = seed;
+  uint64 g = 0;
+  uint64 h = 0;
+  uint64 i = 0;
+  uint64 j = 0;
+  uint64 t = c + d;
+
+  // 240 bytes of input per iter.
+  size_t iters = len / 240;
+  len -= iters * 240;
+  do {
+#define CHUNK(multiplier, z)                                    \
+    {                                                           \
+      uint64 old_a = a;                                         \
+      a = Rotate(b, 41 ^ z) * multiplier + Fetch64(s);          \
+      b = Rotate(c, 27 ^ z) * multiplier + Fetch64(s + 8);      \
+      c = Rotate(d, 41 ^ z) * multiplier + Fetch64(s + 16);     \
+      d = Rotate(e, 33 ^ z) * multiplier + Fetch64(s + 24);     \
+      e = Rotate(t, 25 ^ z) * multiplier + Fetch64(s + 32);     \
+      t = old_a;                                                \
+    }                                                           \
+    f = _mm_crc32_u64(f, a);                                    \
+    g = _mm_crc32_u64(g, b);                                    \
+    h = _mm_crc32_u64(h, c);                                    \
+    i = _mm_crc32_u64(i, d);                                    \
+    j = _mm_crc32_u64(j, e);                                    \
+    s += 40
+
+    CHUNK(1, 1); CHUNK(k0, 0);
+    CHUNK(1, 1); CHUNK(k0, 0);
+    CHUNK(1, 1); CHUNK(k0, 0);
+  } while (--iters > 0);
+  j += i << 32;
+  a = HashLen16(a, j);
+  h += g << 32;
+  b = b * k0 + h;
+  c = HashLen16(c, f) + i;
+  d = HashLen16(d, e);
+  pair<uint64, uint64> v(j + e, HashLen16(h, t));
+  h = v.second + f;
+  // If 0 < len < 240, hash chunks of 32 bytes each from the end of s.
+  for (size_t tail_done = 0; tail_done < len; ) {
+    tail_done += 32;
+    c = Rotate(c - a, 42) * k0 + v.second;
+    d += Fetch64(s + len - tail_done + 16);
+    a = Rotate(a, 49) * k0 + d;
+    d += v.first;
+    v = WeakHashLen32WithSeeds(s + len - tail_done, v.first, v.second);
+  }
+
+  // Final mix.
+  e = HashLen16(a, d) + v.first;
+  f = HashLen16(b, c) + a;
+  g = HashLen16(v.first, v.second) + c;
+  result[0] = e + f + g + h;
+  a = ShiftMix((a + g) * k0) * k0 + b;
+  result[1] += a + result[0];
+  a = ShiftMix(a * k0) * k0 + c;
+  result[2] += a + result[1];
+  a = ShiftMix((a + e) * k0) * k0;
+  result[3] = a + result[2];
+}
+
+// Requires len < 240.
+static void CityHashCrc256Short(const char *s, size_t len, uint64 *result) {
+  char buf[240];
+  memcpy(buf, s, len);
+  memset(buf + len, 0, 240 - len);
+  CityHashCrc256Long(buf, 240, ~static_cast<uint32>(len), result);
+}
+
+void CityHashCrc256(const char *s, size_t len, uint64 *result) {
+  if (LIKELY(len >= 240)) {
+    CityHashCrc256Long(s, len, 0, result);
+  } else {
+    CityHashCrc256Short(s, len, result);
+  }
+}
+
+uint128 CityHashCrc128WithSeed(const char *s, size_t len, uint128 seed) {
+  if (len <= 900) {
+    return CityHash128WithSeed(s, len, seed);
+  } else {
+    uint64 result[4];
+    CityHashCrc256(s, len, result);
+    uint64 u = Uint128High64(seed) + result[0];
+    uint64 v = Uint128Low64(seed) + result[1];
+    return uint128(HashLen16(u, v + result[2]),
+                   HashLen16(Rotate(v, 32), u * k0 + result[3]));
+  }
+}
+
+uint128 CityHashCrc128(const char *s, size_t len) {
+  if (len <= 900) {
+    return CityHash128(s, len);
+  } else {
+    uint64 result[4];
+    CityHashCrc256(s, len, result);
+    return uint128(result[2], result[3]);
+  }
+}
+
+}
+
+#endif
diff --git a/be/src/util/cityhash102/city.h b/be/src/util/cityhash102/city.h
new file mode 100644
index 000000000..77d4c988c
--- /dev/null
+++ b/be/src/util/cityhash102/city.h
@@ -0,0 +1,104 @@
+// Copyright (c) 2011 Google, Inc.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+//
+// CityHash, by Geoff Pike and Jyrki Alakuijala
+//
+// This file provides a few functions for hashing strings. On x86-64
+// hardware in 2011, CityHash64() is faster than other high-quality
+// hash functions, such as Murmur.  This is largely due to higher
+// instruction-level parallelism.  CityHash64() and CityHash128() also perform
+// well on hash-quality tests.
+//
+// CityHash128() is optimized for relatively long strings and returns
+// a 128-bit hash.  For strings more than about 2000 bytes it can be
+// faster than CityHash64().
+//
+// Functions in the CityHash family are not suitable for cryptography.
+//
+// WARNING: This code has not been tested on big-endian platforms!
+// It is known to work well on little-endian platforms that have a small penalty
+// for unaligned reads, such as current Intel and AMD moderate-to-high-end CPUs.
+//
+// By the way, for some hash functions, given strings a and b, the hash
+// of a+b is easily derived from the hashes of a and b.  This property
+// doesn't hold for any hash functions in this file.
+
+#ifndef CITY_HASH_H_
+#define CITY_HASH_H_
+
+#include <stdlib.h>  // for size_t.
+#include <stdint.h>
+#include <utility>
+
+/** This is a version of CityHash that predates v1.0.3 algorithm change.
+  * Why we need exactly this version?
+  * Although hash values of CityHash are not recommended for storing persistently anywhere,
+  * it has already been used this way in ClickHouse:
+  * - for calculation of checksums of compressed chunks and for data parts;
+  * - this version of CityHash is exposed in cityHash64 function in ClickHouse SQL language;
+  * - and already used by many users for data ordering, sampling and sharding.
+  */
+namespace CityHash_v1_0_2
+{
+
+typedef uint8_t uint8;
+typedef uint32_t uint32;
+typedef uint64_t uint64;
+typedef std::pair<uint64, uint64> uint128;
+
+
+inline uint64 Uint128Low64(const uint128& x) { return x.first; }
+inline uint64 Uint128High64(const uint128& x) { return x.second; }
+
+// Hash function for a byte array.
+uint64 CityHash64(const char *buf, size_t len);
+
+// Hash function for a byte array.  For convenience, a 64-bit seed is also
+// hashed into the result.
+uint64 CityHash64WithSeed(const char *buf, size_t len, uint64 seed);
+
+// Hash function for a byte array.  For convenience, two seeds are also
+// hashed into the result.
+uint64 CityHash64WithSeeds(const char *buf, size_t len,
+                           uint64 seed0, uint64 seed1);
+
+// Hash function for a byte array.
+uint128 CityHash128(const char *s, size_t len);
+
+// Hash function for a byte array.  For convenience, a 128-bit seed is also
+// hashed into the result.
+uint128 CityHash128WithSeed(const char *s, size_t len, uint128 seed);
+
+// Hash 128 input bits down to 64 bits of output.
+// This is intended to be a reasonably good hash function.
+inline uint64 Hash128to64(const uint128& x) {
+  // Murmur-inspired hashing.
+  const uint64 kMul = 0x9ddfea08eb382d69ULL;
+  uint64 a = (Uint128Low64(x) ^ Uint128High64(x)) * kMul;
+  a ^= (a >> 47);
+  uint64 b = (Uint128High64(x) ^ a) * kMul;
+  b ^= (b >> 47);
+  b *= kMul;
+  return b;
+}
+
+}
+
+#endif  // CITY_HASH_H_
diff --git a/be/src/util/cityhash102/citycrc.h b/be/src/util/cityhash102/citycrc.h
new file mode 100644
index 000000000..3ec72cc88
--- /dev/null
+++ b/be/src/util/cityhash102/citycrc.h
@@ -0,0 +1,48 @@
+// Copyright (c) 2011 Google, Inc.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+//
+// CityHash, by Geoff Pike and Jyrki Alakuijala
+//
+// This file declares the subset of the CityHash functions that require
+// _mm_crc32_u64().  See the CityHash README for details.
+//
+// Functions in the CityHash family are not suitable for cryptography.
+
+#ifndef CITY_HASH_CRC_H_
+#define CITY_HASH_CRC_H_
+
+#include "city.h"
+
+namespace CityHash_v1_0_2
+{
+
+// Hash function for a byte array.
+uint128 CityHashCrc128(const char *s, size_t len);
+
+// Hash function for a byte array.  For convenience, a 128-bit seed is also
+// hashed into the result.
+uint128 CityHashCrc128WithSeed(const char *s, size_t len, uint128 seed);
+
+// Hash function for a byte array.  Sets result[0] ... result[3].
+void CityHashCrc256(const char *s, size_t len, uint64 *result);
+
+}
+
+#endif  // CITY_HASH_CRC_H_
diff --git a/be/src/util/cityhash102/config.h b/be/src/util/cityhash102/config.h
new file mode 100644
index 000000000..cca744a35
--- /dev/null
+++ b/be/src/util/cityhash102/config.h
@@ -0,0 +1,125 @@
+/* config.h.  Generated from config.h.in by configure.  */
+/* config.h.in.  Generated from configure.ac by autoheader.  */
+
+/* Define if building universal (internal helper macro) */
+/* #undef AC_APPLE_UNIVERSAL_BUILD */
+
+/* Define to 1 if the compiler supports __builtin_expect. */
+#if _MSC_VER
+#define HAVE_BUILTIN_EXPECT 0
+#else
+#define HAVE_BUILTIN_EXPECT 1
+#endif
+
+/* Define to 1 if you have the <dlfcn.h> header file. */
+#define HAVE_DLFCN_H 1
+
+/* Define to 1 if you have the <inttypes.h> header file. */
+#define HAVE_INTTYPES_H 1
+
+/* Define to 1 if you have the <memory.h> header file. */
+#define HAVE_MEMORY_H 1
+
+/* Define to 1 if you have the <stdint.h> header file. */
+#define HAVE_STDINT_H 1
+
+/* Define to 1 if you have the <stdlib.h> header file. */
+#define HAVE_STDLIB_H 1
+
+/* Define to 1 if you have the <strings.h> header file. */
+#define HAVE_STRINGS_H 1
+
+/* Define to 1 if you have the <string.h> header file. */
+#define HAVE_STRING_H 1
+
+/* Define to 1 if you have the <sys/stat.h> header file. */
+#define HAVE_SYS_STAT_H 1
+
+/* Define to 1 if you have the <sys/types.h> header file. */
+#define HAVE_SYS_TYPES_H 1
+
+/* Define to 1 if you have the <unistd.h> header file. */
+#define HAVE_UNISTD_H 1
+
+/* Define to the sub-directory in which libtool stores uninstalled libraries.
+   */
+#define LT_OBJDIR ".libs/"
+
+/* Define to the address where bug reports for this package should be sent. */
+#define PACKAGE_BUGREPORT "cityhash-discuss@googlegroups.com"
+
+/* Define to the full name of this package. */
+#define PACKAGE_NAME "CityHash"
+
+/* Define to the full name and version of this package. */
+#define PACKAGE_STRING "CityHash 1.0.2"
+
+/* Define to the one symbol short name of this package. */
+#define PACKAGE_TARNAME "cityhash"
+
+/* Define to the home page for this package. */
+#define PACKAGE_URL ""
+
+/* Define to the version of this package. */
+#define PACKAGE_VERSION "1.0.2"
+
+/* Define to 1 if you have the ANSI C header files. */
+#define STDC_HEADERS 1
+
+/* Define WORDS_BIGENDIAN to 1 if your processor stores words with the most
+   significant byte first (like Motorola and SPARC, unlike Intel). */
+#if defined AC_APPLE_UNIVERSAL_BUILD
+# if defined __BIG_ENDIAN__
+#  define WORDS_BIGENDIAN 1
+# endif
+#else
+# ifndef WORDS_BIGENDIAN
+/* #  undef WORDS_BIGENDIAN */
+# endif
+#endif
+
+/* Define for Solaris 2.5.1 so the uint32_t typedef from <sys/synch.h>,
+   <pthread.h>, or <semaphore.h> is not used. If the typedef were allowed, the
+   #define below would cause a syntax error. */
+/* #undef _UINT32_T */
+
+/* Define for Solaris 2.5.1 so the uint64_t typedef from <sys/synch.h>,
+   <pthread.h>, or <semaphore.h> is not used. If the typedef were allowed, the
+   #define below would cause a syntax error. */
+/* #undef _UINT64_T */
+
+/* Define for Solaris 2.5.1 so the uint8_t typedef from <sys/synch.h>,
+   <pthread.h>, or <semaphore.h> is not used. If the typedef were allowed, the
+   #define below would cause a syntax error. */
+/* #undef _UINT8_T */
+
+/* Define to `__inline__' or `__inline' if that's what the C compiler
+   calls it, or to nothing if 'inline' is not supported under any name.  */
+#ifndef __cplusplus
+/* #undef inline */
+#endif
+
+/* Define to `unsigned int' if <sys/types.h> does not define. */
+/* #undef size_t */
+
+/* Define to `int' if <sys/types.h> does not define. */
+/* #undef ssize_t */
+
+/* Define to the type of an unsigned integer type of width exactly 32 bits if
+   such a type exists and the standard includes do not define it. */
+/* #undef uint32_t */
+
+/* Define to the type of an unsigned integer type of width exactly 64 bits if
+   such a type exists and the standard includes do not define it. */
+/* #undef uint64_t */
+
+/* Define to the type of an unsigned integer type of width exactly 8 bits if
+   such a type exists and the standard includes do not define it. */
+/* #undef uint8_t */
+
+#ifdef _MSC_VER
+    #include <basetsd.h>
+    typedef SSIZE_T ssize_t;
+#else
+    #include <sys/types.h>
+#endif
diff --git a/be/src/util/jsonb_error.h b/be/src/util/jsonb_error.h
index 77d6fa16d..2ad632fb8 100644
--- a/be/src/util/jsonb_error.h
+++ b/be/src/util/jsonb_error.h
@@ -30,12 +30,14 @@ enum class JsonbErrType {
     E_EMPTY_DOCUMENT,
     E_OUTPUT_FAIL,
     E_INVALID_DOCU,
+    E_INVALID_TYPE,
     E_INVALID_SCALAR_VALUE,
     E_INVALID_KEY_STRING,
     E_INVALID_KEY_LENGTH,
     E_INVALID_STR,
     E_INVALID_OBJ,
     E_INVALID_ARR,
+    E_INVALID_NUMBER,
     E_INVALID_HEX,
     E_INVALID_OCTAL,
     E_INVALID_DECIMAL,
@@ -53,6 +55,7 @@ enum class JsonbErrType {
     E_INVALID_JSONB_OBJ,
     E_NESTING_LVL_OVERFLOW,
     E_INVALID_DOCU_COMPAT,
+    E_EXCEPTION,
 
     // new error code should always be added above
     E_NUM_ERRORS
@@ -77,13 +80,15 @@ private:
             "Invalid document version",
             "Empty document",
             "Fatal error in writing JSONB",
-            "Invalid document: document must be an object or an array",
+            "Invalid document",
+            "Invalid json value type",
             "Invalid scalar value",
             "Invalid key string",
             "Key length exceeds maximum size allowed (64 bytes)",
             "Invalid string value",
             "Invalid JSON object",
             "Invalid JSON array",
+            "Invalid number",
             "Invalid HEX number",
             "Invalid octal number",
             "Invalid decimal number",
@@ -100,7 +105,8 @@ private:
             "Invalid update operation",
             "Invalid JSONB object (internal)",
             "Object or array has too many nesting levels",
-            "Invalid document: document must be an object or an array",
+            "Invalid document",
+            "Exception throwed",
 
             nullptr /* E_NUM_ERRORS */
     };
diff --git a/be/src/util/jsonb_parser_simd.h b/be/src/util/jsonb_parser_simd.h
new file mode 100644
index 000000000..10d19a3f5
--- /dev/null
+++ b/be/src/util/jsonb_parser_simd.h
@@ -0,0 +1,350 @@
+/*
+ *  Copyright (c) 2014, Facebook, Inc.
+ *  All rights reserved.
+ *
+ *  This source code is licensed under the BSD-style license found in the
+ *  LICENSE file in the root directory of this source tree. An additional grant
+ *  of patent rights can be found in the PATENTS file in the same directory.
+ *
+ */
+
+/*
+ * This file defines JsonbParserTSIMD (template) and JsonbParser.
+ *
+ * JsonbParserTSIMD is a template class which implements a JSON parser.
+ * JsonbParserTSIMD parses JSON text, and serialize it to JSONB binary format
+ * by using JsonbWriterT object. By default, JsonbParserTSIMD creates a new
+ * JsonbWriterT object with an output stream object.  However, you can also
+ * pass in your JsonbWriterT or any stream object that implements some basic
+ * interface of std::ostream (see JsonbStream.h).
+ *
+ * JsonbParser specializes JsonbParserTSIMD with JsonbOutStream type (see
+ * JsonbStream.h). So unless you want to provide own a different output stream
+ * type, use JsonbParser object.
+ *
+ * ** Parsing JSON **
+ * JsonbParserTSIMD parses JSON string, and directly serializes into JSONB
+ * packed bytes. There are three ways to parse a JSON string: (1) using
+ * c-string, (2) using string with len, (3) using std::istream object. You can
+ * use custom streambuf to redirect output. JsonbOutBuffer is a streambuf used
+ * internally if the input is raw character buffer.
+ *
+ * You can reuse an JsonbParserTSIMD object to parse/serialize multiple JSON
+ * strings, and the previous JSONB will be overwritten.
+ *
+ * If parsing fails (returned false), the error code will be set to one of
+ * JsonbErrType, and can be retrieved by calling getErrorCode().
+ *
+ * ** External dictionary **
+ * During parsing a JSON string, you can pass a call-back function to map a key
+ * string to an id, and store the dictionary id in JSONB to save space. The
+ * purpose of using an external dictionary is more towards a collection of
+ * documents (which has common keys) rather than a single document, so that
+ * space saving will be significant.
+ *
+ * ** Endianness **
+ * Note: JSONB serialization doesn't assume endianness of the server. However
+ * you will need to ensure that the endianness at the reader side is the same
+ * as that at the writer side (if they are on different machines). Otherwise,
+ * proper conversion is needed when a number value is returned to the
+ * caller/writer.
+ *
+ * @author Tian Xia <tianx@fb.com>
+ * 
+ * this file is copied from 
+ * https://github.com/facebook/mysql-5.6/blob/fb-mysql-5.6.35/fbson/FbsonJsonParser.h
+ * and modified by Doris
+ */
+
+#ifndef JSONB_JSONBJSONPARSERSIMD_H
+#define JSONB_JSONBJSONPARSERSIMD_H
+
+#include <simdjson.h>
+
+#include <cmath>
+#include <limits>
+
+#include "jsonb_document.h"
+#include "jsonb_error.h"
+#include "jsonb_writer.h"
+#include "string_parser.hpp"
+
+namespace doris {
+
+/*
+ * Template JsonbParserTSIMD
+ */
+template <class OS_TYPE>
+class JsonbParserTSIMD {
+public:
+    JsonbParserTSIMD() : err_(JsonbErrType::E_NONE) {}
+
+    explicit JsonbParserTSIMD(OS_TYPE& os) : writer_(os), err_(JsonbErrType::E_NONE) {}
+
+    // parse a UTF-8 JSON string
+    bool parse(const std::string& str, hDictInsert handler = nullptr) {
+        return parse(str.c_str(), (unsigned int)str.size(), handler);
+    }
+
+    // parse a UTF-8 JSON c-style string (NULL terminated)
+    bool parse(const char* c_str, hDictInsert handler = nullptr) {
+        return parse(c_str, (unsigned int)strlen(c_str), handler);
+    }
+
+    // parse a UTF-8 JSON string with length
+    bool parse(const char* pch, unsigned int len, hDictInsert handler = nullptr) {
+        // reset state before parse
+        reset();
+
+        if (!pch || len == 0) {
+            err_ = JsonbErrType::E_EMPTY_DOCUMENT;
+            LOG(WARNING) << "empty json string";
+            return false;
+        }
+
+        // parse json using simdjson, return false on exception
+        try {
+            simdjson::padded_string json_str {pch, len};
+            simdjson::ondemand::document doc = parser_.iterate(json_str);
+
+            // simdjson process top level primitive types specially
+            // so some repeated code here
+            switch (doc.type()) {
+            case simdjson::ondemand::json_type::object:
+            case simdjson::ondemand::json_type::array: {
+                parse(doc.get_value(), handler);
+                break;
+            }
+            case simdjson::ondemand::json_type::null: {
+                if (writer_.writeNull() == 0) {
+                    err_ = JsonbErrType::E_OUTPUT_FAIL;
+                    LOG(WARNING) << "writeNull failed";
+                }
+                break;
+            }
+            case simdjson::ondemand::json_type::boolean: {
+                if (writer_.writeBool(doc.get_bool()) == 0) {
+                    err_ = JsonbErrType::E_OUTPUT_FAIL;
+                    LOG(WARNING) << "writeBool failed";
+                }
+                break;
+            }
+            case simdjson::ondemand::json_type::string: {
+                write_string(doc.get_string());
+                break;
+            }
+            case simdjson::ondemand::json_type::number: {
+                write_number(doc.get_number());
+                break;
+            }
+            }
+
+            return err_ == JsonbErrType::E_NONE;
+        } catch (simdjson::simdjson_error& e) {
+            err_ = JsonbErrType::E_EXCEPTION;
+            LOG(WARNING) << "simdjson parse exception: " << e.what();
+            return false;
+        }
+    }
+
+    // parse json, recursively if necessary, by simdjson
+    //  and serialize to binary format by writer
+    void parse(simdjson::ondemand::value value, hDictInsert handler = nullptr) {
+        switch (value.type()) {
+        case simdjson::ondemand::json_type::null: {
+            if (writer_.writeNull() == 0) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeNull failed";
+            }
+            break;
+        }
+        case simdjson::ondemand::json_type::boolean: {
+            if (writer_.writeBool(value.get_bool()) == 0) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeBool failed";
+            }
+            break;
+        }
+        case simdjson::ondemand::json_type::string: {
+            write_string(value.get_string());
+            break;
+        }
+        case simdjson::ondemand::json_type::number: {
+            write_number(value.get_number());
+            break;
+        }
+        case simdjson::ondemand::json_type::object: {
+            if (!writer_.writeStartObject()) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeStartObject failed";
+                break;
+            }
+
+            for (auto kv : value.get_object()) {
+                std::string_view key;
+                simdjson::error_code e = kv.unescaped_key().get(key);
+                if (e != simdjson::SUCCESS) {
+                    err_ = JsonbErrType::E_INVALID_KEY_STRING;
+                    LOG(WARNING) << "simdjson get key failed: " << e;
+                    break;
+                }
+
+                int key_id = -1;
+                if (handler) {
+                    key_id = handler(key.data(), key.size());
+                }
+
+                if (key_id < 0) {
+                    if (writer_.writeKey(key.data(), key.size()) == 0) {
+                        err_ = JsonbErrType::E_OUTPUT_FAIL;
+                        LOG(WARNING) << "writeKey failed key: " << key;
+                        break;
+                    }
+                } else {
+                    if (writer_.writeKey(key_id) == 0) {
+                        err_ = JsonbErrType::E_OUTPUT_FAIL;
+                        LOG(WARNING) << "writeKey failed key_id: " << key_id;
+                        break;
+                    }
+                }
+
+                // parse object value
+                parse(kv.value(), handler);
+                if (err_ != JsonbErrType::E_NONE) {
+                    LOG(WARNING) << "parse object value failed";
+                    break;
+                }
+            }
+            if (err_ != JsonbErrType::E_NONE) {
+                break;
+            }
+
+            if (!writer_.writeEndObject()) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeEndObject failed";
+                break;
+            }
+
+            break;
+        }
+        case simdjson::ondemand::json_type::array: {
+            if (!writer_.writeStartArray()) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeStartArray failed";
+                break;
+            }
+
+            for (auto elem : value.get_array()) {
+                // parse array element
+                parse(elem.value(), handler);
+                if (err_ != JsonbErrType::E_NONE) {
+                    LOG(WARNING) << "parse array element failed";
+                    break;
+                }
+            }
+            if (err_ != JsonbErrType::E_NONE) {
+                break;
+            }
+
+            if (!writer_.writeEndArray()) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeEndArray failed";
+                break;
+            }
+
+            break;
+        }
+        default: {
+            err_ = JsonbErrType::E_INVALID_TYPE;
+            LOG(WARNING) << "unknown value type: "; // << value;
+            break;
+        }
+
+        } // end of switch
+    }
+
+    void write_string(std::string_view str) {
+        // start writing string
+        if (!writer_.writeStartString()) {
+            err_ = JsonbErrType::E_OUTPUT_FAIL;
+            LOG(WARNING) << "writeStartString failed";
+            return;
+        }
+
+        // write string
+        if (str.size() > 0) {
+            if (writer_.writeString(str.data(), str.size()) == 0) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeString failed";
+                return;
+            }
+        }
+
+        // end writing string
+        if (!writer_.writeEndString()) {
+            err_ = JsonbErrType::E_OUTPUT_FAIL;
+            LOG(WARNING) << "writeEndString failed";
+            return;
+        }
+    }
+
+    void write_number(simdjson::ondemand::number num) {
+        if (num.is_double()) {
+            if (writer_.writeDouble(num.get_double()) == 0) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeDouble failed";
+                return;
+            }
+        } else if (num.is_int64() || num.is_uint64()) {
+            if (num.is_uint64() && num.get_uint64() > std::numeric_limits<int64_t>::max()) {
+                err_ = JsonbErrType::E_OCTAL_OVERFLOW;
+                LOG(WARNING) << "overflow number: " << num.get_uint64();
+                return;
+            }
+            int64_t val = num.is_int64() ? num.get_int64() : num.get_uint64();
+            int size = 0;
+            if (val <= std::numeric_limits<int8_t>::max()) {
+                size = writer_.writeInt8((int8_t)val);
+            } else if (val <= std::numeric_limits<int16_t>::max()) {
+                size = writer_.writeInt16((int16_t)val);
+            } else if (val <= std::numeric_limits<int32_t>::max()) {
+                size = writer_.writeInt32((int32_t)val);
+            } else { // val <= INT64_MAX
+                size = writer_.writeInt64(val);
+            }
+
+            if (size == 0) {
+                err_ = JsonbErrType::E_OUTPUT_FAIL;
+                LOG(WARNING) << "writeInt failed";
+                return;
+            }
+        } else {
+            err_ = JsonbErrType::E_INVALID_NUMBER;
+            LOG(WARNING) << "invalid number: " << num.as_double();
+            return;
+        }
+    }
+
+    JsonbWriterT<OS_TYPE>& getWriter() { return writer_; }
+
+    JsonbErrType getErrorCode() { return err_; }
+
+    // clear error code
+    void clearErr() { err_ = JsonbErrType::E_NONE; }
+
+    void reset() {
+        writer_.reset();
+        clearErr();
+    }
+
+private:
+    simdjson::ondemand::parser parser_;
+    JsonbWriterT<OS_TYPE> writer_;
+    JsonbErrType err_;
+};
+
+using JsonbParserSIMD = JsonbParserTSIMD<JsonbOutStream>;
+
+} // namespace doris
+
+#endif // JSONB_JSONBJSONPARSERSIMD_H
diff --git a/be/src/util/mem_info.cpp b/be/src/util/mem_info.cpp
index 55500feea..cccf77c06 100644
--- a/be/src/util/mem_info.cpp
+++ b/be/src/util/mem_info.cpp
@@ -106,7 +106,7 @@ void MemInfo::process_minor_gc() {
     StoragePageCache::instance()->prune(segment_v2::DATA_PAGE);
     if (config::enable_query_memroy_overcommit) {
         freed_mem +=
-                MemTrackerLimiter::free_top_overcommit_query(_s_process_full_gc_size - freed_mem);
+                MemTrackerLimiter::free_top_overcommit_query(_s_process_minor_gc_size - freed_mem);
     }
 }
 
diff --git a/be/src/util/simd/vstring_function.h b/be/src/util/simd/vstring_function.h
index e48d677eb..3c1a4e7f3 100644
--- a/be/src/util/simd/vstring_function.h
+++ b/be/src/util/simd/vstring_function.h
@@ -43,6 +43,10 @@ static constexpr std::array<uint8, 256> UTF8_BYTE_LENGTH = {
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6};
 
+inline uint8_t get_utf8_byte_length(uint8_t character) {
+    return UTF8_BYTE_LENGTH[character];
+}
+
 namespace simd {
 
 class VStringFunctions {
diff --git a/be/src/vec/core/decimal_comparison.h b/be/src/vec/core/decimal_comparison.h
index 1f2bce420..4c7cfcf76 100644
--- a/be/src/vec/core/decimal_comparison.h
+++ b/be/src/vec/core/decimal_comparison.h
@@ -142,9 +142,11 @@ private:
 
         Shift shift;
         if (decimal0 && decimal1) {
-            auto result_type = decimal_result_type(*decimal0, *decimal1, false, false);
-            shift.a = result_type.scale_factor_for(*decimal0, false);
-            shift.b = result_type.scale_factor_for(*decimal1, false);
+            using Type = std::conditional_t<sizeof(T) >= sizeof(U), T, U>;
+            auto type_ptr = decimal_result_type(*decimal0, *decimal1, false, false, false);
+            const DataTypeDecimal<Type>* result_type = check_decimal<Type>(*type_ptr);
+            shift.a = result_type->scale_factor_for(*decimal0, false);
+            shift.b = result_type->scale_factor_for(*decimal1, false);
         } else if (decimal0) {
             shift.b = decimal0->get_scale_multiplier();
         } else if (decimal1) {
diff --git a/be/src/vec/data_types/data_type_decimal.h b/be/src/vec/data_types/data_type_decimal.h
index ea2995429..c8e08303a 100644
--- a/be/src/vec/data_types/data_type_decimal.h
+++ b/be/src/vec/data_types/data_type_decimal.h
@@ -219,56 +219,30 @@ private:
 };
 
 template <typename T, typename U>
-typename std::enable_if_t<(sizeof(T) >= sizeof(U)), const DataTypeDecimal<T>> decimal_result_type(
-        const DataTypeDecimal<T>& tx, const DataTypeDecimal<U>& ty, bool is_multiply,
-        bool is_divide) {
+DataTypePtr decimal_result_type(const DataTypeDecimal<T>& tx, const DataTypeDecimal<U>& ty,
+                                bool is_multiply, bool is_divide, bool is_plus_minus) {
+    using Type = std::conditional_t<sizeof(T) >= sizeof(U), T, U>;
     if constexpr (IsDecimalV2<T> && IsDecimalV2<U>) {
-        return DataTypeDecimal<T>(max_decimal_precision<T>(), 9);
+        return std::make_shared<DataTypeDecimal<Type>>((max_decimal_precision<T>(), 9));
     } else {
-        UInt32 scale = (tx.get_scale() > ty.get_scale() ? tx.get_scale() : ty.get_scale());
+        UInt32 scale = std::max(tx.get_scale(), ty.get_scale());
+        auto precision = max_decimal_precision<Type>();
+
+        size_t multiply_precision = tx.get_precision() + ty.get_precision();
+        size_t divide_precision = tx.get_precision() + ty.get_scale();
+        size_t plus_minus_precision =
+                std::max(tx.get_precision() - tx.get_scale(), ty.get_precision() - ty.get_scale()) +
+                scale;
         if (is_multiply) {
             scale = tx.get_scale() + ty.get_scale();
+            precision = std::min(multiply_precision, max_decimal_precision<Decimal128I>());
         } else if (is_divide) {
             scale = tx.get_scale();
+            precision = std::min(divide_precision, max_decimal_precision<Decimal128I>());
+        } else if (is_plus_minus) {
+            precision = std::min(plus_minus_precision, max_decimal_precision<Decimal128I>());
         }
-        return DataTypeDecimal<T>(max_decimal_precision<T>(), scale);
-    }
-}
-
-template <typename T, typename U>
-typename std::enable_if_t<(sizeof(T) < sizeof(U)), const DataTypeDecimal<U>> decimal_result_type(
-        const DataTypeDecimal<T>& tx, const DataTypeDecimal<U>& ty, bool is_multiply,
-        bool is_divide) {
-    if constexpr (IsDecimalV2<T> && IsDecimalV2<U>) {
-        return DataTypeDecimal<U>(max_decimal_precision<U>(), 9);
-    } else {
-        UInt32 scale = (tx.get_scale() > ty.get_scale() ? tx.get_scale() : ty.get_scale());
-        if (is_multiply) {
-            scale = tx.get_scale() + ty.get_scale();
-        } else if (is_divide) {
-            scale = tx.get_scale();
-        }
-        return DataTypeDecimal<U>(max_decimal_precision<U>(), scale);
-    }
-}
-
-template <typename T, typename U>
-const DataTypeDecimal<T> decimal_result_type(const DataTypeDecimal<T>& tx, const DataTypeNumber<U>&,
-                                             bool, bool) {
-    if constexpr (IsDecimalV2<T> && IsDecimalV2<U>) {
-        return DataTypeDecimal<T>(max_decimal_precision<T>(), 9);
-    } else {
-        return DataTypeDecimal<T>(max_decimal_precision<T>(), tx.get_scale());
-    }
-}
-
-template <typename T, typename U>
-const DataTypeDecimal<U> decimal_result_type(const DataTypeNumber<T>&, const DataTypeDecimal<U>& ty,
-                                             bool, bool) {
-    if constexpr (IsDecimalV2<T> && IsDecimalV2<U>) {
-        return DataTypeDecimal<U>(max_decimal_precision<U>(), 9);
-    } else {
-        return DataTypeDecimal<U>(max_decimal_precision<U>(), ty.get_scale());
+        return create_decimal(precision, scale, false);
     }
 }
 
diff --git a/be/src/vec/exec/scan/new_jdbc_scan_node.cpp b/be/src/vec/exec/scan/new_jdbc_scan_node.cpp
index f30596d1e..73b5f6331 100644
--- a/be/src/vec/exec/scan/new_jdbc_scan_node.cpp
+++ b/be/src/vec/exec/scan/new_jdbc_scan_node.cpp
@@ -25,7 +25,8 @@ NewJdbcScanNode::NewJdbcScanNode(ObjectPool* pool, const TPlanNode& tnode,
         : VScanNode(pool, tnode, descs),
           _table_name(tnode.jdbc_scan_node.table_name),
           _tuple_id(tnode.jdbc_scan_node.tuple_id),
-          _query_string(tnode.jdbc_scan_node.query_string) {
+          _query_string(tnode.jdbc_scan_node.query_string),
+          _table_type(tnode.jdbc_scan_node.table_type) {
     _output_tuple_id = tnode.jdbc_scan_node.tuple_id;
 }
 
@@ -49,8 +50,8 @@ Status NewJdbcScanNode::_init_scanners(std::list<VScanner*>* scanners) {
     if (_eos == true) {
         return Status::OK();
     }
-    NewJdbcScanner* scanner =
-            new NewJdbcScanner(_state, this, _limit_per_scanner, _tuple_id, _query_string);
+    NewJdbcScanner* scanner = new NewJdbcScanner(_state, this, _limit_per_scanner, _tuple_id,
+                                                 _query_string, _table_type);
     _scanner_pool.add(scanner);
     RETURN_IF_ERROR(scanner->prepare(_state, _vconjunct_ctx_ptr.get()));
     scanners->push_back(static_cast<VScanner*>(scanner));
diff --git a/be/src/vec/exec/scan/new_jdbc_scan_node.h b/be/src/vec/exec/scan/new_jdbc_scan_node.h
index d527f24b1..1cc34e83f 100644
--- a/be/src/vec/exec/scan/new_jdbc_scan_node.h
+++ b/be/src/vec/exec/scan/new_jdbc_scan_node.h
@@ -37,6 +37,7 @@ private:
     std::string _table_name;
     TupleId _tuple_id;
     std::string _query_string;
+    TOdbcTableType::type _table_type;
 };
 } // namespace vectorized
 } // namespace doris
diff --git a/be/src/vec/exec/scan/new_jdbc_scanner.cpp b/be/src/vec/exec/scan/new_jdbc_scanner.cpp
index ef9004530..cbb9588bb 100644
--- a/be/src/vec/exec/scan/new_jdbc_scanner.cpp
+++ b/be/src/vec/exec/scan/new_jdbc_scanner.cpp
@@ -19,13 +19,15 @@
 
 namespace doris::vectorized {
 NewJdbcScanner::NewJdbcScanner(RuntimeState* state, NewJdbcScanNode* parent, int64_t limit,
-                               const TupleId& tuple_id, const std::string& query_string)
+                               const TupleId& tuple_id, const std::string& query_string,
+                               TOdbcTableType::type table_type)
         : VScanner(state, static_cast<VScanNode*>(parent), limit),
           _is_init(false),
           _jdbc_eos(false),
           _tuple_id(tuple_id),
           _query_string(query_string),
-          _tuple_desc(nullptr) {}
+          _tuple_desc(nullptr),
+          _table_type(table_type) {}
 
 Status NewJdbcScanner::prepare(RuntimeState* state, VExprContext** vconjunct_ctx_ptr) {
     VLOG_CRITICAL << "NewJdbcScanner::Prepare";
@@ -63,6 +65,7 @@ Status NewJdbcScanner::prepare(RuntimeState* state, VExprContext** vconjunct_ctx
     _jdbc_param.passwd = jdbc_table->jdbc_passwd();
     _jdbc_param.tuple_desc = _tuple_desc;
     _jdbc_param.query_string = std::move(_query_string);
+    _jdbc_param.table_type = _table_type;
 
     _jdbc_connector.reset(new (std::nothrow) JdbcConnector(_jdbc_param));
     if (_jdbc_connector == nullptr) {
diff --git a/be/src/vec/exec/scan/new_jdbc_scanner.h b/be/src/vec/exec/scan/new_jdbc_scanner.h
index f5584cd5d..9fa17c411 100644
--- a/be/src/vec/exec/scan/new_jdbc_scanner.h
+++ b/be/src/vec/exec/scan/new_jdbc_scanner.h
@@ -26,12 +26,12 @@ namespace vectorized {
 class NewJdbcScanner : public VScanner {
 public:
     NewJdbcScanner(RuntimeState* state, NewJdbcScanNode* parent, int64_t limit,
-                   const TupleId& tuple_id, const std::string& query_string);
+                   const TupleId& tuple_id, const std::string& query_string,
+                   TOdbcTableType::type table_type);
 
     Status open(RuntimeState* state) override;
     Status close(RuntimeState* state) override;
 
-public:
     Status prepare(RuntimeState* state, VExprContext** vconjunct_ctx_ptr);
 
 protected:
@@ -48,6 +48,8 @@ private:
     std::string _query_string;
     // Descriptor of tuples read from JDBC table.
     const TupleDescriptor* _tuple_desc;
+    // the sql query database type: like mysql, PG...
+    TOdbcTableType::type _table_type;
     // Scanner of JDBC.
     std::unique_ptr<JdbcConnector> _jdbc_connector;
     JdbcConnectorParam _jdbc_param;
diff --git a/be/src/vec/exec/scan/new_olap_scan_node.cpp b/be/src/vec/exec/scan/new_olap_scan_node.cpp
index 07e933c4a..891d53cf5 100644
--- a/be/src/vec/exec/scan/new_olap_scan_node.cpp
+++ b/be/src/vec/exec/scan/new_olap_scan_node.cpp
@@ -233,6 +233,20 @@ Status NewOlapScanNode::_build_key_ranges_and_filters() {
             }
         }
 
+        for (auto& iter : _compound_value_ranges) {
+            std::vector<TCondition> filters;
+            std::visit(
+                    [&](auto&& range) {
+                        if (range.is_in_compound_value_range()) {
+                            range.to_condition_in_compound(filters);
+                        }
+                    },
+                    iter);
+            for (const auto& filter : filters) {
+                _compound_filters.push_back(filter);
+            }
+        }
+
         // Append value ranges in "_not_in_value_ranges"
         for (auto& range : _not_in_value_ranges) {
             std::visit([&](auto&& the_range) { the_range.to_in_condition(_olap_filters, false); },
@@ -385,6 +399,8 @@ Status NewOlapScanNode::_init_scanners(std::list<VScanner*>* scanners) {
             NewOlapScanner* scanner = new NewOlapScanner(
                     _state, this, _limit_per_scanner, _olap_scan_node.is_preaggregation,
                     _need_agg_finalize, *scan_range, _scanner_profile.get());
+
+            scanner->set_compound_filters(_compound_filters);
             // add scanner to pool before doing prepare.
             // so that scanner can be automatically deconstructed if prepare failed.
             _scanner_pool.add(scanner);
diff --git a/be/src/vec/exec/scan/new_olap_scan_node.h b/be/src/vec/exec/scan/new_olap_scan_node.h
index cf477bf85..3d37f2c5d 100644
--- a/be/src/vec/exec/scan/new_olap_scan_node.h
+++ b/be/src/vec/exec/scan/new_olap_scan_node.h
@@ -65,6 +65,9 @@ private:
     std::vector<std::unique_ptr<TPaloScanRange>> _scan_ranges;
     OlapScanKeys _scan_keys;
     std::vector<TCondition> _olap_filters;
+    // _compound_filters store conditions in the one compound relationship in conjunct expr tree except leaf node of `and` node,
+    // such as: "(a or b) and (c or d)", conditions for a,b,c,d will be stored
+    std::vector<TCondition> _compound_filters;
 
 private:
     std::unique_ptr<RuntimeProfile> _segment_profile;
diff --git a/be/src/vec/exec/scan/new_olap_scanner.cpp b/be/src/vec/exec/scan/new_olap_scanner.cpp
index fe1a010df..93919e2d4 100644
--- a/be/src/vec/exec/scan/new_olap_scanner.cpp
+++ b/be/src/vec/exec/scan/new_olap_scanner.cpp
@@ -127,6 +127,10 @@ Status NewOlapScanner::open(RuntimeState* state) {
     return Status::OK();
 }
 
+void NewOlapScanner::set_compound_filters(const std::vector<TCondition>& compound_filters) {
+    _compound_filters = compound_filters;
+}
+
 // it will be called under tablet read lock because capture rs readers need
 Status NewOlapScanner::_init_tablet_reader_params(
         const std::vector<OlapScanRange*>& key_ranges, const std::vector<TCondition>& filters,
@@ -168,11 +172,18 @@ Status NewOlapScanner::_init_tablet_reader_params(
                 real_parent->_olap_scan_node.push_down_agg_type_opt;
     }
     _tablet_reader_params.version = Version(0, _version);
+    _tablet_reader_params.remaining_vconjunct_root =
+            (_vconjunct_ctx == nullptr) ? nullptr : _vconjunct_ctx->root();
 
     // Condition
     for (auto& filter : filters) {
         _tablet_reader_params.conditions.push_back(filter);
     }
+
+    std::copy(_compound_filters.cbegin(), _compound_filters.cend(),
+              std::inserter(_tablet_reader_params.conditions_except_leafnode_of_andnode,
+                            _tablet_reader_params.conditions_except_leafnode_of_andnode.begin()));
+
     std::copy(filter_predicates.bloom_filters.cbegin(), filter_predicates.bloom_filters.cend(),
               std::inserter(_tablet_reader_params.bloom_filters,
                             _tablet_reader_params.bloom_filters.begin()));
diff --git a/be/src/vec/exec/scan/new_olap_scanner.h b/be/src/vec/exec/scan/new_olap_scanner.h
index 5f253ea93..b15376dd9 100644
--- a/be/src/vec/exec/scan/new_olap_scanner.h
+++ b/be/src/vec/exec/scan/new_olap_scanner.h
@@ -49,6 +49,8 @@ public:
 
     const std::string& scan_disk() const { return _tablet->data_dir()->path(); }
 
+    void set_compound_filters(const std::vector<TCondition>& compound_filters);
+
 protected:
     Status _get_block_impl(RuntimeState* state, Block* block, bool* eos) override;
     void _update_counters_before_close() override;
@@ -75,6 +77,7 @@ private:
 
     std::vector<uint32_t> _return_columns;
     std::unordered_set<uint32_t> _tablet_columns_convert_to_null_set;
+    std::vector<TCondition> _compound_filters;
 
     // ========= profiles ==========
     int64_t _compressed_bytes_read = 0;
diff --git a/be/src/vec/exec/scan/vfile_scanner.cpp b/be/src/vec/exec/scan/vfile_scanner.cpp
index 81cfe7fbd..2b35f2c61 100644
--- a/be/src/vec/exec/scan/vfile_scanner.cpp
+++ b/be/src/vec/exec/scan/vfile_scanner.cpp
@@ -655,8 +655,27 @@ Status VFileScanner::_init_expr_ctxes() {
         }
     }
 
+    // set column name to default value expr map
+    for (auto slot_desc : _real_tuple_desc->slots()) {
+        if (!slot_desc->is_materialized()) {
+            continue;
+        }
+        vectorized::VExprContext* ctx = nullptr;
+        auto it = _params.default_value_of_src_slot.find(slot_desc->id());
+        if (it != std::end(_params.default_value_of_src_slot)) {
+            if (!it->second.nodes.empty()) {
+                RETURN_IF_ERROR(
+                        vectorized::VExpr::create_expr_tree(_state->obj_pool(), it->second, &ctx));
+                RETURN_IF_ERROR(ctx->prepare(_state, *_default_val_row_desc));
+                RETURN_IF_ERROR(ctx->open(_state));
+            }
+            // if expr is empty, the default value will be null
+            _col_default_value_ctx.emplace(slot_desc->col_name(), ctx);
+        }
+    }
+
     if (_is_load) {
-        // follow desc expr map and src default value expr map is only for load task.
+        // follow desc expr map is only for load task.
         bool has_slot_id_map = _params.__isset.dest_sid_to_src_sid_without_trans;
         int idx = 0;
         for (auto slot_desc : _output_tuple_desc->slots()) {
@@ -695,24 +714,6 @@ Status VFileScanner::_init_expr_ctxes() {
                 }
             }
         }
-
-        for (auto slot_desc : _real_tuple_desc->slots()) {
-            if (!slot_desc->is_materialized()) {
-                continue;
-            }
-            vectorized::VExprContext* ctx = nullptr;
-            auto it = _params.default_value_of_src_slot.find(slot_desc->id());
-            if (it != std::end(_params.default_value_of_src_slot)) {
-                if (!it->second.nodes.empty()) {
-                    RETURN_IF_ERROR(vectorized::VExpr::create_expr_tree(_state->obj_pool(),
-                                                                        it->second, &ctx));
-                    RETURN_IF_ERROR(ctx->prepare(_state, *_default_val_row_desc));
-                    RETURN_IF_ERROR(ctx->open(_state));
-                }
-                // if expr is empty, the default value will be null
-                _col_default_value_ctx.emplace(slot_desc->col_name(), ctx);
-            }
-        }
     }
     return Status::OK();
 }
diff --git a/be/src/vec/exec/scan/vscan_node.cpp b/be/src/vec/exec/scan/vscan_node.cpp
index b0c495799..2445a5d9c 100644
--- a/be/src/vec/exec/scan/vscan_node.cpp
+++ b/be/src/vec/exec/scan/vscan_node.cpp
@@ -504,6 +504,15 @@ Status VScanNode::_normalize_predicate(VExpr* conjunct_expr_root, VExpr** output
                         },
                         *range);
             }
+
+            if (pdt == PushDownType::UNACCEPTABLE &&
+                TExprNodeType::COMPOUND_PRED == cur_expr->node_type()) {
+                _normalize_compound_predicate(cur_expr, *(_vconjunct_ctx_ptr.get()), &pdt,
+                                              in_predicate_checker, eq_predicate_checker);
+                *output_expr = conjunct_expr_root; // remaining in conjunct tree
+                return Status::OK();
+            }
+
             if (pdt == PushDownType::ACCEPTABLE && _is_key_column(slot->col_name())) {
                 *output_expr = nullptr;
                 return Status::OK();
@@ -910,6 +919,104 @@ Status VScanNode::_normalize_noneq_binary_predicate(VExpr* expr, VExprContext* e
     return Status::OK();
 }
 
+Status VScanNode::_normalize_compound_predicate(
+        vectorized::VExpr* expr, VExprContext* expr_ctx, PushDownType* pdt,
+        const std::function<bool(const std::vector<VExpr*>&, const VSlotRef**, VExpr**)>&
+                in_predicate_checker,
+        const std::function<bool(const std::vector<VExpr*>&, const VSlotRef**, VExpr**)>&
+                eq_predicate_checker) {
+    if (TExprNodeType::COMPOUND_PRED == expr->node_type()) {
+        auto compound_fn_name = expr->fn().name.function_name;
+        auto children_num = expr->children().size();
+        for (auto i = 0; i < children_num; ++i) {
+            VExpr* child_expr = expr->children()[i];
+            if (TExprNodeType::BINARY_PRED == child_expr->node_type()) {
+                SlotDescriptor* slot = nullptr;
+                ColumnValueRangeType* range_on_slot = nullptr;
+                if (_is_predicate_acting_on_slot(child_expr, in_predicate_checker, &slot,
+                                                 &range_on_slot) ||
+                    _is_predicate_acting_on_slot(child_expr, eq_predicate_checker, &slot,
+                                                 &range_on_slot)) {
+                    ColumnValueRangeType active_range =
+                            *range_on_slot; // copy, in order not to affect the range in the _colname_to_value_range
+                    std::visit(
+                            [&](auto& value_range) {
+                                _normalize_binary_in_compound_predicate(child_expr, expr_ctx, slot,
+                                                                        value_range, pdt);
+                            },
+                            active_range);
+
+                    _compound_value_ranges.emplace_back(active_range);
+                }
+            } else if (TExprNodeType::COMPOUND_PRED == child_expr->node_type()) {
+                _normalize_compound_predicate(child_expr, expr_ctx, pdt, in_predicate_checker,
+                                              eq_predicate_checker);
+            }
+        }
+    }
+
+    return Status::OK();
+}
+
+template <PrimitiveType T>
+Status VScanNode::_normalize_binary_in_compound_predicate(vectorized::VExpr* expr,
+                                                          VExprContext* expr_ctx,
+                                                          SlotDescriptor* slot,
+                                                          ColumnValueRange<T>& range,
+                                                          PushDownType* pdt) {
+    DCHECK(expr->children().size() == 2);
+    if (TExprNodeType::BINARY_PRED == expr->node_type()) {
+        auto eq_checker = [](const std::string& fn_name) { return fn_name == "eq"; };
+        auto ne_checker = [](const std::string& fn_name) { return fn_name == "ne"; };
+        auto noneq_checker = [](const std::string& fn_name) {
+            return fn_name != "ne" && fn_name != "eq";
+        };
+
+        StringRef value;
+        int slot_ref_child = -1;
+        PushDownType eq_pdt;
+        PushDownType ne_pdt;
+        PushDownType noneq_pdt;
+        RETURN_IF_ERROR(_should_push_down_binary_predicate(
+                reinterpret_cast<VectorizedFnCall*>(expr), expr_ctx, &value, &slot_ref_child,
+                eq_checker, eq_pdt));
+        RETURN_IF_ERROR(_should_push_down_binary_predicate(
+                reinterpret_cast<VectorizedFnCall*>(expr), expr_ctx, &value, &slot_ref_child,
+                ne_checker, ne_pdt));
+        RETURN_IF_ERROR(_should_push_down_binary_predicate(
+                reinterpret_cast<VectorizedFnCall*>(expr), expr_ctx, &value, &slot_ref_child,
+                noneq_checker, noneq_pdt));
+        if (eq_pdt == PushDownType::UNACCEPTABLE && ne_pdt == PushDownType::UNACCEPTABLE &&
+            noneq_pdt == PushDownType::UNACCEPTABLE) {
+            return Status::OK();
+        }
+        DCHECK(slot_ref_child >= 0);
+        const std::string& fn_name =
+                reinterpret_cast<VectorizedFnCall*>(expr)->fn().name.function_name;
+        if (eq_pdt == PushDownType::ACCEPTABLE || ne_pdt == PushDownType::ACCEPTABLE ||
+            noneq_pdt == PushDownType::ACCEPTABLE) {
+            if (value.data != nullptr) {
+                if constexpr (T == TYPE_CHAR || T == TYPE_VARCHAR || T == TYPE_STRING ||
+                              T == TYPE_HLL) {
+                    auto val = StringValue(value.data, value.size);
+                    RETURN_IF_ERROR(_change_value_range<false>(
+                            range, reinterpret_cast<void*>(&val),
+                            ColumnValueRange<T>::add_compound_value_range, fn_name,
+                            slot_ref_child));
+                } else {
+                    RETURN_IF_ERROR(_change_value_range<false>(
+                            range, reinterpret_cast<void*>(const_cast<char*>(value.data)),
+                            ColumnValueRange<T>::add_compound_value_range, fn_name,
+                            slot_ref_child));
+                }
+            }
+            *pdt = PushDownType::ACCEPTABLE;
+        }
+    }
+
+    return Status::OK();
+}
+
 template <bool IsFixed, PrimitiveType PrimitiveType, typename ChangeFixedValueRangeFunc>
 Status VScanNode::_change_value_range(ColumnValueRange<PrimitiveType>& temp_range, void* value,
                                       const ChangeFixedValueRangeFunc& func,
diff --git a/be/src/vec/exec/scan/vscan_node.h b/be/src/vec/exec/scan/vscan_node.h
index 6279330be..836d6a67d 100644
--- a/be/src/vec/exec/scan/vscan_node.h
+++ b/be/src/vec/exec/scan/vscan_node.h
@@ -215,8 +215,21 @@ protected:
     phmap::flat_hash_map<int, std::pair<SlotDescriptor*, ColumnValueRangeType>>
             _slot_id_to_value_range;
     // column -> ColumnValueRange
+    // We use _colname_to_value_range to store a column and its conresponding value ranges.
     std::unordered_map<std::string, ColumnValueRangeType> _colname_to_value_range;
-    // We use _colname_to_value_range to store a column and its corresponding value ranges.
+    /**
+     * _colname_to_value_range only store the leaf of and in the conjunct expr tree,
+     * we use _compound_value_ranges to store conresponding value ranges 
+     * in the one compound relationship except the leaf of and node,
+     * such as `where a > 1 or b > 10 and c < 200`, the expr tree like: 
+     *     or
+     *   /   \ 
+     *  a     and
+     *       /   \
+     *      b     c
+     * the value ranges of column a,b,c will all store into _compound_value_ranges
+     */
+    std::vector<ColumnValueRangeType> _compound_value_ranges;
     // But if a col is with value range, eg: 1 < col < 10, which is "!is_fixed_range",
     // in this case we can not merge "1 < col < 10" with "col not in (2)".
     // So we have to save "col not in (2)" to another structure: "_not_in_value_ranges".
@@ -309,6 +322,18 @@ private:
                                              SlotDescriptor* slot, ColumnValueRange<T>& range,
                                              PushDownType* pdt);
 
+    Status _normalize_compound_predicate(
+            vectorized::VExpr* expr, VExprContext* expr_ctx, PushDownType* pdt,
+            const std::function<bool(const std::vector<VExpr*>&, const VSlotRef**, VExpr**)>&
+                    in_predicate_checker,
+            const std::function<bool(const std::vector<VExpr*>&, const VSlotRef**, VExpr**)>&
+                    eq_predicate_checker);
+
+    template <PrimitiveType T>
+    Status _normalize_binary_in_compound_predicate(vectorized::VExpr* expr, VExprContext* expr_ctx,
+                                                   SlotDescriptor* slot, ColumnValueRange<T>& range,
+                                                   PushDownType* pdt);
+
     template <PrimitiveType T>
     Status _normalize_is_null_predicate(vectorized::VExpr* expr, VExprContext* expr_ctx,
                                         SlotDescriptor* slot, ColumnValueRange<T>& range,
diff --git a/be/src/vec/exec/vjdbc_connector.cpp b/be/src/vec/exec/vjdbc_connector.cpp
index 5a656392a..9aed3c20b 100644
--- a/be/src/vec/exec/vjdbc_connector.cpp
+++ b/be/src/vec/exec/vjdbc_connector.cpp
@@ -417,8 +417,19 @@ Status JdbcConnector::_convert_column_data(JNIEnv* env, jobject jobj,
         M(TYPE_FLOAT, float, vectorized::ColumnVector<vectorized::Float32>)
         M(TYPE_DOUBLE, double, vectorized::ColumnVector<vectorized::Float64>)
 #undef M
+    case TYPE_CHAR: {
+        std::string data = _jobject_to_string(env, jobj);
+        // Now have test pg and oracle with char(100), if data='abc'
+        // but read string data length is 100, so need trim extra spaces
+        if ((_conn_param.table_type == TOdbcTableType::POSTGRESQL) ||
+            (_conn_param.table_type == TOdbcTableType::ORACLE)) {
+            data = data.erase(data.find_last_not_of(' ') + 1);
+        }
+        reinterpret_cast<vectorized::ColumnString*>(col_ptr)->insert_data(data.c_str(),
+                                                                          data.length());
+        break;
+    }
     case TYPE_STRING:
-    case TYPE_CHAR:
     case TYPE_VARCHAR: {
         std::string data = _jobject_to_string(env, jobj);
         reinterpret_cast<vectorized::ColumnString*>(col_ptr)->insert_data(data.c_str(),
diff --git a/be/src/vec/exec/vjdbc_connector.h b/be/src/vec/exec/vjdbc_connector.h
index e6da1a015..84ca17e02 100644
--- a/be/src/vec/exec/vjdbc_connector.h
+++ b/be/src/vec/exec/vjdbc_connector.h
@@ -33,6 +33,7 @@ struct JdbcConnectorParam {
     std::string user;
     std::string passwd;
     std::string query_string;
+    TOdbcTableType::type table_type;
 
     const TupleDescriptor* tuple_desc;
 };
diff --git a/be/src/vec/exprs/vectorized_fn_call.cpp b/be/src/vec/exprs/vectorized_fn_call.cpp
index 399959971..cfb2657ff 100644
--- a/be/src/vec/exprs/vectorized_fn_call.cpp
+++ b/be/src/vec/exprs/vectorized_fn_call.cpp
@@ -104,12 +104,38 @@ doris::Status VectorizedFnCall::execute(VExprContext* context, doris::vectorized
     size_t num_columns_without_result = block->columns();
     // prepare a column to save result
     block->insert({nullptr, _data_type, _expr_name});
+    if (_function->can_fast_execute()) {
+        bool ok = fast_execute(context->fn_context(_fn_context_index), *block, arguments,
+                               num_columns_without_result, block->rows());
+        if (ok) {
+            *result_column_id = num_columns_without_result;
+            return Status::OK();
+        }
+    }
+
     RETURN_IF_ERROR(_function->execute(context->fn_context(_fn_context_index), *block, arguments,
                                        num_columns_without_result, block->rows(), false));
     *result_column_id = num_columns_without_result;
     return Status::OK();
 }
 
+// fast_execute can direct copy expr filter result which build by apply index in segment_iterator
+bool VectorizedFnCall::fast_execute(FunctionContext* context, Block& block,
+                                    const ColumnNumbers& arguments, size_t result,
+                                    size_t input_rows_count) {
+    auto query_value = block.get_by_position(arguments[1]).to_string(0);
+    std::string column_name = block.get_by_position(arguments[0]).name;
+    auto result_column_name = column_name + "_" + _function->get_name() + "_" + query_value;
+    if (!block.has(result_column_name)) {
+        return false;
+    }
+
+    auto result_column =
+            block.get_by_name(result_column_name).column->convert_to_full_column_if_const();
+    block.replace_by_position(result, std::move(result_column));
+    return true;
+}
+
 const std::string& VectorizedFnCall::expr_name() const {
     return _expr_name;
 }
diff --git a/be/src/vec/exprs/vectorized_fn_call.h b/be/src/vec/exprs/vectorized_fn_call.h
index 63a11847d..9434f02c9 100644
--- a/be/src/vec/exprs/vectorized_fn_call.h
+++ b/be/src/vec/exprs/vectorized_fn_call.h
@@ -35,6 +35,9 @@ public:
     std::string debug_string() const override;
     static std::string debug_string(const std::vector<VectorizedFnCall*>& exprs);
 
+    bool fast_execute(FunctionContext* context, Block& block, const ColumnNumbers& arguments,
+                      size_t result, size_t input_rows_count);
+
 private:
     FunctionBasePtr _function;
     std::string _expr_name;
diff --git a/be/src/vec/exprs/vliteral.cpp b/be/src/vec/exprs/vliteral.cpp
index 27324e5c0..a0ef2deb1 100644
--- a/be/src/vec/exprs/vliteral.cpp
+++ b/be/src/vec/exprs/vliteral.cpp
@@ -191,11 +191,8 @@ Status VLiteral::execute(VExprContext* context, vectorized::Block* block, int* r
     return Status::OK();
 }
 
-std::string VLiteral::debug_string() const {
+std::string VLiteral::value() const {
     std::stringstream out;
-    out << "VLiteral (name = " << _expr_name;
-    out << ", type = " << _data_type->get_name();
-    out << ", value = (";
     for (size_t i = 0; i < _column_ptr->size(); i++) {
         if (i != 0) {
             out << ", ";
@@ -283,8 +280,17 @@ std::string VLiteral::debug_string() const {
             }
         }
     }
+    return out.str();
+}
+
+std::string VLiteral::debug_string() const {
+    std::stringstream out;
+    out << "VLiteral (name = " << _expr_name;
+    out << ", type = " << _data_type->get_name();
+    out << ", value = (" << value();
     out << "))";
     return out.str();
 }
+
 } // namespace vectorized
 } // namespace doris
diff --git a/be/src/vec/exprs/vliteral.h b/be/src/vec/exprs/vliteral.h
index c5c1c4706..90f40be81 100644
--- a/be/src/vec/exprs/vliteral.h
+++ b/be/src/vec/exprs/vliteral.h
@@ -37,6 +37,8 @@ public:
     VExpr* clone(doris::ObjectPool* pool) const override { return pool->add(new VLiteral(*this)); }
     std::string debug_string() const override;
 
+    std::string value() const;
+
 protected:
     ColumnPtr _column_ptr;
     std::string _expr_name;
diff --git a/be/src/vec/functions/function.h b/be/src/vec/functions/function.h
index 9e00f36a3..334497d0c 100644
--- a/be/src/vec/functions/function.h
+++ b/be/src/vec/functions/function.h
@@ -144,6 +144,8 @@ public:
 
     virtual bool is_stateful() const { return false; }
 
+    virtual bool can_fast_execute() const { return false; }
+
     /** Should we evaluate this function while constant folding, if arguments are constants?
       * Usually this is true. Notable counterexample is function 'sleep'.
       * If we will call it during query analysis, we will sleep extra amount of time.
@@ -527,6 +529,12 @@ public:
 
     bool is_deterministic() const override { return function->is_deterministic(); }
 
+    bool can_fast_execute() const override {
+        return function->get_name() == "eq" || function->get_name() == "ne" ||
+               function->get_name() == "lt" || function->get_name() == "gt" ||
+               function->get_name() == "le" || function->get_name() == "ge";
+    }
+
     bool is_deterministic_in_scope_of_query() const override {
         return function->is_deterministic_in_scope_of_query();
     }
diff --git a/be/src/vec/functions/function_binary_arithmetic.h b/be/src/vec/functions/function_binary_arithmetic.h
index 5c98e7248..2a8da748e 100644
--- a/be/src/vec/functions/function_binary_arithmetic.h
+++ b/be/src/vec/functions/function_binary_arithmetic.h
@@ -730,10 +730,9 @@ public:
                     if constexpr (!std::is_same_v<ResultDataType, InvalidType>) {
                         if constexpr (IsDataTypeDecimal<LeftDataType> &&
                                       IsDataTypeDecimal<RightDataType>) {
-                            ResultDataType result_type = decimal_result_type(
-                                    left, right, OpTraits::is_multiply, OpTraits::is_division);
-                            type_res = std::make_shared<ResultDataType>(result_type.get_precision(),
-                                                                        result_type.get_scale());
+                            type_res = decimal_result_type(left, right, OpTraits::is_multiply,
+                                                           OpTraits::is_division,
+                                                           OpTraits::is_plus_minus);
                         } else if constexpr (IsDataTypeDecimal<LeftDataType>) {
                             type_res = std::make_shared<LeftDataType>(left.get_precision(),
                                                                       left.get_scale());
diff --git a/be/src/vec/functions/function_jsonb.cpp b/be/src/vec/functions/function_jsonb.cpp
index ea84ddf3a..02f352fb5 100644
--- a/be/src/vec/functions/function_jsonb.cpp
+++ b/be/src/vec/functions/function_jsonb.cpp
@@ -18,6 +18,7 @@
 #include <boost/token_functions.hpp>
 #include <vector>
 
+// #include "util/jsonb_parser_simd.h"
 #include "util/string_parser.hpp"
 #include "util/string_util.h"
 #include "vec/columns/column.h"
@@ -47,7 +48,7 @@ enum class JsonbParseErrorMode { FAIL = 0, RETURN_NULL, RETURN_VALUE, RETURN_INV
 template <NullalbeMode nullable_mode, JsonbParseErrorMode parse_error_handle_mode>
 class FunctionJsonbParseBase : public IFunction {
 private:
-    JsonbParser default_value_parser;
+    JsonbParserSIMD default_value_parser;
     bool has_const_default_value = false;
 
 public:
@@ -193,6 +194,10 @@ public:
         size_t size = col_from.size();
         col_to->reserve(size);
 
+        // parser can be reused for performance
+        JsonbParserSIMD parser;
+        JsonbErrType error = JsonbErrType::E_NONE;
+
         for (size_t i = 0; i < input_rows_count; ++i) {
             if (col_from.is_null_at(i)) {
                 null_map->get_data()[i] = 1;
@@ -201,8 +206,6 @@ public:
             }
 
             const auto& val = col_from_string->get_data_at(i);
-            JsonbParser parser;
-            JsonbErrType error = JsonbErrType::E_NONE;
             if (parser.parse(val.data, val.size)) {
                 // insert jsonb format data
                 col_to->insert_data(parser.getWriter().getOutput()->getBuffer(),
diff --git a/be/src/vec/runtime/vdata_stream_recvr.cpp b/be/src/vec/runtime/vdata_stream_recvr.cpp
index 0195640ec..831bf79c1 100644
--- a/be/src/vec/runtime/vdata_stream_recvr.cpp
+++ b/be/src/vec/runtime/vdata_stream_recvr.cpp
@@ -295,7 +295,8 @@ VDataStreamRecvr::VDataStreamRecvr(
         std::shared_ptr<QueryStatisticsRecvr> sub_plan_query_statistics_recvr)
         : _mgr(stream_mgr),
 #ifdef USE_MEM_TRACKER
-          _state(state),
+          _query_mem_tracker(state->query_mem_tracker()),
+          _query_id(state->query_id()),
 #endif
           _fragment_instance_id(fragment_instance_id),
           _dest_node_id(dest_node_id),
@@ -366,8 +367,7 @@ Status VDataStreamRecvr::create_merger(const std::vector<VExprContext*>& orderin
 
 void VDataStreamRecvr::add_block(const PBlock& pblock, int sender_id, int be_number,
                                  int64_t packet_seq, ::google::protobuf::Closure** done) {
-    SCOPED_ATTACH_TASK(_state->query_mem_tracker(), print_id(_state->query_id()),
-                       _fragment_instance_id);
+    SCOPED_ATTACH_TASK(_query_mem_tracker, print_id(_query_id), _fragment_instance_id);
     int use_sender_id = _is_merging ? sender_id : 0;
     _sender_queues[use_sender_id]->add_block(pblock, be_number, packet_seq, done);
 }
diff --git a/be/src/vec/runtime/vdata_stream_recvr.h b/be/src/vec/runtime/vdata_stream_recvr.h
index 13ab43810..cddd21e6b 100644
--- a/be/src/vec/runtime/vdata_stream_recvr.h
+++ b/be/src/vec/runtime/vdata_stream_recvr.h
@@ -104,7 +104,8 @@ private:
     VDataStreamMgr* _mgr;
 
 #ifdef USE_MEM_TRACKER
-    RuntimeState* _state;
+    std::shared_ptr<MemTrackerLimiter> _query_mem_tracker;
+    TUniqueId _query_id;
 #endif
 
     // Fragment and node id of the destination exchange node this receiver is used by.
diff --git a/be/test/CMakeLists.txt b/be/test/CMakeLists.txt
index ce33cd1fd..0d57b06bc 100644
--- a/be/test/CMakeLists.txt
+++ b/be/test/CMakeLists.txt
@@ -96,6 +96,7 @@ set(OLAP_TEST_FILES
     olap/byte_buffer_test.cpp
     olap/lru_cache_test.cpp
     olap/bloom_filter_test.cpp
+    olap/itoken_extractor_test.cpp
     olap/file_helper_test.cpp
     olap/file_utils_test.cpp
     olap/cumulative_compaction_policy_test.cpp
diff --git a/be/test/io/cache/remote_file_cache_test.cpp b/be/test/io/cache/remote_file_cache_test.cpp
index 6c917a5f2..596ff8872 100644
--- a/be/test/io/cache/remote_file_cache_test.cpp
+++ b/be/test/io/cache/remote_file_cache_test.cpp
@@ -141,7 +141,7 @@ protected:
         EXPECT_NE("", writer.min_encoded_key().to_string());
         EXPECT_NE("", writer.max_encoded_key().to_string());
 
-        st = segment_v2::Segment::open(fs, path, "", 0, {}, query_schema, res);
+        st = segment_v2::Segment::open(fs, path, 0, {}, query_schema, res);
         EXPECT_TRUE(st.ok());
         EXPECT_EQ(nrows, (*res)->num_rows());
     }
@@ -172,7 +172,7 @@ protected:
 
         std::vector<segment_v2::SegmentSharedPtr> segments;
         Status st = rowset.load_segments(&segments);
-        ASSERT_TRUE(st.ok());
+        ASSERT_TRUE(st.ok()) << st;
     }
 };
 
diff --git a/be/test/olap/itoken_extractor_test.cpp b/be/test/olap/itoken_extractor_test.cpp
new file mode 100644
index 000000000..d57682a1e
--- /dev/null
+++ b/be/test/olap/itoken_extractor_test.cpp
@@ -0,0 +1,78 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include "olap/itoken_extractor.h"
+
+#include <gtest/gtest.h>
+
+#include <string>
+
+#include "common/logging.h"
+#include "util/utf8_check.h"
+
+namespace doris {
+
+class TestITokenExtractor : public testing::Test {
+public:
+    void SetUp() {}
+    void TearDown() {}
+};
+
+void runNextInString(const ITokenExtractor& extractor, std::string statement,
+                     std::vector<std::string> expect) {
+    ASSERT_TRUE(validate_utf8(statement.c_str(), statement.length()));
+
+    std::vector<std::string> actual;
+    actual.reserve(expect.size());
+    size_t pos = 0;
+    size_t token_start = 0;
+    size_t token_length = 0;
+    while (extractor.next_in_string(statement.c_str(), statement.size(), &pos, &token_start,
+                                    &token_length)) {
+        actual.push_back(statement.substr(token_start, token_length));
+    }
+    ASSERT_EQ(expect, actual);
+}
+
+void runNextInStringLike(const ITokenExtractor& extractor, std::string statement,
+                         std::vector<std::string> expect) {
+    std::vector<std::string> actual;
+    actual.reserve(expect.size());
+    size_t pos = 0;
+    std::string str;
+    while (extractor.next_in_string_like(statement.c_str(), statement.length(), &pos, str)) {
+        actual.push_back(str);
+    }
+    ASSERT_EQ(expect, actual);
+}
+
+TEST_F(TestITokenExtractor, ngram_extractor) {
+    std::string statement = u8"È¢ÑËÆ°09ÂèëÂ∏Éi13ÊâãÊú∫„ÄÇ";
+    std::vector<std::string> expect = {u8"È¢ÑËÆ°", u8"ËÆ°0", u8"09",  u8"9Âèë",  u8"ÂèëÂ∏É", u8"Â∏Éi",
+                                       u8"i1",   u8"13",  u8"3Êâã", u8"ÊâãÊú∫", u8"Êú∫„ÄÇ"};
+    NgramTokenExtractor ngram_extractor(2);
+    runNextInString(ngram_extractor, statement, expect);
+}
+
+TEST_F(TestITokenExtractor, ngram_like_extractor) {
+    NgramTokenExtractor ngram_extractor(2);
+    runNextInStringLike(ngram_extractor, u8"%ÊâãÊú∫%", {u8"ÊâãÊú∫"});
+    runNextInStringLike(ngram_extractor, u8"%Êú∫%", {});
+    runNextInStringLike(ngram_extractor, {u8"i_%ÊâãÊú∫%"}, {u8"ÊâãÊú∫"});
+    runNextInStringLike(ngram_extractor, {u8"\\_ÊâãÊú∫%"}, {u8"_Êâã", u8"ÊâãÊú∫"});
+}
+} // namespace doris
diff --git a/be/test/olap/rowset/segment_v2/block_bloom_filter_test.cpp b/be/test/olap/rowset/segment_v2/block_bloom_filter_test.cpp
index 9c3f15dcd..e48612253 100644
--- a/be/test/olap/rowset/segment_v2/block_bloom_filter_test.cpp
+++ b/be/test/olap/rowset/segment_v2/block_bloom_filter_test.cpp
@@ -174,5 +174,34 @@ TEST_F(BlockBloomFilterTest, slice) {
     EXPECT_FALSE(bf->test_bytes(s.data, s.size));
 }
 
+// Test contains
+TEST_F(BlockBloomFilterTest, contains) {
+    std::unique_ptr<BloomFilter> bf1;
+    auto st1 = BloomFilter::create(NGRAM_BLOOM_FILTER, &bf1, 512);
+    ASSERT_TRUE(st1.ok());
+    ASSERT_NE(nullptr, bf1);
+    ASSERT_TRUE(st1.ok());
+    ASSERT_TRUE(bf1->size() > 0);
+
+    std::unique_ptr<BloomFilter> bf2;
+    auto st2 = BloomFilter::create(NGRAM_BLOOM_FILTER, &bf2, 512);
+    ASSERT_TRUE(st2.ok());
+    ASSERT_NE(nullptr, bf2);
+    ASSERT_TRUE(st2.ok());
+    ASSERT_TRUE(bf2->size() > 0);
+
+    std::vector<std::string> str_list = {"abc", "csx", "d2", "csxx", "vaa"};
+    for (int i = 0; i < str_list.size(); ++i) {
+        auto str = str_list[i];
+        bf1->add_bytes(str.data(), str.size());
+        if (1 == i % 2) {
+            bf2->add_bytes(str.data(), str.size());
+        }
+    }
+
+    ASSERT_TRUE(bf1->contains(*bf2));
+    ASSERT_FALSE(bf2->contains(*bf1));
+}
+
 } // namespace segment_v2
 } // namespace doris
diff --git a/be/test/olap/tablet_test.cpp b/be/test/olap/tablet_test.cpp
index b1f949489..db3ffb02d 100644
--- a/be/test/olap/tablet_test.cpp
+++ b/be/test/olap/tablet_test.cpp
@@ -21,6 +21,7 @@
 
 #include <sstream>
 
+#include "http/action/pad_rowset_action.h"
 #include "olap/olap_define.h"
 #include "olap/rowset/beta_rowset.h"
 #include "olap/storage_engine.h"
@@ -28,6 +29,7 @@
 #include "olap/tablet_meta.h"
 #include "olap/tablet_schema_cache.h"
 #include "testutil/mock_rowset.h"
+#include "util/file_utils.h"
 #include "util/time.h"
 
 using namespace std;
@@ -38,6 +40,8 @@ using namespace ErrorCode;
 using RowsetMetaSharedContainerPtr = std::shared_ptr<std::vector<RowsetMetaSharedPtr>>;
 
 static StorageEngine* k_engine = nullptr;
+static const std::string kTestDir = "/data_test/data/tablet_test";
+static const uint32_t MAX_PATH_LEN = 1024;
 
 class TestTablet : public testing::Test {
 public:
@@ -65,6 +69,17 @@ public:
             },
             "creation_time": 1553765670
         })";
+        char buffer[MAX_PATH_LEN];
+        EXPECT_NE(getcwd(buffer, MAX_PATH_LEN), nullptr);
+        absolute_dir = std::string(buffer) + kTestDir;
+
+        if (FileUtils::check_exist(absolute_dir)) {
+            EXPECT_TRUE(FileUtils::remove_all(absolute_dir).ok());
+        }
+        EXPECT_TRUE(FileUtils::create_dir(absolute_dir).ok());
+        EXPECT_TRUE(FileUtils::create_dir(absolute_dir + "/tablet_path").ok());
+        _data_dir = std::make_unique<DataDir>(absolute_dir);
+        _data_dir->update_capacity();
 
         doris::EngineOptions options;
         k_engine = new StorageEngine(options);
@@ -72,6 +87,9 @@ public:
     }
 
     void TearDown() override {
+        if (FileUtils::check_exist(absolute_dir)) {
+            EXPECT_TRUE(FileUtils::remove_all(absolute_dir).ok());
+        }
         if (k_engine != nullptr) {
             k_engine->stop();
             delete k_engine;
@@ -197,6 +215,8 @@ public:
 protected:
     std::string _json_rowset_meta;
     TabletMetaSharedPtr _tablet_meta;
+    string absolute_dir;
+    std::unique_ptr<DataDir> _data_dir;
 };
 
 TEST_F(TestTablet, delete_expired_stale_rowset) {
@@ -225,6 +245,41 @@ TEST_F(TestTablet, delete_expired_stale_rowset) {
     _tablet.reset();
 }
 
+TEST_F(TestTablet, pad_rowset) {
+    std::vector<RowsetMetaSharedPtr> rs_metas;
+    auto ptr1 = std::make_shared<RowsetMeta>();
+    init_rs_meta(ptr1, 1, 2);
+    rs_metas.push_back(ptr1);
+    RowsetSharedPtr rowset1 = make_shared<BetaRowset>(nullptr, "", ptr1);
+
+    auto ptr2 = std::make_shared<RowsetMeta>();
+    init_rs_meta(ptr2, 3, 4);
+    rs_metas.push_back(ptr2);
+    RowsetSharedPtr rowset2 = make_shared<BetaRowset>(nullptr, "", ptr2);
+
+    auto ptr3 = std::make_shared<RowsetMeta>();
+    init_rs_meta(ptr3, 6, 7);
+    rs_metas.push_back(ptr3);
+    RowsetSharedPtr rowset3 = make_shared<BetaRowset>(nullptr, "", ptr3);
+
+    for (auto& rowset : rs_metas) {
+        _tablet_meta->add_rs_meta(rowset);
+    }
+
+    _data_dir->init();
+    TabletSharedPtr _tablet(new Tablet(_tablet_meta, _data_dir.get()));
+    _tablet->init();
+
+    Version version(5, 5);
+    std::vector<RowsetReaderSharedPtr> readers;
+    ASSERT_FALSE(_tablet->capture_rs_readers(version, &readers).ok());
+    readers.clear();
+
+    PadRowsetAction action;
+    action._pad_rowset(_tablet, version);
+    ASSERT_TRUE(_tablet->capture_rs_readers(version, &readers).ok());
+}
+
 TEST_F(TestTablet, cooldown_policy) {
     std::vector<RowsetMetaSharedPtr> rs_metas;
     RowsetMetaSharedPtr ptr1(new RowsetMeta());
diff --git a/docs/en/community/developer-guide/debug-tool.md b/docs/en/community/developer-guide/debug-tool.md
index 4e7f50e9b..96c705b45 100644
--- a/docs/en/community/developer-guide/debug-tool.md
+++ b/docs/en/community/developer-guide/debug-tool.md
@@ -235,6 +235,59 @@ From the above output, we can see that 1024 bytes have been leaked, and the stac
 
 **NOTE: if the LSAN switch is turned on, the TCMalloc will be automatically turned off**
 
+#### JEMALLOC HEAP PROFILE
+
+##### 1. runtime heap dump by http
+No need to restart BE, use jemalloc heap dump http interface, jemalloc generates heap dump file on the corresponding BE machine according to the current memory usage.
+
+The directory where the heap dump file is located can be configured through the ``jeprofile_dir`` variable in ``be.conf``, and the default is ``${DORIS_HOME}/log``
+
+```shell
+curl http://be_host:be_webport/jeheap/dump
+```
+
+##### 2. heap dump by JEMALLOC_CONF
+Perform heap dump by restarting BE after changing the `JEMALLOC_CONF` variable in `start_be.sh`
+
+1. Dump every 1MB:
+
+   Two new variable settings `prof:true,lg_prof_interval:20` have been added to the `JEMALLOC_CONF` variable, where `prof:true` is to enable profiling, and `lg_prof_interval:20` means that a dump is generated every 1MB (2^20)
+2. Dump each time a new high is reached:
+
+   Added two variable settings `prof:true,prof_gdump:true` in the `JEMALLOC_CONF` variable, where `prof:true` is to enable profiling, and `prof_gdump:true` means to generate a dump when the memory usage reaches a new high
+3. Memory leak dump when the program exits:
+
+   Added three new variable settings `prof_leak: true, lg_prof_sample: 0, prof_final: true` in the `JEMALLOC_CONF` variable
+
+
+#### 3. jemalloc heap dump profiling
+
+3.1 Generating plain text analysis results
+```shell
+jeprof lib/doris_be --base=heap_dump_file_1 heap_dump_file_2
+```
+
+3.2 Generate call relationship picture
+
+ Install dependencies required for plotting
+ ```shell
+ yum install ghostscript graphviz
+ ```
+ Multiple dump files can be generated by running the above command multiple times in a short period of time, and the first dump file can be selected as the baseline for diff comparison analysis
+
+ ```shell
+ jeprof --dot lib/doris_be --base=heap_dump_file_1 heap_dump_file_2
+ ```
+ After executing the above command, the terminal will output a diagram of dot syntax, and paste it to [online dot drawing website](http://www.webgraphviz.com/), generate a memory allocation diagram, and then analyze it. This method can Drawing directly through the terminal output results is more suitable for servers where file transfer is not very convenient.
+
+ You can also use the following command to directly generate the call relationship result.pdf file and transfer it to the local for viewing
+ ```shell
+ jeprof --pdf lib/doris_be --base=heap_dump_file_1 heap_dump_file_2 > result.pdf
+ ```
+
+In the above jeprof related commands, remove the `--base` option to analyze only a single heap dump file
+
+
 #### ASAN
 
 Except for the unreasonable use and leakage of memory. Sometimes there will be memory access illegal address and other errors. At this time, we can use [ASAN](https://github.com/google/sanitizers/wiki/addresssanitizer) to help us find the cause of the problem. Like LSAN, ASAN is integrated into GCC. Doris can open this function by compiling as follows
diff --git a/docs/en/docs/admin-manual/config/be-config.md b/docs/en/docs/admin-manual/config/be-config.md
index a6e236f02..00ecb77a8 100644
--- a/docs/en/docs/admin-manual/config/be-config.md
+++ b/docs/en/docs/admin-manual/config/be-config.md
@@ -57,13 +57,13 @@ There are two ways to configure BE configuration items:
    After BE starts, the configuration items can be dynamically set with the following commands.
 
     ```
-    curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}'
+    curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}
     ```
 
    In version 0.13 and before, the configuration items modified in this way will become invalid after the BE process restarts. In 0.14 and later versions, the modified configuration can be persisted through the following command. The modified configuration items are stored in the `be_custom.conf` file.
 
     ```
-    curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}&persis=true
+    curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}\&persist=true
     ```
 
 ## Examples
diff --git a/docs/en/docs/admin-manual/http-actions/pad-rowset.md b/docs/en/docs/admin-manual/http-actions/pad-rowset.md
new file mode 100644
index 000000000..9ff6b8905
--- /dev/null
+++ b/docs/en/docs/admin-manual/http-actions/pad-rowset.md
@@ -0,0 +1,41 @@
+---
+{
+    "title": "PAD ROWSET",
+    "language": "en"
+}
+---
+
+<!-- 
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# PAD ROWSET
+## description
+   
+    Pad one empty rowset as one substitute for error replica.
+
+    METHOD: POST
+    URI: http://be_host:be_http_port/api/pad_rowset?tablet_id=xxx&start_version=xxx&end_version=xxx
+
+## example
+
+    curl -X POST "http://hostname:8088/api/pad_rowset?tablet_id=123456\&start_version=1111111\$end_version=1111112"
+
+## keyword
+
+    ROWSET,TABLET,ROWSET,TABLET
diff --git a/docs/en/docs/data-operate/import/import-way/stream-load-manual.md b/docs/en/docs/data-operate/import/import-way/stream-load-manual.md
index 51103df3b..1d8d7e025 100644
--- a/docs/en/docs/data-operate/import/import-way/stream-load-manual.md
+++ b/docs/en/docs/data-operate/import/import-way/stream-load-manual.md
@@ -65,8 +65,7 @@ The final result of the import is returned to the user by Coordinator BE.
 
 ## Support data format
 
-Currently Stream Load supports two data formats: CSV (text) and JSON
-
+Stream Load currently supports data formats: CSV (text), JSON, <version since="1.2" type="inline"> PARQUET and ORC</version>.
 ## Basic operations
 ### Create Load
 
diff --git a/docs/en/docs/data-operate/import/load-manual.md b/docs/en/docs/data-operate/import/load-manual.md
index 84469e7e0..1eedeed42 100644
--- a/docs/en/docs/data-operate/import/load-manual.md
+++ b/docs/en/docs/data-operate/import/load-manual.md
@@ -60,8 +60,8 @@ Different import methods support slightly different data formats.
 
 | Import Methods | Supported Formats       |
 | -------------- | ----------------------- |
-| Broker Load    | Parquet, ORC, csv, gzip |
-| Stream Load    | csv, gzip, json         |
+| Broker Load    | parquet, orc, csv, gzip |
+| Stream Load    | csv, json, parquet, orc |
 | Routine Load   | csv, json               |
 
 ## import instructions
diff --git a/docs/en/docs/data-table/index/ngram-bloomfilter-index.md b/docs/en/docs/data-table/index/ngram-bloomfilter-index.md
new file mode 100644
index 000000000..331b78468
--- /dev/null
+++ b/docs/en/docs/data-table/index/ngram-bloomfilter-index.md
@@ -0,0 +1,79 @@
+---
+{
+    "title": "NGram BloomFilter Index",
+    "language": "en"
+}
+---
+
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Doris NGram BloomFilter Index
+
+In order to improve the like query performance, the NGram BloomFilter index was implemented, which referenced to the ClickHouse's ngrambf skip indices;
+
+## Create Column With NGram BloomFilter Index
+
+During create tableÔºö
+
+```sql
+CREATE TABLE `table3` (
+  `siteid` int(11) NULL DEFAULT "10" COMMENT "",
+  `citycode` smallint(6) NULL COMMENT "",
+  `username` varchar(100) NULL DEFAULT "" COMMENT "",
+  INDEX idx_ngrambf (`username`) USING NGRAM_BF PROPERTIES("gram_size"="3", "bf_size"="256") COMMENT 'username ngram_bf index'
+) ENGINE=OLAP
+AGGREGATE KEY(`siteid`, `citycode`, `username`) COMMENT "OLAP"
+DISTRIBUTED BY HASH(`siteid`) BUCKETS 10
+PROPERTIES (
+"replication_num" = "1"
+);
+
+-- PROPERTIES("gram_size"="3", "bf_size"="1024")Ôºåindicate the number of gram and bytes of bloom filter respectively.
+-- the gram size set to same as the like query pattern string length. and the suitable bytes of bloom filter can be get by test, more larger more better, 256 maybe is a good start.
+-- Usually, if the data's cardinality is small, you can increase the bytes of bloom filter to improve the efficiency.
+```
+
+## Show NGram BloomFilter Index
+
+```sql
+show index from example_db.table3;
+```
+
+## Drop NGram BloomFilter Index
+
+
+```sql
+alter table example_db.table3 drop index idx_ngrambf;
+```
+
+## Add NGram BloomFilter Index
+
+Add NGram BloomFilter Index for old column:
+
+```sql
+alter table example_db.table3 add index idx_ngrambf(username) using NGRAM_BF PROPERTIES("gram_size"="3", "bf_size"="512")comment 'username ngram_bf index' 
+```
+
+## **Some notes about Doris NGram BloomFilter**
+
+1. NGram BloomFilter only support CHAR/VARCHAR/String column.
+2. NGram BloomFilter index and BloomFilter index should be exclusive on same column
+3. The gram number and bytes of BloomFilter can be adjust and optimize. Like if gram is too small, you can increase the bytes of BloomFilter.
+4. To find some query whether use the NGram BloomFilter index, you can check the query profile.
diff --git a/docs/en/docs/ecosystem/flink-doris-connector.md b/docs/en/docs/ecosystem/flink-doris-connector.md
index e5d947bab..15a2caf2f 100644
--- a/docs/en/docs/ecosystem/flink-doris-connector.md
+++ b/docs/en/docs/ecosystem/flink-doris-connector.md
@@ -139,6 +139,8 @@ Add flink-doris-connector Maven dependencies
 
 1. Please replace the corresponding Connector and Flink dependency versions according to different Flink and Scala versions. Version 1.1.0 only supports Flink1.14
 
+2. You can also download the relevant version jar package from [here](https://repo.maven.apache.org/maven2/org/apache/doris/).
+
 ## How to use
 
 There are three ways to use Flink Doris Connector. 
@@ -419,9 +421,17 @@ The most suitable scenario for using Flink Doris Connector is to synchronize sou
 1. The Flink Doris Connector mainly relies on Checkpoint for streaming writing, so the interval between Checkpoints is the visible delay time of the data.
 2. To ensure the Exactly Once semantics of Flink, the Flink Doris Connector enables two-phase commit by default, and Doris enables two-phase commit by default after version 1.1. 1.0 can be enabled by modifying the BE parameters, please refer to [two_phase_commit](../data-operate/import/import-way/stream-load-manual.md).
 
-### common problem
+## FAQ
+
+1. **After Doris Source finishes reading data, why does the stream end?**
+
+Currently Doris Source is a bounded stream and does not support CDC reading.
+
+2. **Can Flink read Doris and perform conditional pushdown?**
 
-1. **Bitmap type write**
+By configuring the doris.filter.query parameter, refer to the configuration section for details.
+
+3. **How to write Bitmap type?**
 
 ```sql
 CREATE TABLE bitmap_sink (
@@ -439,12 +449,28 @@ WITH (
    'sink.properties.columns' = 'dt,page,user_id,user_id=to_bitmap(user_id)'
 )
 ````
-2. **errCode = 2, detailMessage = Label [label_0_1] has already been used, relate to txn [19650]**
+4. **errCode = 2, detailMessage = Label [label_0_1] has already been used, relate to txn [19650]**
 
 In the Exactly-Once scenario, the Flink Job must be restarted from the latest Checkpoint/Savepoint, otherwise the above error will be reported.
 When Exactly-Once is not required, it can also be solved by turning off 2PC commits (sink.enable-2pc=false) or changing to a different sink.label-prefix.
 
-3. **errCode = 2, detailMessage = transaction [19650] not found**
+5. **errCode = 2, detailMessage = transaction [19650] not found**
 
 Occurred in the Commit phase, the transaction ID recorded in the checkpoint has expired on the FE side, and the above error will occur when committing again at this time.
-At this time, it cannot be started from the checkpoint, and the expiration time can be extended by modifying the streaming_label_keep_max_second configuration in fe.conf, which defaults to 12 hours.
\ No newline at end of file
+At this time, it cannot be started from the checkpoint, and the expiration time can be extended by modifying the streaming_label_keep_max_second configuration in fe.conf, which defaults to 12 hours.
+
+6. **errCode = 2, detailMessage = current running txns on db 10006 is 100, larger than limit 100**
+
+This is because the concurrent import of the same library exceeds 100, which can be solved by adjusting the parameter `max_running_txn_num_per_db` of fe.conf. For details, please refer to [max_running_txn_num_per_db](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/fe-config/#max_running_txn_num_per_db)
+
+7. **How to ensure the order of a batch of data when Flink writes to the Uniq model?**
+
+You can add sequence column configuration to ensure that, for details, please refer to [sequence](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual)
+
+8. **The Flink task does not report an error, but the data cannot be synchronized? **
+
+Before Connector1.1.0, it was written in batches, and the writing was driven by data. It was necessary to determine whether there was data written upstream. After 1.1.0, it depends on Checkpoint, and Checkpoint must be enabled to write.
+
+9. **tablet writer write failed, tablet_id=190958, txn_id=3505530, err=-235**
+
+It usually occurs before Connector1.1.0, because the writing frequency is too fast, resulting in too many versions. The frequency of Streamload can be reduced by setting the sink.batch.size and sink.batch.interval parameters.
\ No newline at end of file
diff --git a/docs/en/docs/install/source-install/compilation.md b/docs/en/docs/install/source-install/compilation.md
index 727ad228a..ecc063616 100644
--- a/docs/en/docs/install/source-install/compilation.md
+++ b/docs/en/docs/install/source-install/compilation.md
@@ -107,8 +107,6 @@ This document focuses on how to code Doris through source code.
     After starting the mirror, you should be in the container. The Doris source code can be downloaded from the following command (local source directory mounted is not required):
 
     ```
-    $ wget https://dist.apache.org/repos/dist/dev/doris/xxx.tar.gz
-    or
     $ git clone https://github.com/apache/doris.git
     ```
 
diff --git a/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_double.md b/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_double.md
index ef6e18d0a..2b7a6dddc 100644
--- a/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_double.md
+++ b/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_double.md
@@ -37,6 +37,8 @@ Use [] to denote array subscripts, starting at 0.
 The content of path cannot contain ",[and].
 If the json_string format is incorrect, or the json_path format is incorrect, or matches cannot be found, NULL is returned.
 
+In addition, it is recommended to use the jsonb type and jsonb_extract_XXX function performs the same function.
+
 ### example
 
 1. Get the value of key as "k1"
diff --git a/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_int.md b/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_int.md
index 20144d9fd..4b5a63f93 100644
--- a/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_int.md
+++ b/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_int.md
@@ -37,6 +37,8 @@ Use [] to denote array subscripts, starting at 0.
 The content of path cannot contain ",[and].
 If the json_string format is incorrect, or the json_path format is incorrect, or matches cannot be found, NULL is returned.
 
+In addition, it is recommended to use the jsonb type and jsonb_extract_XXX function performs the same function.
+
 ### example
 
 1. Get the value of key as "k1"
diff --git a/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_string.md b/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_string.md
index 746750e99..50ece140e 100644
--- a/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_string.md
+++ b/docs/en/docs/sql-manual/sql-functions/json-functions/get_json_string.md
@@ -37,6 +37,8 @@ Use [] to denote array subscripts, starting at 0.
 The content of path cannot contain ",[and].
 If the json_string format is incorrect, or the json_path format is incorrect, or matches cannot be found, NULL is returned.
 
+In addition, it is recommended to use the jsonb type and jsonb_extract_XXX function performs the same function.
+
 ### example
 
 1. Get the value of key as "k1"
diff --git a/docs/en/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md b/docs/en/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md
index ccba690ec..e5dfb6cef 100644
--- a/docs/en/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md
+++ b/docs/en/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md
@@ -25,6 +25,13 @@ under the License.
 -->
 
 ## jsonb_extract
+
+<version since="1.2.0">
+
+jsonb_extract
+
+</version>
+
 ### description
 
 jsonb_extract functions extract field specified by json_path from JSONB. A series of functions are provided for different datatype.
diff --git a/docs/en/docs/sql-manual/sql-functions/string-functions/uuid.md b/docs/en/docs/sql-manual/sql-functions/string-functions/uuid.md
index 328fe9c18..5000354d5 100644
--- a/docs/en/docs/sql-manual/sql-functions/string-functions/uuid.md
+++ b/docs/en/docs/sql-manual/sql-functions/string-functions/uuid.md
@@ -25,6 +25,13 @@ under the License.
 -->
 
 ## uuid
+
+<version since="1.2.0">
+
+uuid
+
+</version>
+
 ### description
 #### Syntax
 
diff --git a/docs/en/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md b/docs/en/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md
index 088eebabe..50d655d65 100644
--- a/docs/en/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md
+++ b/docs/en/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md
@@ -61,6 +61,12 @@ DROP STORAGE POLICY policy_name1
    ```sql
    DROP ROW POLICY test_row_policy_1 on table1 for test
    ```
+
+3. Drop the storage policy named policy_name1
+```sql
+DROP STORAGE POLICY policy_name1
+```
+
 ### Keywords
 
     DROP, POLICY
diff --git a/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md b/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md
index 000d0784b..0313c3f34 100644
--- a/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md
+++ b/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md
@@ -99,7 +99,7 @@ Parameter introduction:
 
 11. exec_mem_limit: Import memory limit. Default is 2GB. The unit is bytes.
 
-12. format: Specify the import data format, the default is csv, and csv_with_names(filter out the first row of your csv file), csv_with_names_and_types(filter out the first two lines of your csv file), json format are supported.
+12. format: Specify load data format, support csv, json, <version since="1.2" type="inline"> csv_with_names(support csv file line header filter), csv_with_names_and_types(support csv file first two lines filter), parquet, orc</version>, default is csv.
 
 13. jsonpaths: The way of importing json is divided into: simple mode and matching mode.
 
diff --git a/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md b/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md
index a8aa259e8..215769aa5 100644
--- a/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md
+++ b/docs/en/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md
@@ -69,7 +69,7 @@ SELECT
 
    6. `INTO OUTFILE 'file_name' ` : save the result to a new file (which did not exist before), the difference lies in the save format.
 
-   7. `Group by having`: Group the result set, and brush the result of group by when having appears. `Grouping Sets`, `Rollup`, `Cube` are extensions of group by, please refer to [GROUPING SETS DESIGN](../../../../../community/design/grouping_sets_design.md) for details.
+   7. `Group by having`: Group the result set, and brush the result of group by when having appears. `Grouping Sets`, `Rollup`, `Cube` are extensions of group by, please refer to [GROUPING SETS DESIGN](https://doris.apache.org/community/design/grouping_sets_design) for details.
 
    8. `Order by`: Sort the final result, Order by sorts the result set by comparing the size of one or more columns.
 
diff --git a/docs/sidebars.json b/docs/sidebars.json
index 2a4ef60cb..3318bcbd6 100644
--- a/docs/sidebars.json
+++ b/docs/sidebars.json
@@ -1119,6 +1119,7 @@
                             ]
                         },
                         "admin-manual/http-actions/restore-tablet",
+                        "admin-manual/http-actions/pad-rowset",
                         "admin-manual/http-actions/get-load-state",
                         "admin-manual/http-actions/tablet-migration-action",
                         "admin-manual/http-actions/cancel-label",
diff --git a/docs/zh-CN/community/developer-guide/debug-tool.md b/docs/zh-CN/community/developer-guide/debug-tool.md
index bf56879c2..b4952f9a1 100644
--- a/docs/zh-CN/community/developer-guide/debug-tool.md
+++ b/docs/zh-CN/community/developer-guide/debug-tool.md
@@ -199,6 +199,59 @@ Total: 1296.4 MB
 
 Ëøô‰∏™ÂëΩ‰ª§ÁöÑËæìÂá∫‰∏éHEAP PROFILEÁöÑËæìÂá∫ÂèäÊü•ÁúãÊñπÂºè‰∏ÄÊ†∑ÔºåËøôÈáåÂ∞±‰∏çÂÜçËØ¶ÁªÜËØ¥Êòé„ÄÇËøô‰∏™ÂëΩ‰ª§Âè™ÊúâÂú®ÊâßË°åÁöÑËøáÁ®ã‰∏≠Êâç‰ºöÂºÄÂêØÁªüËÆ°ÔºåÁõ∏ÊØîHEAP PROFILEÂØπ‰∫éËøõÁ®ãÊÄßËÉΩÁöÑÂΩ±ÂìçÊúâÈôê„ÄÇ
 
+#### JEMALLOC HEAP PROFILE
+
+##### 1. runtime heap dump by http 
+Êó†ÈúÄÈáçÂêØBE, ‰ΩøÁî®jemalloc heap dump httpÊé•Âè£ÔºåjemallocÊ†πÊçÆÂΩìÂâçÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµÔºåÂú®ÂØπÂ∫îÁöÑBEÊú∫Âô®‰∏äÁîüÊàêheap dumpÊñá‰ª∂„ÄÇ
+
+heap dumpÊñá‰ª∂ÊâÄÂú®ÁõÆÂΩïÂèØ‰ª•Âú® ``be.conf`` ‰∏≠ÈÄöËøá``jeprofile_dir``ÂèòÈáèËøõË°åÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫``${DORIS_HOME}/log``
+
+```shell
+curl http://be_host:be_webport/jeheap/dump
+```
+
+##### 2. heap dump by JEMALLOC_CONF
+ÈÄöËøáÊõ¥Êîπ`start_be.sh` ‰∏≠`JEMALLOC_CONF` ÂèòÈáèÂêéÈáçÊñ∞ÂêØÂä®BE Êù•ËøõË°åheap dump
+
+1. ÊØè1MB dump‰∏ÄÊ¨°:
+
+   `JEMALLOC_CONF`ÂèòÈáè‰∏≠Êñ∞Â¢û‰∏§‰∏™ÂèòÈáèËÆæÁΩÆ`prof:true,lg_prof_interval:20`  ÂÖ∂‰∏≠`prof:true`ÊòØÊâìÂºÄprofilingÔºå`lg_prof_interval:20`‰∏≠Ë°®Á§∫ÊØè1MB(2^20)ÁîüÊàê‰∏ÄÊ¨°dump 
+2. ÊØèÊ¨°ËææÂà∞Êñ∞È´òÊó∂dump:
+   
+   `JEMALLOC_CONF`ÂèòÈáè‰∏≠Êñ∞Â¢û‰∏§‰∏™ÂèòÈáèËÆæÁΩÆ`prof:true,prof_gdump:true` ÂÖ∂‰∏≠`prof:true`ÊòØÊâìÂºÄprofilingÔºå`prof_gdump:true` ‰ª£Ë°®ÂÜÖÂ≠ò‰ΩøÁî®ËææÂà∞Êñ∞È´òÊó∂ÁîüÊàêdump
+3. Á®ãÂ∫èÈÄÄÂá∫Êó∂ÂÜÖÂ≠òÊ≥ÑÊºèdump:
+   
+   `JEMALLOC_CONF`ÂèòÈáè‰∏≠Êñ∞Â¢û‰∏â‰∏™ÂèòÈáèËÆæÁΩÆ`prof_leak:true,lg_prof_sample:0,prof_final:true`
+
+
+#### 3. jemalloc heap dump profiling
+
+3.1  ÁîüÊàêÁ∫ØÊñáÊú¨ÂàÜÊûêÁªìÊûú
+   ```shell
+   jeprof lib/doris_be --base=heap_dump_file_1 heap_dump_file_2
+   ```
+   
+3.2 ÁîüÊàêË∞ÉÁî®ÂÖ≥Á≥ªÂõæÁâá
+
+   ÂÆâË£ÖÁªòÂõæÊâÄÈúÄÁöÑ‰æùËµñÈ°π
+   ```shell
+   yum install ghostscript graphviz
+   ```
+   ÈÄöËøáÂú®‰∏ÄÁü≠Êó∂Èó¥ÂÜÖÂ§öÊ¨°ËøêË°å‰∏äËø∞ÂëΩ‰ª§ÂèØ‰ª•ÁîüÊàêÂ§ö‰ªΩdump Êñá‰ª∂ÔºåÂèØ‰ª•ÈÄâÂèñÁ¨¨‰∏Ä‰ªΩdump Êñá‰ª∂‰Ωú‰∏∫baseline ËøõË°ådiffÂØπÊØîÂàÜÊûê
+   
+   ```shell
+   jeprof --dot lib/doris_be --base=heap_dump_file_1 heap_dump_file_2
+   ```
+   ÊâßË°åÂÆå‰∏äËø∞ÂëΩ‰ª§ÔºåÁªàÁ´Ø‰∏≠‰ºöËæìÂá∫dotËØ≠Ê≥ïÁöÑÂõæÔºåÂ∞ÜÂÖ∂Ë¥¥Âà∞[Âú®Á∫ødotÁªòÂõæÁΩëÁ´ô](http://www.webgraphviz.com/)ÔºåÁîüÊàêÂÜÖÂ≠òÂàÜÈÖçÂõæÔºåÁÑ∂ÂêéËøõË°åÂàÜÊûêÔºåÊ≠§ÁßçÊñπÂºèËÉΩÂ§üÁõ¥Êé•ÈÄöËøáÁªàÁ´ØËæìÂá∫ÁªìÊûúËøõË°åÁªòÂõæÔºåÊØîËæÉÈÄÇÁî®‰∫é‰º†ËæìÊñá‰ª∂‰∏çÊòØÂæàÊñπ‰æøÁöÑÊúçÂä°Âô®„ÄÇ
+   
+   ‰πüÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ãÂëΩ‰ª§Áõ¥Êé•ÁîüÊàêË∞ÉÁî®ÂÖ≥Á≥ªresult.pdfÊñá‰ª∂‰º†ËæìÂà∞Êú¨Âú∞ÂêéËøõË°åÊü•Áúã
+   ```shell
+   jeprof --pdf lib/doris_be --base=heap_dump_file_1 heap_dump_file_2 > result.pdf
+   ```
+   
+‰∏äËø∞jeprofÁõ∏ÂÖ≥ÂëΩ‰ª§‰∏≠ÂùáÂéªÊéâ `--base` ÈÄâÈ°πÊù•Âè™ÂàÜÊûêÂçï‰∏™heap dumpÊñá‰ª∂
+
+
 #### LSAN
 
 [LSAN](https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer)ÊòØ‰∏Ä‰∏™Âú∞ÂùÄÊ£ÄÊü•Â∑•ÂÖ∑ÔºåGCCÂ∑≤ÁªèÈõÜÊàê„ÄÇÂú®Êàë‰ª¨ÁºñËØë‰ª£Á†ÅÁöÑÊó∂ÂÄôÂºÄÂêØÁõ∏Â∫îÁöÑÁºñËØëÈÄâÈ°πÔºåÂ∞±ËÉΩÂ§üÂºÄÂêØËøô‰∏™ÂäüËÉΩ„ÄÇÂΩìÁ®ãÂ∫èÂèëÁîüÂèØ‰ª•Á°ÆÂÆöÁöÑÂÜÖÂ≠òÊ≥ÑÈú≤Êó∂Ôºå‰ºöÂ∞ÜÊ≥ÑÈú≤Â†ÜÊ†àÊâìÂç∞„ÄÇDoris BEÂ∑≤ÁªèÈõÜÊàê‰∫ÜËøô‰∏™Â∑•ÂÖ∑ÔºåÂè™ÈúÄË¶ÅÂú®ÁºñËØëÁöÑÊó∂ÂÄô‰ΩøÁî®Â¶Ç‰∏ãÁöÑÂëΩ‰ª§ËøõË°åÁºñËØëÂ∞±ËÉΩÂ§üÁîüÊàêÂ∏¶ÊúâÂÜÖÂ≠òÊ≥ÑÈú≤Ê£ÄÊµãÁâàÊú¨ÁöÑBE‰∫åËøõÂà∂
diff --git a/docs/zh-CN/docs/admin-manual/config/be-config.md b/docs/zh-CN/docs/admin-manual/config/be-config.md
index 008274586..c37fd2e40 100644
--- a/docs/zh-CN/docs/admin-manual/config/be-config.md
+++ b/docs/zh-CN/docs/admin-manual/config/be-config.md
@@ -55,13 +55,13 @@ BE ÁöÑÈÖçÁΩÆÈ°πÊúâ‰∏§ÁßçÊñπÂºèËøõË°åÈÖçÁΩÆÔºö
 BE ÂêØÂä®ÂêéÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Âä®ÊÄÅËÆæÁΩÆÈÖçÁΩÆÈ°π„ÄÇ
 
   ```
-  curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}'
+  curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}
   ```
 
 Âú® 0.13 ÁâàÊú¨Âèä‰πãÂâçÔºåÈÄöËøáËØ•ÊñπÂºè‰øÆÊîπÁöÑÈÖçÁΩÆÈ°πÂ∞ÜÂú® BE ËøõÁ®ãÈáçÂêØÂêéÂ§±Êïà„ÄÇÂú® 0.14 Âèä‰πãÂêéÁâàÊú¨‰∏≠ÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÊåÅ‰πÖÂåñ‰øÆÊîπÂêéÁöÑÈÖçÁΩÆ„ÄÇ‰øÆÊîπÂêéÁöÑÈÖçÁΩÆÈ°πÂ≠òÂÇ®Âú® `be_custom.conf` Êñá‰ª∂‰∏≠„ÄÇ
 
   ```
-  curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}&persist=true
+  curl -X POST http://{be_ip}:{be_http_port}/api/update_config?{key}={value}\&persist=true
   ```
 
 ## Â∫îÁî®‰∏æ‰æã
diff --git a/docs/zh-CN/docs/admin-manual/http-actions/pad_rowset.md b/docs/zh-CN/docs/admin-manual/http-actions/pad_rowset.md
new file mode 100644
index 000000000..dba241fca
--- /dev/null
+++ b/docs/zh-CN/docs/admin-manual/http-actions/pad_rowset.md
@@ -0,0 +1,43 @@
+---
+{
+    "title": "PAD ROWSET",
+    "language": "zh-CN"
+}
+---
+
+<!-- 
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# PAD ROWSET
+## description
+   
+    ËØ•ÂäüËÉΩÁî®‰∫é‰ΩøÁî®‰∏Ä‰∏™Á©∫ÁöÑrowsetÂ°´ÂÖÖÊçüÂùèÁöÑÂâØÊú¨„ÄÇ
+
+    ËØ¥ÊòéÔºöËøô‰∏™ÂäüËÉΩÊöÇÊó∂Âè™Âú®beÊúçÂä°‰∏≠Êèê‰æõ‰∏Ä‰∏™httpÊé•Âè£„ÄÇÂ¶ÇÊûúË¶Å‰ΩøÁî®Ôºå
+    ÈúÄË¶ÅÂêëË¶ÅËøõË°åÊï∞ÊçÆÊÅ¢Â§çÁöÑÈÇ£Âè∞beÊú∫Âô®ÁöÑhttpÁ´ØÂè£ÂèëÈÄÅpad rowset apiËØ∑Ê±Ç„ÄÇapiÊ†ºÂºèÂ¶Ç‰∏ãÔºö
+    METHOD: POST
+    URI: http://be_host:be_http_port/api/pad_rowset?tablet_id=xxx&start_version=xxx&end_version=xxx
+
+## example
+
+    curl -X POST "http://hostname:8088/api/pad_rowset?tablet_id=123456\&start_version=1111111\&end_version=1111112"
+
+## keyword
+
+    PAD,ROWSET,PAD,ROWSET
diff --git a/docs/zh-CN/docs/data-operate/import/import-way/stream-load-manual.md b/docs/zh-CN/docs/data-operate/import/import-way/stream-load-manual.md
index 0d4526ce2..c82849e51 100644
--- a/docs/zh-CN/docs/data-operate/import/import-way/stream-load-manual.md
+++ b/docs/zh-CN/docs/data-operate/import/import-way/stream-load-manual.md
@@ -65,7 +65,7 @@ Stream load ‰∏≠ÔºåDoris ‰ºöÈÄâÂÆö‰∏Ä‰∏™ËäÇÁÇπ‰Ωú‰∏∫ Coordinator ËäÇÁÇπ„ÄÇËØ•ËäÇ
 
 ## ÊîØÊåÅÊï∞ÊçÆÊ†ºÂºè
 
-ÁõÆÂâç Stream Load ÊîØÊåÅ‰∏§‰∏™Êï∞ÊçÆÊ†ºÂºèÔºöCSVÔºàÊñáÊú¨Ôºâ Âíå JSON
+ÁõÆÂâç Stream Load ÊîØÊåÅÊï∞ÊçÆÊ†ºÂºèÔºöCSVÔºàÊñáÊú¨Ôºâ„ÄÅJSON„ÄÅ<version since="1.2" type="inline"> PARQUET Âíå ORC</version>„ÄÇ
 
 ## Âü∫Êú¨Êìç‰Ωú
 
diff --git a/docs/zh-CN/docs/data-operate/import/load-manual.md b/docs/zh-CN/docs/data-operate/import/load-manual.md
index 4d8d80bac..ab4407ebb 100644
--- a/docs/zh-CN/docs/data-operate/import/load-manual.md
+++ b/docs/zh-CN/docs/data-operate/import/load-manual.md
@@ -58,11 +58,11 @@ Doris Êèê‰æõÂ§öÁßçÊï∞ÊçÆÂØºÂÖ•ÊñπÊ°àÔºåÂèØ‰ª•ÈíàÂØπ‰∏çÂêåÁöÑÊï∞ÊçÆÊ∫êËøõË°åÈÄâ
 
 ‰∏çÂêåÁöÑÂØºÂÖ•ÊñπÂºèÊîØÊåÅÁöÑÊï∞ÊçÆÊ†ºÂºèÁï•Êúâ‰∏çÂêå„ÄÇ
 
-| ÂØºÂÖ•ÊñπÂºè     | ÊîØÊåÅÁöÑÊ†ºÂºè              |
+| ÂØºÂÖ•ÊñπÂºè     | ÊîØÊåÅÁöÑÊ†ºÂºè                |
 | ------------ | ----------------------- |
-| Broker Load  | ParquetÔºåORCÔºåcsvÔºågzip |
-| Stream Load  | csv, gzip, json         |
-| Routine Load | csv, json               |
+| Broker Load  | parquet„ÄÅorc„ÄÅcsv„ÄÅgzip |
+| Stream Load  | csv„ÄÅjson„ÄÅparquet„ÄÅorc |
+| Routine Load | csv„ÄÅjson               |
 
 ## ÂØºÂÖ•ËØ¥Êòé
 
diff --git a/docs/zh-CN/docs/data-table/index/ngram-bloomfilter-index.md b/docs/zh-CN/docs/data-table/index/ngram-bloomfilter-index.md
new file mode 100644
index 000000000..5c94803f5
--- /dev/null
+++ b/docs/zh-CN/docs/data-table/index/ngram-bloomfilter-index.md
@@ -0,0 +1,81 @@
+---
+{
+    "title": "NGram BloomFilterÁ¥¢Âºï",
+    "language": "zh-CN"
+}
+---
+
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Doris NGram BloomFilterÁ¥¢ÂºïÂèä‰ΩøÁî®‰ΩøÁî®Âú∫ÊôØ
+
+‰∏∫‰∫ÜÊèêÂçálikeÁöÑÊü•ËØ¢ÊÄßËÉΩÔºåÂ¢ûÂä†‰∫ÜNGram BloomFilterÁ¥¢ÂºïÔºåÂÖ∂ÂÆûÁé∞‰∏ªË¶ÅÂèÇÁÖß‰∫ÜClickHouseÁöÑngrambf„ÄÇ
+
+## NGram BloomFilterÂàõÂª∫
+
+Ë°®ÂàõÂª∫Êó∂ÊåáÂÆöÔºö
+
+```sql
+CREATE TABLE `table3` (
+  `siteid` int(11) NULL DEFAULT "10" COMMENT "",
+  `citycode` smallint(6) NULL COMMENT "",
+  `username` varchar(32) NULL DEFAULT "" COMMENT "",
+  INDEX idx_ngrambf (`username`) USING NGRAM_BF PROPERTIES("gram_size"="3", "bf_size"="256") COMMENT 'username ngram_bf index'
+) ENGINE=OLAP
+AGGREGATE KEY(`siteid`, `citycode`, `username`) COMMENT "OLAP"
+DISTRIBUTED BY HASH(`siteid`) BUCKETS 10
+PROPERTIES (
+"replication_num" = "1"
+);
+
+-- PROPERTIES("gram_size"="3", "bf_size"="256")ÔºåÂàÜÂà´Ë°®Á§∫gramÁöÑ‰∏™Êï∞Âíåbloom filterÁöÑÂ≠óËäÇÊï∞„ÄÇ
+-- gramÁöÑ‰∏™Êï∞Ë∑üÂÆûÈôÖÊü•ËØ¢Âú∫ÊôØÁõ∏ÂÖ≥ÔºåÈÄöÂ∏∏ËÆæÁΩÆ‰∏∫Â§ßÈÉ®ÂàÜÊü•ËØ¢Â≠óÁ¨¶‰∏≤ÁöÑÈïøÂ∫¶Ôºåbloom filterÂ≠óËäÇÊï∞ÔºåÂèØ‰ª•ÈÄöËøáÊµãËØïÂæóÂá∫ÔºåÈÄöÂ∏∏Ë∂äÂ§ßËøáÊª§ÊïàÊûúË∂äÂ•ΩÔºåÂèØ‰ª•‰ªé256ÂºÄÂßãËøõË°åÈ™åËØÅÊµãËØïÁúãÁúãÊïàÊûú„ÄÇÂΩìÁÑ∂Â≠óËäÇÊï∞Ë∂äÂ§ß‰πü‰ºöÂ∏¶Êù•Á¥¢ÂºïÂ≠òÂÇ®„ÄÅÂÜÖÂ≠òcost‰∏äÂçá„ÄÇ
+-- Â¶ÇÊûúÊï∞ÊçÆÂü∫Êï∞ÊØîËæÉÈ´òÔºåÂ≠óËäÇÊï∞ÂèØ‰ª•‰∏çÁî®ËÆæÁΩÆËøáÂ§ßÔºåÂ¶ÇÊûúÂü∫Êï∞‰∏çÊòØÂæàÈ´òÔºåÂèØ‰ª•ÈÄöËøáÂ¢ûÂä†Â≠óËäÇÊï∞Êù•ÊèêÂçáËøáÊª§ÊïàÊûú„ÄÇ
+```
+
+## Êü•ÁúãNGram BloomFilterÁ¥¢Âºï
+
+Êü•ÁúãÊàë‰ª¨Âú®Ë°®‰∏äÂª∫Á´ãÁöÑNGram BloomFilterÁ¥¢ÂºïÊòØ‰ΩøÁî®:
+
+```sql
+show index from example_db.table3;
+```
+
+## Âà†Èô§NGram BloomFilterÁ¥¢Âºï
+
+
+```sql
+alter table example_db.table3 drop index idx_ngrambf;
+```
+
+## ‰øÆÊîπNGram BloomFilterÁ¥¢Âºï
+
+‰∏∫Â∑≤ÊúâÂàóÊñ∞Â¢ûNGram BloomFilterÁ¥¢ÂºïÔºö
+
+```sql
+alter table example_db.table3 add index idx_ngrambf(username) using NGRAM_BF PROPERTIES("gram_size"="2", "bf_size"="512")comment 'username ngram_bf index' 
+```
+
+## **Doris NGram BloomFilter‰ΩøÁî®Ê≥®ÊÑè‰∫ãÈ°π**
+
+1. NGram BloomFilterÂè™ÊîØÊåÅÂ≠óÁ¨¶‰∏≤Âàó
+2. NGram BloomFilterÁ¥¢ÂºïÂíåBloomFilterÁ¥¢Âºï‰∏∫‰∫íÊñ•ÂÖ≥Á≥ªÔºåÂç≥Âêå‰∏Ä‰∏™ÂàóÂè™ËÉΩËÆæÁΩÆ‰∏§ËÄÖ‰∏≠ÁöÑ‰∏Ä‰∏™
+3. NGramÂ§ßÂ∞èÂíåBloomFilterÁöÑÂ≠óËäÇÊï∞ÔºåÂèØ‰ª•Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµË∞É‰ºòÔºåÂ¶ÇÊûúNGramÊØîËæÉÂ∞èÔºåÂèØ‰ª•ÈÄÇÂΩìÂ¢ûÂä†BloomFilterÂ§ßÂ∞è
+4. Â¶ÇÊûúË¶ÅÊü•ÁúãÊüê‰∏™Êü•ËØ¢ÊòØÂê¶ÂëΩ‰∏≠‰∫ÜNGram Bloom FilterÁ¥¢ÂºïÔºåÂèØ‰ª•ÈÄöËøáÊü•ËØ¢ÁöÑProfile‰ø°ÊÅØÊü•Áúã
diff --git a/docs/zh-CN/docs/ecosystem/flink-doris-connector.md b/docs/zh-CN/docs/ecosystem/flink-doris-connector.md
index 0be44b154..90438ed2e 100644
--- a/docs/zh-CN/docs/ecosystem/flink-doris-connector.md
+++ b/docs/zh-CN/docs/ecosystem/flink-doris-connector.md
@@ -144,6 +144,8 @@ enable_http_server_v2 = true
 
 1.ËØ∑Ê†πÊçÆ‰∏çÂêåÁöÑ Flink Âíå Scala ÁâàÊú¨ÊõøÊç¢ÂØπÂ∫îÁöÑ Connector Âíå Flink ‰æùËµñÁâàÊú¨„ÄÇ
 
+2.‰πüÂèØ‰ªé[ËøôÈáå](https://repo.maven.apache.org/maven2/org/apache/doris/)‰∏ãËΩΩÁõ∏ÂÖ≥ÁâàÊú¨jarÂåÖ„ÄÇ 
+
 ## ‰ΩøÁî®ÊñπÊ≥ï
 
 Flink ËØªÂÜô Doris Êï∞ÊçÆ‰∏ªË¶ÅÊúâ‰∏§ÁßçÊñπÂºè
@@ -416,9 +418,17 @@ insert into doris_sink select id,name from cdc_mysql_source;
 1. Flink Doris Connector‰∏ªË¶ÅÊòØ‰æùËµñCheckpointËøõË°åÊµÅÂºèÂÜôÂÖ•ÔºåÊâÄ‰ª•CheckpointÁöÑÈó¥ÈöîÂç≥‰∏∫Êï∞ÊçÆÁöÑÂèØËßÅÂª∂ËøüÊó∂Èó¥„ÄÇ
 2. ‰∏∫‰∫Ü‰øùËØÅFlinkÁöÑExactly OnceËØ≠‰πâÔºåFlink Doris Connector ÈªòËÆ§ÂºÄÂêØ‰∏§Èò∂ÊÆµÊèê‰∫§ÔºåDorisÂú®1.1ÁâàÊú¨ÂêéÈªòËÆ§ÂºÄÂêØ‰∏§Èò∂ÊÆµÊèê‰∫§„ÄÇ1.0ÂèØÈÄöËøá‰øÆÊîπBEÂèÇÊï∞ÂºÄÂêØÔºåÂèØÂèÇËÄÉ[two_phase_commit](../data-operate/import/import-way/stream-load-manual.md)„ÄÇ
 
-### Â∏∏ËßÅÈóÆÈ¢ò
+## Â∏∏ËßÅÈóÆÈ¢ò
+
+1. **Doris SourceÂú®Êï∞ÊçÆËØªÂèñÂÆåÊàêÂêéÔºåÊµÅ‰∏∫‰ªÄ‰πàÂ∞±ÁªìÊùü‰∫ÜÔºü**
+
+ÁõÆÂâçDoris SourceÊòØÊúâÁïåÊµÅÔºå‰∏çÊîØÊåÅCDCÊñπÂºèËØªÂèñ„ÄÇ
+
+2. **FlinkËØªÂèñDorisÂèØ‰ª•ËøõË°åÊù°‰ª∂‰∏ãÊé®ÂêóÔºü**
 
-1. **BitmapÁ±ªÂûãÂÜôÂÖ•**
+ÈÄöËøáÈÖçÁΩÆdoris.filter.queryÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉÈÖçÁΩÆÂ∞èËäÇ„ÄÇ
+
+3. **Â¶Ç‰ΩïÂÜôÂÖ•BitmapÁ±ªÂûãÔºü**
 
 ```sql
 CREATE TABLE bitmap_sink (
@@ -436,12 +446,28 @@ WITH (
   'sink.properties.columns' = 'dt,page,user_id,user_id=to_bitmap(user_id)'
 )
 ```
-2. **errCode = 2, detailMessage = Label [label_0_1] has already been used, relate to txn [19650]**
+4. **errCode = 2, detailMessage = Label [label_0_1] has already been used, relate to txn [19650]**
 
 Exactly-OnceÂú∫ÊôØ‰∏ãÔºåFlink JobÈáçÂêØÊó∂ÂøÖÈ°ª‰ªéÊúÄÊñ∞ÁöÑCheckpoint/SavepointÂêØÂä®ÔºåÂê¶Âàô‰ºöÊä•Â¶Ç‰∏äÈîôËØØ„ÄÇ
 ‰∏çË¶ÅÊ±ÇExactly-OnceÊó∂Ôºå‰πüÂèØÈÄöËøáÂÖ≥Èó≠2PCÊèê‰∫§Ôºàsink.enable-2pc=falseÔºâ ÊàñÊõ¥Êç¢‰∏çÂêåÁöÑsink.label-prefixËß£ÂÜ≥„ÄÇ
 
-3. **errCode = 2, detailMessage = transaction [19650] not found**
+5. **errCode = 2, detailMessage = transaction [19650] not found**
 
 ÂèëÁîüÂú®CommitÈò∂ÊÆµÔºåcheckpointÈáåÈù¢ËÆ∞ÂΩïÁöÑ‰∫ãÂä°IDÔºåÂú®FE‰æßÂ∑≤ÁªèËøáÊúüÔºåÊ≠§Êó∂ÂÜçÊ¨°commitÂ∞±‰ºöÂá∫Áé∞‰∏äËø∞ÈîôËØØ„ÄÇ
 Ê≠§Êó∂Êó†Ê≥ï‰ªécheckpointÂêØÂä®ÔºåÂêéÁª≠ÂèØÈÄöËøá‰øÆÊîπfe.confÁöÑstreaming_label_keep_max_secondÈÖçÁΩÆÊù•Âª∂ÈïøËøáÊúüÊó∂Èó¥ÔºåÈªòËÆ§12Â∞èÊó∂„ÄÇ
+
+6. **errCode = 2, detailMessage = current running txns on db 10006 is 100, larger than limit 100**
+
+ËøôÊòØÂõ†‰∏∫Âêå‰∏Ä‰∏™Â∫ìÂπ∂ÂèëÂØºÂÖ•Ë∂ÖËøá‰∫Ü100ÔºåÂèØÈÄöËøáË∞ÉÊï¥ fe.confÁöÑÂèÇÊï∞ `max_running_txn_num_per_db` Êù•Ëß£ÂÜ≥„ÄÇÂÖ∑‰ΩìÂèØÂèÇËÄÉ [max_running_txn_num_per_db](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/fe-config/#max_running_txn_num_per_db)
+
+7. **FlinkÂÜôÂÖ•UniqÊ®°ÂûãÊó∂ÔºåÂ¶Ç‰Ωï‰øùËØÅ‰∏ÄÊâπÊï∞ÊçÆÁöÑÊúâÂ∫èÊÄßÔºü**
+
+ÂèØ‰ª•Ê∑ªÂä†sequenceÂàóÈÖçÁΩÆÊù•‰øùËØÅÔºåÂÖ∑‰ΩìÂèØÂèÇËÄÉ [sequence](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual)
+
+8. **Flink‰ªªÂä°Ê≤°Êä•ÈîôÔºå‰ΩÜÊòØÊó†Ê≥ïÂêåÊ≠•Êï∞ÊçÆÔºü**
+
+Connector1.1.0ÁâàÊú¨‰ª•ÂâçÔºåÊòØÊîíÊâπÂÜôÂÖ•ÁöÑÔºåÂÜôÂÖ•ÂùáÊòØÁî±Êï∞ÊçÆÈ©±Âä®ÔºåÈúÄË¶ÅÂà§Êñ≠‰∏äÊ∏∏ÊòØÂê¶ÊúâÊï∞ÊçÆÂÜôÂÖ•„ÄÇ1.1.0‰πãÂêéÔºå‰æùËµñCheckpointÔºåÂøÖÈ°ªÂºÄÂêØCheckpointÊâçËÉΩÂÜôÂÖ•„ÄÇ
+
+9. **tablet writer write failed, tablet_id=190958, txn_id=3505530, err=-235**
+
+ÈÄöÂ∏∏ÂèëÁîüÂú®Connector1.1.0‰πãÂâçÔºåÊòØÁî±‰∫éÂÜôÂÖ•È¢ëÁéáËøáÂø´ÔºåÂØºËá¥ÁâàÊú¨ËøáÂ§ö„ÄÇÂèØ‰ª•ÈÄöËøáËÆæÁΩÆsink.batch.size Âíå sink.batch.intervalÂèÇÊï∞Êù•Èôç‰ΩéStreamloadÁöÑÈ¢ëÁéá„ÄÇ
\ No newline at end of file
diff --git a/docs/zh-CN/docs/install/source-install/compilation.md b/docs/zh-CN/docs/install/source-install/compilation.md
index dbe1fe8a7..d28a9da9c 100644
--- a/docs/zh-CN/docs/install/source-install/compilation.md
+++ b/docs/zh-CN/docs/install/source-install/compilation.md
@@ -106,8 +106,6 @@ under the License.
    ÂêØÂä®ÈïúÂÉèÂêéÔºå‰Ω†Â∫îËØ•Â∑≤ÁªèÂ§Ñ‰∫éÂÆπÂô®ÂÜÖ„ÄÇÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§‰∏ãËΩΩ Doris Ê∫êÁ†ÅÔºàÂ∑≤ÊåÇËΩΩÊú¨Âú∞Ê∫êÁ†ÅÁõÆÂΩïÂàô‰∏çÁî®ÔºâÔºö
 
     ```
-    $ wget https://dist.apache.org/repos/dist/dev/doris/xxx.tar.gz
-    or
     $ git clone https://github.com/apache/doris.git
     ```
 
diff --git a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_double.md b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_double.md
index 057fc7309..22612b12f 100644
--- a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_double.md
+++ b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_double.md
@@ -37,6 +37,8 @@ under the License.
 path ÁöÑÂÜÖÂÆπ‰∏çËÉΩÂåÖÂê´ ", [ Âíå ]„ÄÇ
 Â¶ÇÊûú json_string Ê†ºÂºè‰∏çÂØπÔºåÊàñ json_path Ê†ºÂºè‰∏çÂØπÔºåÊàñÊó†Ê≥ïÊâæÂà∞ÂåπÈÖçÈ°πÔºåÂàôËøîÂõû NULL„ÄÇ
 
+Âè¶Â§ñÔºåÊé®Ëçê‰ΩøÁî®jsonbÁ±ªÂûãÂíåjsonb_extract_XXXÂáΩÊï∞ÂÆûÁé∞ÂêåÊ†∑ÁöÑÂäüËÉΩ„ÄÇ
+
 ### example
 
 1. Ëé∑Âèñ key ‰∏∫ "k1" ÁöÑ value
diff --git a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_int.md b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_int.md
index 5135e1b85..ecc29de09 100644
--- a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_int.md
+++ b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_int.md
@@ -37,6 +37,8 @@ under the License.
 path ÁöÑÂÜÖÂÆπ‰∏çËÉΩÂåÖÂê´ ", [ Âíå ]„ÄÇ
 Â¶ÇÊûú json_string Ê†ºÂºè‰∏çÂØπÔºåÊàñ json_path Ê†ºÂºè‰∏çÂØπÔºåÊàñÊó†Ê≥ïÊâæÂà∞ÂåπÈÖçÈ°πÔºåÂàôËøîÂõû NULL„ÄÇ
 
+Âè¶Â§ñÔºåÊé®Ëçê‰ΩøÁî®jsonbÁ±ªÂûãÂíåjsonb_extract_XXXÂáΩÊï∞ÂÆûÁé∞ÂêåÊ†∑ÁöÑÂäüËÉΩ„ÄÇ
+
 ### example
 
 1. Ëé∑Âèñ key ‰∏∫ "k1" ÁöÑ value
diff --git a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_string.md b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_string.md
index ce022fabb..58d475915 100644
--- a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_string.md
+++ b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/get_json_string.md
@@ -37,6 +37,8 @@ under the License.
 path ÁöÑÂÜÖÂÆπ‰∏çËÉΩÂåÖÂê´ ", [ Âíå ]„ÄÇ
 Â¶ÇÊûú json_string Ê†ºÂºè‰∏çÂØπÔºåÊàñ json_path Ê†ºÂºè‰∏çÂØπÔºåÊàñÊó†Ê≥ïÊâæÂà∞ÂåπÈÖçÈ°πÔºåÂàôËøîÂõû NULL„ÄÇ
 
+Âè¶Â§ñÔºåÊé®Ëçê‰ΩøÁî®jsonbÁ±ªÂûãÂíåjsonb_extract_XXXÂáΩÊï∞ÂÆûÁé∞ÂêåÊ†∑ÁöÑÂäüËÉΩ„ÄÇ
+
 ### example
 
 1. Ëé∑Âèñ key ‰∏∫ "k1" ÁöÑ value
diff --git a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md
index 1eb2d5e17..56bb81875 100644
--- a/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md
+++ b/docs/zh-CN/docs/sql-manual/sql-functions/json-functions/jsonb_extract.md
@@ -25,6 +25,13 @@ under the License.
 -->
 
 ## jsonb_extract
+
+<version since="1.2.0">
+
+jsonb_extract
+
+</version>
+
 ### description
 #### Syntax
 
diff --git a/docs/zh-CN/docs/sql-manual/sql-functions/string-functions/uuid.md b/docs/zh-CN/docs/sql-manual/sql-functions/string-functions/uuid.md
index 0b1e6b166..29339d1d6 100644
--- a/docs/zh-CN/docs/sql-manual/sql-functions/string-functions/uuid.md
+++ b/docs/zh-CN/docs/sql-manual/sql-functions/string-functions/uuid.md
@@ -25,6 +25,13 @@ under the License.
 -->
 
 ## uuid
+
+<version since="1.2.0">
+
+uuid
+
+</version>
+
 ### description
 #### Syntax
 
diff --git a/docs/zh-CN/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md b/docs/zh-CN/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md
index d76a70eff..f860a189e 100644
--- a/docs/zh-CN/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md
+++ b/docs/zh-CN/docs/sql-manual/sql-reference/Data-Definition-Statements/Drop/DROP-POLICY.md
@@ -43,6 +43,11 @@ DROP POLICY
 DROP ROW POLICY test_row_policy_1 on table1 [FOR user];
 ```
 
+2. Âà†Èô§Â≠òÂÇ®Á≠ñÁï•
+```sql
+DROP STORAGE POLICY policy_name1
+```
+
 ### Example
 
 1. Âà†Èô§ table1 ÁöÑ test_row_policy_1
@@ -56,6 +61,12 @@ DROP ROW POLICY test_row_policy_1 on table1 [FOR user];
    ```sql
    DROP ROW POLICY test_row_policy_1 on table1 for test
    ```
+
+3. Âà†Èô§ÂêçÂ≠ó‰∏∫policy_name1ÁöÑÂ≠òÂÇ®Á≠ñÁï•
+```sql
+DROP STORAGE POLICY policy_name1
+```
+
 ### Keywords
 
     DROP, POLICY
diff --git a/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md b/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md
index f0974a161..1d4098646 100644
--- a/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md
+++ b/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD.md
@@ -99,7 +99,7 @@ curl --location-trusted -u user:passwd [-H ""...] -T data.file -XPUT http://fe_h
 
 11. exec_mem_limit: ÂØºÂÖ•ÂÜÖÂ≠òÈôêÂà∂„ÄÇÈªòËÆ§‰∏∫ 2GB„ÄÇÂçï‰Ωç‰∏∫Â≠óËäÇ„ÄÇ
 
-12. format: ÊåáÂÆöÂØºÂÖ•Êï∞ÊçÆÊ†ºÂºèÔºåÈªòËÆ§ÊòØcsvÔºå‰πüÊîØÊåÅÔºö<version since="1.2" type="inline"> csv_with_names(ÊîØÊåÅcsvÊñá‰ª∂Ë°åÈ¶ñËøáÊª§)Ôºåcsv_with_names_and_types(ÊîØÊåÅcsvÊñá‰ª∂Ââç‰∏§Ë°åËøáÊª§) </version> Êàñ jsonÊ†ºÂºè„ÄÇ
+12. format: ÊåáÂÆöÂØºÂÖ•Êï∞ÊçÆÊ†ºÂºèÔºåÊîØÊåÅcsv„ÄÅjson„ÄÅ<version since="1.2" type="inline"> csv_with_names(ÊîØÊåÅcsvÊñá‰ª∂Ë°åÈ¶ñËøáÊª§)„ÄÅcsv_with_names_and_types(ÊîØÊåÅcsvÊñá‰ª∂Ââç‰∏§Ë°åËøáÊª§)„ÄÅparquet„ÄÅorc</version>ÔºåÈªòËÆ§ÊòØcsv„ÄÇ
 
 13. jsonpaths: ÂØºÂÖ•jsonÊñπÂºèÂàÜ‰∏∫ÔºöÁÆÄÂçïÊ®°ÂºèÂíåÂåπÈÖçÊ®°Âºè„ÄÇ
     
diff --git a/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md b/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md
index eed87118d..c9364c137 100644
--- a/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md
+++ b/docs/zh-CN/docs/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/SELECT.md
@@ -69,7 +69,7 @@ SELECT
 
 6. `INTO OUTFILE 'file_name' ` Ôºö‰øùÂ≠òÁªìÊûúËá≥Êñ∞Êñá‰ª∂Ôºà‰πãÂâç‰∏çÂ≠òÂú®Ôºâ‰∏≠ÔºåÂå∫Âà´Âú®‰∫é‰øùÂ≠òÁöÑÊ†ºÂºè„ÄÇ
    
-7. `Group by having`ÔºöÂØπÁªìÊûúÈõÜËøõË°åÂàÜÁªÑÔºåhaving Âá∫Áé∞ÂàôÂØπ group by ÁöÑÁªìÊûúËøõË°åÂà∑ÈÄâ„ÄÇ`Grouping Sets`„ÄÅ`Rollup`„ÄÅ`Cube` ‰∏∫group byÁöÑÊâ©Â±ïÔºåËØ¶ÁªÜÂèØÂèÇËÄÉ[GROUPING SETS ËÆæËÆ°ÊñáÊ°£](../../../../../community/design/grouping_sets_design.md)„ÄÇ
+7. `Group by having`ÔºöÂØπÁªìÊûúÈõÜËøõË°åÂàÜÁªÑÔºåhaving Âá∫Áé∞ÂàôÂØπ group by ÁöÑÁªìÊûúËøõË°åÂà∑ÈÄâ„ÄÇ`Grouping Sets`„ÄÅ`Rollup`„ÄÅ`Cube` ‰∏∫group byÁöÑÊâ©Â±ïÔºåËØ¶ÁªÜÂèØÂèÇËÄÉ[GROUPING SETS ËÆæËÆ°ÊñáÊ°£](https://doris.apache.org/zh-CN/community/design/grouping_sets_design)„ÄÇ
 
 8. `Order by `: ÂØπÊúÄÂêéÁöÑÁªìÊûúËøõË°åÊéíÂ∫èÔºåOrder by ÈÄöËøáÊØîËæÉ‰∏ÄÂàóÊàñËÄÖÂ§öÂàóÁöÑÂ§ßÂ∞èÊù•ÂØπÁªìÊûúÈõÜËøõË°åÊéíÂ∫è„ÄÇ
 
diff --git a/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-ALTER.md b/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-ALTER.md
index 8c665814c..366385d20 100644
--- a/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-ALTER.md
+++ b/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-ALTER.md
@@ -43,9 +43,9 @@ SHOW ALTER [CLUSTER | TABLE [COLUMN | ROLLUP] [FROM db_name]];
 
 1. TABLE COLUMNÔºöÂ±ïÁ§∫‰øÆÊîπÂàóÁöÑ ALTER ‰ªªÂä°
 2. ÊîØÊåÅËØ≠Ê≥ï[WHERE TableName|CreateTime|FinishTime|State] [ORDER BY] [LIMIT]
-3.  TABLE ROLLUPÔºöÂ±ïÁ§∫ÂàõÂª∫ÊàñÂà†Èô§ ROLLUP index ÁöÑ‰ªªÂä°
-4.  Â¶ÇÊûú‰∏çÊåáÂÆö db_nameÔºå‰ΩøÁî®ÂΩìÂâçÈªòËÆ§ db
-5.  CLUSTER: Â±ïÁ§∫ÈõÜÁæ§Êìç‰ΩúÁõ∏ÂÖ≥‰ªªÂä°ÊÉÖÂÜµÔºà‰ªÖÁÆ°ÁêÜÂëò‰ΩøÁî®ÔºÅÂæÖÂÆûÁé∞...Ôºâ
+3. TABLE ROLLUPÔºöÂ±ïÁ§∫ÂàõÂª∫ÊàñÂà†Èô§ ROLLUP index ÁöÑ‰ªªÂä°
+4. Â¶ÇÊûú‰∏çÊåáÂÆö db_nameÔºå‰ΩøÁî®ÂΩìÂâçÈªòËÆ§ db
+5. CLUSTER: Â±ïÁ§∫ÈõÜÁæ§Êìç‰ΩúÁõ∏ÂÖ≥‰ªªÂä°ÊÉÖÂÜµÔºà‰ªÖÁÆ°ÁêÜÂëò‰ΩøÁî®ÔºÅÂæÖÂÆûÁé∞...Ôºâ
 
 ### Example
 
diff --git a/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-LOAD-WARNINGS.md b/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-LOAD-WARNINGS.md
index 19787a14f..4c3e314ee 100644
--- a/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-LOAD-WARNINGS.md
+++ b/docs/zh-CN/docs/sql-manual/sql-reference/Show-Statements/SHOW-LOAD-WARNINGS.md
@@ -32,7 +32,7 @@ SHOW LOAD WARNINGS
 
 ### Description
 
-Â¶ÇÊûúÂØºÂÖ•‰ªªÂä°Â§±Ë¥•‰∏îÈîôËØØ‰ø°ÊÅØ‰∏∫ `ETL_QUALITY_UNSATISFIED`ÔºåÂàôËØ¥ÊòéÂ≠òÂú®ÂØºÂÖ•Ë¥®ÈáèÈóÆÈ¢ò, Â¶ÇÊûúÊÉ≥ÁúãÂà∞Ëøô‰∫õÊúâË¥®ÈáèÈóÆÈ¢òÁöÑÂØºÂÖ•‰ªªÂä°ÔºåÊîπËØ≠Âè•Â∞±ÊòØÂÆåÊàêËøô‰∏™Êìç‰ΩúÁöÑ„ÄÇ
+Â¶ÇÊûúÂØºÂÖ•‰ªªÂä°Â§±Ë¥•‰∏îÈîôËØØ‰ø°ÊÅØ‰∏∫ `ETL_QUALITY_UNSATISFIED`ÔºåÂàôËØ¥ÊòéÂ≠òÂú®ÂØºÂÖ•Ë¥®ÈáèÈóÆÈ¢ò, Â¶ÇÊûúÊÉ≥ÁúãÂà∞Ëøô‰∫õÊúâË¥®ÈáèÈóÆÈ¢òÁöÑÂØºÂÖ•‰ªªÂä°ÔºåËØ•ËØ≠Âè•Â∞±ÊòØÂÆåÊàêËøô‰∏™Êìç‰ΩúÁöÑ„ÄÇ
 
 ËØ≠Ê≥ïÔºö
 
@@ -46,9 +46,9 @@ SHOW LOAD WARNINGS
 ]
 ```
 
-1) Â¶ÇÊûú‰∏çÊåáÂÆö db_nameÔºå‰ΩøÁî®ÂΩìÂâçÈªòËÆ§db
-2) Â¶ÇÊûú‰ΩøÁî® LABEL = ÔºåÂàôÁ≤æÁ°ÆÂåπÈÖçÊåáÂÆöÁöÑ label
-3) Â¶ÇÊûúÊåáÂÆö‰∫Ü LOAD_JOB_IDÔºåÂàôÁ≤æÁ°ÆÂåπÈÖçÊåáÂÆöÁöÑ JOB ID
+1. Â¶ÇÊûú‰∏çÊåáÂÆö db_nameÔºå‰ΩøÁî®ÂΩìÂâçÈªòËÆ§db
+2. Â¶ÇÊûú‰ΩøÁî® LABEL = ÔºåÂàôÁ≤æÁ°ÆÂåπÈÖçÊåáÂÆöÁöÑ label
+3. Â¶ÇÊûúÊåáÂÆö‰∫Ü LOAD_JOB_IDÔºåÂàôÁ≤æÁ°ÆÂåπÈÖçÊåáÂÆöÁöÑ JOB ID
 
 ### Example
 
diff --git a/fe/fe-core/src/main/antlr4/org/apache/doris/nereids/DorisParser.g4 b/fe/fe-core/src/main/antlr4/org/apache/doris/nereids/DorisParser.g4
index 8dbd7dd23..e4b28a006 100644
--- a/fe/fe-core/src/main/antlr4/org/apache/doris/nereids/DorisParser.g4
+++ b/fe/fe-core/src/main/antlr4/org/apache/doris/nereids/DorisParser.g4
@@ -226,6 +226,7 @@ multipartIdentifier
 // -----------------Expression-----------------
 namedExpression
     : expression (AS? name=errorCapturingIdentifier)?
+    | expression (AS? strName=STRING+)?
     ;
 
 namedExpressionSeq
diff --git a/fe/fe-core/src/main/cup/sql_parser.cup b/fe/fe-core/src/main/cup/sql_parser.cup
index ac678a289..b6bba6a5d 100644
--- a/fe/fe-core/src/main/cup/sql_parser.cup
+++ b/fe/fe-core/src/main/cup/sql_parser.cup
@@ -267,6 +267,7 @@ terminal String
     KW_BINLOG,
     KW_BITMAP,
     KW_BITMAP_UNION,
+    KW_NGRAM_BF,
     KW_BLOB,
     KW_BOOLEAN,
     KW_BROKER,
@@ -2143,7 +2144,7 @@ opt_password_lock_time ::=
     | KW_PASSWORD_LOCK_TIME passwd_lock_time_opt:opt
     {:
         RESULT = opt;
-    :} 
+    :}
     ;
 
 passwd_lock_time_opt ::=
@@ -3270,6 +3271,10 @@ opt_index_type ::=
     {:
         RESULT = IndexDef.IndexType.BITMAP;
     :}
+    | KW_USING KW_NGRAM_BF
+    {:
+        RESULT = IndexDef.IndexType.NGRAM_BF;
+    :}
     | KW_USING KW_INVERTED
     {:
         RESULT = IndexDef.IndexType.INVERTED;
@@ -5662,10 +5667,10 @@ func_args_def ::=
 
 cast_expr ::=
   KW_CAST LPAREN expr:e KW_AS type_def:targetType RPAREN
-  {: 
-    CastExpr castExpr = new CastExpr(targetType, e); 
+  {:
+    CastExpr castExpr = new CastExpr(targetType, e);
     if (targetType.getType().getLength() != -1
-        && (targetType.getType().getPrimitiveType() == PrimitiveType.VARCHAR 
+        && (targetType.getType().getPrimitiveType() == PrimitiveType.VARCHAR
         || targetType.getType().getPrimitiveType() == PrimitiveType.CHAR)) {
         // transfer cast(xx as char(N)/varchar(N)) to substr(cast(xx as char), 1, N)
         // this is just a workaround to make the result correct
@@ -6504,6 +6509,8 @@ keyword ::=
     {: RESULT = id; :}
     | KW_BITMAP_UNION:id
     {: RESULT = id; :}
+    | KW_NGRAM_BF:id
+    {: RESULT = id; :}
     | KW_QUANTILE_UNION:id
     {: RESULT = id; :}
     | KW_BLOB:id
diff --git a/fe/fe-core/src/main/java/org/apache/doris/alter/Alter.java b/fe/fe-core/src/main/java/org/apache/doris/alter/Alter.java
index e506a1adc..d3f188c46 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/alter/Alter.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/alter/Alter.java
@@ -52,9 +52,11 @@ import org.apache.doris.catalog.OlapTable;
 import org.apache.doris.catalog.OlapTable.OlapTableState;
 import org.apache.doris.catalog.Partition;
 import org.apache.doris.catalog.PartitionInfo;
+import org.apache.doris.catalog.Replica;
 import org.apache.doris.catalog.ReplicaAllocation;
 import org.apache.doris.catalog.Table;
 import org.apache.doris.catalog.TableIf.TableType;
+import org.apache.doris.catalog.Tablet;
 import org.apache.doris.catalog.View;
 import org.apache.doris.common.AnalysisException;
 import org.apache.doris.common.DdlException;
@@ -187,8 +189,17 @@ public class Alter {
         if (currentAlterOps.checkTableStoragePolicy(alterClauses)) {
             String tableStoragePolicy = olapTable.getStoragePolicy();
             if (!tableStoragePolicy.equals("")) {
-                throw new DdlException("Do not support alter table's storage policy , this table ["
-                        + olapTable.getName() + "] has storage policy " + tableStoragePolicy);
+                for (Partition partition : olapTable.getAllPartitions()) {
+                    for (Tablet tablet : partition.getBaseIndex().getTablets()) {
+                        for (Replica replica : tablet.getReplicas()) {
+                            if (replica.getRowCount() > 0 || replica.getDataSize() > 0) {
+                                throw new DdlException("Do not support alter table's storage policy , this table ["
+                                        + olapTable.getName() + "] has storage policy " + tableStoragePolicy
+                                        + ", the table need to be empty.");
+                            }
+                        }
+                    }
+                }
             }
             String currentStoragePolicy = currentAlterOps.getTableStoragePolicy(alterClauses);
             // check currentStoragePolicy resource exist.
diff --git a/fe/fe-core/src/main/java/org/apache/doris/alter/SchemaChangeHandler.java b/fe/fe-core/src/main/java/org/apache/doris/alter/SchemaChangeHandler.java
index 1a0489ea5..de171e25c 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/alter/SchemaChangeHandler.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/alter/SchemaChangeHandler.java
@@ -1247,6 +1247,8 @@ public class SchemaChangeHandler extends AlterHandler {
             bfFpp = 0;
         }
 
+        Index.checkConflict(newSet, bfColumns);
+
         // property 3: timeout
         long timeoutSecond = PropertyAnalyzer.analyzeTimeout(propertyMap, Config.alter_table_timeout_second);
 
@@ -2058,9 +2060,13 @@ public class SchemaChangeHandler extends AlterHandler {
             }
             Set<String> existedIdxColSet = Sets.newTreeSet(String.CASE_INSENSITIVE_ORDER);
             existedIdxColSet.addAll(existedIdx.getColumns());
-            if (newColset.equals(existedIdxColSet)) {
+            if (existedIdx.getIndexType() == indexDef.getIndexType() && newColset.equals(existedIdxColSet)) {
                 throw new DdlException(
-                        "index for columns (" + String.join(",", indexDef.getColumns()) + " ) already exist.");
+                    indexDef.getIndexType()
+                    + " index for columns ("
+                    + String.join(",", indexDef.getColumns())
+                    + " ) already exist."
+                );
             }
         }
 
@@ -2069,7 +2075,7 @@ public class SchemaChangeHandler extends AlterHandler {
             if (column != null) {
                 indexDef.checkColumn(column, olapTable.getKeysType());
             } else {
-                throw new DdlException("BITMAP column does not exist in table. invalid column: " + col);
+                throw new DdlException("index column does not exist in table. invalid column: " + col);
             }
         }
 
diff --git a/fe/fe-core/src/main/java/org/apache/doris/analysis/DropPolicyStmt.java b/fe/fe-core/src/main/java/org/apache/doris/analysis/DropPolicyStmt.java
index 2c4199d8c..50ff6185e 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/analysis/DropPolicyStmt.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/analysis/DropPolicyStmt.java
@@ -18,7 +18,6 @@
 package org.apache.doris.analysis;
 
 import org.apache.doris.catalog.Env;
-import org.apache.doris.common.DdlException;
 import org.apache.doris.common.ErrorCode;
 import org.apache.doris.common.ErrorReport;
 import org.apache.doris.common.UserException;
@@ -57,8 +56,7 @@ public class DropPolicyStmt extends DdlStmt {
         super.analyze(analyzer);
         switch (type) {
             case STORAGE:
-                // current not support drop storage policy, because be use it policy name to find s3 resource.
-                throw new DdlException("current not support drop storage policy.");
+                break;
             case ROW:
             default:
                 tableName.analyze(analyzer);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/analysis/IndexDef.java b/fe/fe-core/src/main/java/org/apache/doris/analysis/IndexDef.java
index 4df18a987..ed03dbd84 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/analysis/IndexDef.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/analysis/IndexDef.java
@@ -37,6 +37,11 @@ public class IndexDef {
     private String comment;
     private Map<String, String> properties;
 
+    public static final String NGRAM_SIZE_KEY = "gram_size";
+    public static final String NGRAM_BF_SIZE_KEY = "bf_size";
+    public static final String DEFAULT_NGRAM_SIZE = "2";
+    public static final String DEFAULT_NGRAM_BF_SIZE = "256";
+
     public IndexDef(String indexName, boolean ifNotExists, List<String> columns, IndexType indexType,
                     Map<String, String> properties, String comment) {
         this.indexName = indexName;
@@ -57,6 +62,10 @@ public class IndexDef {
         } else {
             this.properties = properties;
         }
+        if (indexType == IndexType.NGRAM_BF) {
+            properties.putIfAbsent(NGRAM_SIZE_KEY, DEFAULT_NGRAM_SIZE);
+            properties.putIfAbsent(NGRAM_BF_SIZE_KEY, DEFAULT_NGRAM_BF_SIZE);
+        }
     }
 
     public void analyze() throws AnalysisException {
@@ -155,6 +164,7 @@ public class IndexDef {
         BITMAP,
         INVERTED,
         BLOOMFILTER,
+        NGRAM_BF
     }
 
     public boolean isInvertedIndex() {
@@ -162,7 +172,8 @@ public class IndexDef {
     }
 
     public void checkColumn(Column column, KeysType keysType) throws AnalysisException {
-        if (indexType == IndexType.BITMAP || indexType == IndexType.INVERTED || indexType == IndexType.BLOOMFILTER) {
+        if (indexType == IndexType.BITMAP || indexType == IndexType.INVERTED || indexType == IndexType.BLOOMFILTER
+                || indexType == IndexType.NGRAM_BF) {
             String indexColName = column.getName();
             PrimitiveType colType = column.getDataType();
             if (!(colType.isDateType() || colType.isDecimalV2Type() || colType.isDecimalV3Type()
@@ -177,6 +188,31 @@ public class IndexDef {
 
             if (indexType == IndexType.INVERTED) {
                 InvertedIndexUtil.checkInvertedIndexParser(indexColName, colType, properties);
+            } else if (indexType == IndexType.NGRAM_BF) {
+                if (colType != PrimitiveType.CHAR && colType != PrimitiveType.VARCHAR
+                        && colType != PrimitiveType.STRING) {
+                    throw new AnalysisException(colType + " is not supported in ngram_bf index. "
+                                                    + "invalid column: " + indexColName);
+                } else if ((keysType == KeysType.AGG_KEYS && !column.isKey())) {
+                    throw new AnalysisException(
+                        "ngram_bf index only used in columns of DUP_KEYS/UNIQUE_KEYS table or key columns of"
+                        + " AGG_KEYS table. invalid column: " + indexColName);
+                }
+                if (properties.size() != 2) {
+                    throw new AnalysisException("ngram_bf index should have gram_size and bf_size properties");
+                }
+                try {
+                    int ngramSize = Integer.parseInt(properties.get(NGRAM_SIZE_KEY));
+                    int bfSize = Integer.parseInt(properties.get(NGRAM_BF_SIZE_KEY));
+                    if (ngramSize > 256 || ngramSize < 1) {
+                        throw new AnalysisException("gram_size should be integer and less than 256");
+                    }
+                    if (bfSize > 65536 || bfSize < 64) {
+                        throw new AnalysisException("bf_size should be integer and between 64 and 65536");
+                    }
+                } catch (NumberFormatException e) {
+                    throw new AnalysisException("invalid ngram properties:" + e.getMessage(), e);
+                }
             }
         } else {
             throw new AnalysisException("Unsupported index type: " + indexType);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/AggregateFunction.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/AggregateFunction.java
index 0729b0aa0..07996451d 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/AggregateFunction.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/AggregateFunction.java
@@ -559,23 +559,32 @@ public class AggregateFunction extends Function {
             sb.append(" INTERMEDIATE " + getIntermediateType());
         }
 
-        sb.append(" PROPERTIES (")
-                .append("\n  \"INIT_FN\"=\"" + getInitFnSymbol() + "\"")
-                .append(",\n  \"UPDATE_FN\"=\"" + getUpdateFnSymbol() + "\"")
-                .append(",\n  \"MERGE_FN\"=\"" + getMergeFnSymbol() + "\"");
-        if (getSerializeFnSymbol() != null) {
-            sb.append(",\n  \"SERIALIZE_FN\"=\"" + getSerializeFnSymbol() + "\"");
-        }
-        if (getFinalizeFnSymbol() != null) {
-            sb.append(",\n  \"FINALIZE_FN\"=\"" + getFinalizeFnSymbol() + "\"");
+        sb.append(" PROPERTIES (");
+        if (getBinaryType() != TFunctionBinaryType.JAVA_UDF) {
+            sb.append("\n  \"INIT_FN\"=\"" + getInitFnSymbol() + "\",")
+                    .append("\n  \"UPDATE_FN\"=\"" + getUpdateFnSymbol() + "\",")
+                    .append("\n  \"MERGE_FN\"=\"" + getMergeFnSymbol() + "\",");
+            if (getSerializeFnSymbol() != null) {
+                sb.append("\n  \"SERIALIZE_FN\"=\"" + getSerializeFnSymbol() + "\",");
+            }
+            if (getFinalizeFnSymbol() != null) {
+                sb.append("\n  \"FINALIZE_FN\"=\"" + getFinalizeFnSymbol() + "\",");
+            }
         }
         if (getSymbolName() != null) {
-            sb.append(",\n  \"SYMBOL\"=\"" + getSymbolName() + "\"");
+            sb.append("\n  \"SYMBOL\"=\"" + getSymbolName() + "\",");
         }
 
-        sb.append(",\n  \"OBJECT_FILE\"=")
-                .append("\"" + (getLocation() == null ? "" : getLocation().toString()) + "\"");
-        sb.append(",\n  \"MD5\"=").append("\"" + getChecksum() + "\"");
+        if (getBinaryType() == TFunctionBinaryType.JAVA_UDF) {
+            sb.append("\n  \"FILE\"=")
+                    .append("\"" + (getLocation() == null ? "" : getLocation().toString()) + "\",");
+            boolean isReturnNull = this.getNullableMode() == NullableMode.ALWAYS_NULLABLE;
+            sb.append("\n  \"ALWAYS_NULLABLE\"=").append("\"" + isReturnNull + "\",");
+        } else {
+            sb.append("\n  \"OBJECT_FILE\"=")
+                    .append("\"" + (getLocation() == null ? "" : getLocation().toString()) + "\",");
+        }
+        sb.append("\n  \"TYPE\"=").append("\"" + this.getBinaryType() + "\"");
         sb.append("\n);");
         return sb.toString();
     }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/Index.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/Index.java
index 2da8ce35f..b7058d2d2 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/Index.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/Index.java
@@ -19,6 +19,7 @@ package org.apache.doris.catalog;
 
 import org.apache.doris.analysis.IndexDef;
 import org.apache.doris.analysis.InvertedIndexUtil;
+import org.apache.doris.common.AnalysisException;
 import org.apache.doris.common.io.Text;
 import org.apache.doris.common.io.Writable;
 import org.apache.doris.common.util.PrintableMap;
@@ -32,9 +33,13 @@ import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 /**
  * Internal representation of index, including index type, name, columns and comments.
@@ -197,4 +202,31 @@ public class Index implements Writable {
         }
         return tIndex;
     }
+
+    public static void checkConflict(Collection<Index> indices, Set<String> bloomFilters) throws AnalysisException {
+        indices = indices == null ? Collections.emptyList() : indices;
+        bloomFilters = bloomFilters == null ? Collections.emptySet() : bloomFilters;
+        Set<String> bfColumns = new HashSet<>();
+        for (Index index : indices) {
+            if (IndexDef.IndexType.NGRAM_BF == index.getIndexType()
+                    || IndexDef.IndexType.BLOOMFILTER == index.getIndexType()) {
+                for (String column : index.getColumns()) {
+                    column = column.toLowerCase();
+                    if (bfColumns.contains(column)) {
+                        throw new AnalysisException(column + " should have only one ngram bloom filter index or bloom "
+                            + "filter index");
+                    }
+                    bfColumns.add(column);
+                }
+            }
+        }
+        for (String column : bloomFilters) {
+            column = column.toLowerCase();
+            if (bfColumns.contains(column)) {
+                throw new AnalysisException(column + " should have only one ngram bloom filter index or bloom "
+                    + "filter index");
+            }
+            bfColumns.add(column);
+        }
+    }
 }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarFunction.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarFunction.java
index 148faa8ec..c8e89cae1 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarFunction.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarFunction.java
@@ -389,9 +389,17 @@ public class ScalarFunction extends Function {
         if (getCloseFnSymbol() != null) {
             sb.append(",\n  \"CLOSE_FN\"=").append("\"" + getCloseFnSymbol() + "\"");
         }
-        sb.append(",\n  \"OBJECT_FILE\"=")
-                .append("\"" + (getLocation() == null ? "" : getLocation().toString()) + "\"");
-        sb.append(",\n  \"MD5\"=").append("\"" + getChecksum() + "\"");
+
+        if (getBinaryType() == TFunctionBinaryType.JAVA_UDF) {
+            sb.append(",\n  \"FILE\"=")
+                    .append("\"" + (getLocation() == null ? "" : getLocation().toString()) + "\"");
+            boolean isReturnNull = this.getNullableMode() == NullableMode.ALWAYS_NULLABLE;
+            sb.append(",\n  \"ALWAYS_NULLABLE\"=").append("\"" + isReturnNull + "\"");
+        } else {
+            sb.append(",\n  \"OBJECT_FILE\"=")
+                    .append("\"" + (getLocation() == null ? "" : getLocation().toString()) + "\"");
+        }
+        sb.append(",\n  \"TYPE\"=").append("\"" + this.getBinaryType() + "\"");
         sb.append("\n);");
         return sb.toString();
     }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarType.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarType.java
index 4564af509..3ce7c73c5 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarType.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/ScalarType.java
@@ -582,7 +582,7 @@ public class ScalarType extends Type {
                 }
                 break;
             case DATETIMEV2:
-                stringBuilder.append("datetime").append("(").append(scale).append(")");
+                stringBuilder.append("datetimev2").append("(").append(scale).append(")");
                 break;
             case TIME:
                 stringBuilder.append("time");
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/TableIf.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/TableIf.java
index 2385863a4..df013b7cc 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/TableIf.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/TableIf.java
@@ -127,6 +127,8 @@ public interface TableIf {
 
     BaseAnalysisTask createAnalysisTask(AnalysisTaskScheduler scheduler, AnalysisTaskInfo info);
 
+    long estimatedRowCount();
+
     /**
      * Doris table type.
      */
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/external/ExternalTable.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/external/ExternalTable.java
index 8f111cf06..8d30eda46 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/external/ExternalTable.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/external/ExternalTable.java
@@ -305,6 +305,11 @@ public class ExternalTable implements TableIf, Writable, GsonPostProcessable {
         throw new NotImplementedException();
     }
 
+    @Override
+    public long estimatedRowCount() {
+        return 1;
+    }
+
     @Override
     public void write(DataOutput out) throws IOException {
         String json = GsonUtils.GSON.toJson(this);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/external/HMSExternalTable.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/external/HMSExternalTable.java
index b5db5df42..29cff71cf 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/external/HMSExternalTable.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/external/HMSExternalTable.java
@@ -194,6 +194,7 @@ public class HMSExternalTable extends ExternalTable {
 
     @Override
     public boolean isView() {
+        makeSureInitialized();
         return remoteTable.isSetViewOriginalText() || remoteTable.isSetViewExpandedText();
     }
 
diff --git a/fe/fe-core/src/main/java/org/apache/doris/common/util/PrintableMap.java b/fe/fe-core/src/main/java/org/apache/doris/common/util/PrintableMap.java
index 571fa9297..ae2b50bcc 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/common/util/PrintableMap.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/common/util/PrintableMap.java
@@ -32,6 +32,7 @@ public class PrintableMap<K, V> {
     private String entryDelimiter = ",";
 
     public static final Set<String> SENSITIVE_KEY;
+    public static final String PASSWORD_MASK = "*XXX";
 
     static {
         SENSITIVE_KEY = Sets.newTreeSet(String.CASE_INSENSITIVE_ORDER);
@@ -39,6 +40,7 @@ public class PrintableMap<K, V> {
         SENSITIVE_KEY.add("kerberos_keytab_content");
         SENSITIVE_KEY.add("bos_secret_accesskey");
         SENSITIVE_KEY.add("jdbc.password");
+        SENSITIVE_KEY.add("elasticsearch.password");
     }
 
     public PrintableMap(Map<K, V> map, String keyValueSeparator,
@@ -80,7 +82,7 @@ public class PrintableMap<K, V> {
                 sb.append("\"");
             }
             if (hidePassword && SENSITIVE_KEY.contains(entry.getKey())) {
-                sb.append("*XXX");
+                sb.append(PASSWORD_MASK);
             } else {
                 sb.append(entry.getValue());
             }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/common/util/ProfileManager.java b/fe/fe-core/src/main/java/org/apache/doris/common/util/ProfileManager.java
index 3922c496c..6429b0854 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/common/util/ProfileManager.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/common/util/ProfileManager.java
@@ -74,6 +74,8 @@ public class ProfileManager {
 
     public static final String INSTANCES_NUM_PER_BE = "Instances Num Per BE";
 
+    public static final String PARALLEL_FRAGMENT_EXEC_INSTANCE = "Parallel Fragment Exec Instance Num";
+
     public static final String TRACE_ID = "Trace ID";
 
     public enum ProfileType {
diff --git a/fe/fe-core/src/main/java/org/apache/doris/datasource/CatalogMgr.java b/fe/fe-core/src/main/java/org/apache/doris/datasource/CatalogMgr.java
index 255b63526..a62562acd 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/datasource/CatalogMgr.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/datasource/CatalogMgr.java
@@ -396,7 +396,11 @@ public class CatalogMgr implements Writable, GsonPostProcessable {
                     rows.add(Arrays.asList("resource", catalog.getResource()));
                 }
                 for (Map.Entry<String, String> elem : catalog.getProperties().entrySet()) {
-                    rows.add(Arrays.asList(elem.getKey(), elem.getValue()));
+                    if (PrintableMap.SENSITIVE_KEY.contains(elem.getKey())) {
+                        rows.add(Arrays.asList(elem.getKey(), PrintableMap.PASSWORD_MASK));
+                    } else {
+                        rows.add(Arrays.asList(elem.getKey(), elem.getValue()));
+                    }
                 }
             }
         } finally {
diff --git a/fe/fe-core/src/main/java/org/apache/doris/datasource/InternalCatalog.java b/fe/fe-core/src/main/java/org/apache/doris/datasource/InternalCatalog.java
index bed44c699..e68b82ec5 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/datasource/InternalCatalog.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/datasource/InternalCatalog.java
@@ -1918,6 +1918,8 @@ public class InternalCatalog implements CatalogIf<Database> {
             throw new DdlException(e.getMessage());
         }
 
+        Index.checkConflict(stmt.getIndexes(), bfColumns);
+
         olapTable.setReplicationAllocation(replicaAlloc);
 
         // set in memory
diff --git a/fe/fe-core/src/main/java/org/apache/doris/metric/PrometheusMetricVisitor.java b/fe/fe-core/src/main/java/org/apache/doris/metric/PrometheusMetricVisitor.java
index 48b713202..60787f3f2 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/metric/PrometheusMetricVisitor.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/metric/PrometheusMetricVisitor.java
@@ -190,20 +190,20 @@ public class PrometheusMetricVisitor extends MetricVisitor {
             }
         }
         final String fullName = prefix + String.join("_", names);
-        final String fullTag = String.join(" ", tags);
+        final String fullTag = String.join(",", tags);
         sb.append(HELP).append(fullName).append(" ").append("\n");
         sb.append(TYPE).append(fullName).append(" ").append("summary\n");
-
+        String delimiter = tags.isEmpty() ? "" : ",";
         Snapshot snapshot = histogram.getSnapshot();
-        sb.append(fullName).append("{quantile=\"0.75\" ").append(fullTag).append("} ")
+        sb.append(fullName).append("{quantile=\"0.75\"").append(delimiter).append(fullTag).append("} ")
             .append(snapshot.get75thPercentile()).append("\n");
-        sb.append(fullName).append("{quantile=\"0.95\" ").append(fullTag).append("} ")
+        sb.append(fullName).append("{quantile=\"0.95\"").append(delimiter).append(fullTag).append("} ")
             .append(snapshot.get95thPercentile()).append("\n");
-        sb.append(fullName).append("{quantile=\"0.98\" ").append(fullTag).append("} ")
+        sb.append(fullName).append("{quantile=\"0.98\"").append(delimiter).append(fullTag).append("} ")
             .append(snapshot.get98thPercentile()).append("\n");
-        sb.append(fullName).append("{quantile=\"0.99\" ").append(fullTag).append("} ")
+        sb.append(fullName).append("{quantile=\"0.99\"").append(delimiter).append(fullTag).append("} ")
             .append(snapshot.get99thPercentile()).append("\n");
-        sb.append(fullName).append("{quantile=\"0.999\" ").append(fullTag).append("} ")
+        sb.append(fullName).append("{quantile=\"0.999\"").append(delimiter).append(fullTag).append("} ")
             .append(snapshot.get999thPercentile()).append("\n");
         sb.append(fullName).append("_sum {").append(fullTag).append("} ")
             .append(histogram.getCount() * snapshot.getMean()).append("\n");
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/CascadesContext.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/CascadesContext.java
index 26243a9b5..1a9af3126 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/CascadesContext.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/CascadesContext.java
@@ -215,22 +215,6 @@ public class CascadesContext {
         return this;
     }
 
-    public void addToTable(Table table) {
-        tables.add(table);
-    }
-
-    public void lockTableOnRead() {
-        for (Table t : tables) {
-            t.readLock();
-        }
-    }
-
-    public void releaseTableReadLock() {
-        for (Table t : tables) {
-            t.readUnlock();
-        }
-    }
-
     /**
      * Extract tables.
      */
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/cost/CostCalculator.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/cost/CostCalculator.java
index b37b3ca2c..51fb7076f 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/cost/CostCalculator.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/cost/CostCalculator.java
@@ -27,6 +27,7 @@ import org.apache.doris.nereids.properties.DistributionSpecReplicated;
 import org.apache.doris.nereids.trees.plans.Plan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalAssertNumRows;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalDistribute;
+import org.apache.doris.nereids.trees.plans.physical.PhysicalFileScan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalGenerate;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalHashAggregate;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalHashJoin;
@@ -115,6 +116,12 @@ public class CostCalculator {
                     costEstimate.getNetworkCost(), costEstimate.getPenalty());
         }
 
+        @Override
+        public CostEstimate visitPhysicalFileScan(PhysicalFileScan physicalFileScan, PlanContext context) {
+            StatsDeriveResult statistics = context.getStatisticsWithCheck();
+            return CostEstimate.ofCpu(statistics.getRowCount());
+        }
+
         @Override
         public CostEstimate visitPhysicalProject(PhysicalProject<? extends Plan> physicalProject, PlanContext context) {
             return CostEstimate.ofCpu(1);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/glue/translator/PhysicalPlanTranslator.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/glue/translator/PhysicalPlanTranslator.java
index 7adedfcc1..c49013d19 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/glue/translator/PhysicalPlanTranslator.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/glue/translator/PhysicalPlanTranslator.java
@@ -34,6 +34,8 @@ import org.apache.doris.analysis.TupleDescriptor;
 import org.apache.doris.analysis.TupleId;
 import org.apache.doris.catalog.OlapTable;
 import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.TableIf;
+import org.apache.doris.catalog.external.ExternalTable;
 import org.apache.doris.common.Pair;
 import org.apache.doris.nereids.exceptions.AnalysisException;
 import org.apache.doris.nereids.properties.DistributionSpecAny;
@@ -65,6 +67,7 @@ import org.apache.doris.nereids.trees.plans.physical.PhysicalAssertNumRows;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalDistribute;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalEmptyRelation;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalExcept;
+import org.apache.doris.nereids.trees.plans.physical.PhysicalFileScan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalFilter;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalGenerate;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalHashAggregate;
@@ -112,6 +115,7 @@ import org.apache.doris.planner.SetOperationNode;
 import org.apache.doris.planner.SortNode;
 import org.apache.doris.planner.TableFunctionNode;
 import org.apache.doris.planner.UnionNode;
+import org.apache.doris.planner.external.ExternalFileScanNode;
 import org.apache.doris.tablefunction.TableValuedFunctionIf;
 import org.apache.doris.thrift.TPartitionType;
 import org.apache.doris.thrift.TPushAggOp;
@@ -264,6 +268,7 @@ public class PhysicalPlanTranslator extends DefaultPlanVisitor<PlanFragment, Pla
                     .map(slot -> slot.getId().asInt())
                     .collect(ImmutableList.toImmutableList());
         }
+
         boolean isPartial = aggregate.getAggregateParam().aggMode.productAggregateBuffer;
         AggregateInfo aggInfo = AggregateInfo.create(execGroupingExpressions, execAggregateFunctions,
                 aggFunOutputIds, isPartial, outputTupleDesc, outputTupleDesc, aggregate.getAggPhase().toExec());
@@ -514,6 +519,28 @@ public class PhysicalPlanTranslator extends DefaultPlanVisitor<PlanFragment, Pla
         return planFragment;
     }
 
+    @Override
+    public PlanFragment visitPhysicalFileScan(PhysicalFileScan fileScan, PlanTranslatorContext context) {
+        List<Slot> slotList = fileScan.getOutput();
+        ExternalTable table = fileScan.getTable();
+        TupleDescriptor tupleDescriptor = generateTupleDesc(slotList, table, context);
+        tupleDescriptor.setTable(table);
+        ExternalFileScanNode fileScanNode = new ExternalFileScanNode(context.nextPlanNodeId(), tupleDescriptor);
+        TableName tableName = new TableName(null, "", "");
+        TableRef ref = new TableRef(tableName, null, null);
+        BaseTableRef tableRef = new BaseTableRef(ref, table, tableName);
+        tupleDescriptor.setRef(tableRef);
+
+        Utils.execWithUncheckedException(fileScanNode::init);
+        context.addScanNode(fileScanNode);
+        Utils.execWithUncheckedException(fileScanNode::finalizeForNerieds);
+        // Create PlanFragment
+        DataPartition dataPartition = DataPartition.RANDOM;
+        PlanFragment planFragment = new PlanFragment(context.nextFragmentId(), fileScanNode, dataPartition);
+        context.addPlanFragment(planFragment);
+        return planFragment;
+    }
+
     @Override
     public PlanFragment visitPhysicalTVFRelation(PhysicalTVFRelation tvfRelation, PlanTranslatorContext context) {
         List<Slot> slots = tvfRelation.getLogicalProperties().getOutput();
@@ -1406,7 +1433,7 @@ public class PhysicalPlanTranslator extends DefaultPlanVisitor<PlanFragment, Pla
         }
     }
 
-    private TupleDescriptor generateTupleDesc(List<Slot> slotList, Table table, PlanTranslatorContext context) {
+    private TupleDescriptor generateTupleDesc(List<Slot> slotList, TableIf table, PlanTranslatorContext context) {
         TupleDescriptor tupleDescriptor = context.generateTupleDesc();
         tupleDescriptor.setTable(table);
         for (Slot slot : slotList) {
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/jobs/batch/AnalyzeRulesJob.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/jobs/batch/AnalyzeRulesJob.java
index fc7fd7885..9e7cebac7 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/jobs/batch/AnalyzeRulesJob.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/jobs/batch/AnalyzeRulesJob.java
@@ -31,6 +31,9 @@ import org.apache.doris.nereids.rules.analysis.ReplaceExpressionByChildOutput;
 import org.apache.doris.nereids.rules.analysis.ResolveOrdinalInOrderByAndGroupBy;
 import org.apache.doris.nereids.rules.analysis.Scope;
 import org.apache.doris.nereids.rules.analysis.UserAuthentication;
+import org.apache.doris.nereids.rules.expression.rewrite.ExpressionNormalization;
+import org.apache.doris.nereids.rules.expression.rewrite.rules.CharacterLiteralTypeCoercion;
+import org.apache.doris.nereids.rules.expression.rewrite.rules.TypeCoercion;
 import org.apache.doris.nereids.rules.rewrite.logical.HideOneRowRelationUnderUnion;
 
 import com.google.common.collect.ImmutableList;
@@ -68,7 +71,9 @@ public class AnalyzeRulesJob extends BatchRulesJob {
                     new ProjectWithDistinctToAggregate(),
                     new ResolveOrdinalInOrderByAndGroupBy(),
                     new ReplaceExpressionByChildOutput(),
-                    new HideOneRowRelationUnderUnion()
+                    new HideOneRowRelationUnderUnion(),
+                    new ExpressionNormalization(cascadesContext.getConnectContext(),
+                                ImmutableList.of(CharacterLiteralTypeCoercion.INSTANCE, TypeCoercion.INSTANCE))
                 )),
                 topDownBatch(ImmutableList.of(
                     new FillUpMissingSlots(),
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/parser/LogicalPlanBuilder.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/parser/LogicalPlanBuilder.java
index 269704fe1..551a594ca 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/parser/LogicalPlanBuilder.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/parser/LogicalPlanBuilder.java
@@ -122,6 +122,7 @@ import org.apache.doris.nereids.trees.expressions.GreaterThan;
 import org.apache.doris.nereids.trees.expressions.GreaterThanEqual;
 import org.apache.doris.nereids.trees.expressions.InPredicate;
 import org.apache.doris.nereids.trees.expressions.InSubquery;
+import org.apache.doris.nereids.trees.expressions.IntegralDivide;
 import org.apache.doris.nereids.trees.expressions.IsNull;
 import org.apache.doris.nereids.trees.expressions.LessThan;
 import org.apache.doris.nereids.trees.expressions.LessThanEqual;
@@ -199,6 +200,7 @@ import org.apache.doris.nereids.trees.plans.logical.LogicalSort;
 import org.apache.doris.nereids.trees.plans.logical.LogicalSubQueryAlias;
 import org.apache.doris.nereids.trees.plans.logical.LogicalUnion;
 import org.apache.doris.nereids.trees.plans.logical.RelationUtil;
+import org.apache.doris.nereids.trees.plans.logical.UsingJoin;
 import org.apache.doris.nereids.types.DataType;
 import org.apache.doris.nereids.types.IntegerType;
 import org.apache.doris.nereids.types.TinyIntType;
@@ -450,7 +452,11 @@ public class LogicalPlanBuilder extends DorisParserBaseVisitor<Object> {
 
     @Override
     public LogicalPlan visitAliasedQuery(AliasedQueryContext ctx) {
-        return withTableAlias(visitQuery(ctx.query()), ctx.tableAlias());
+        LogicalPlan plan = withTableAlias(visitQuery(ctx.query()), ctx.tableAlias());
+        for (LateralViewContext lateralViewContext : ctx.lateralView()) {
+            plan = withGenerate(plan, lateralViewContext);
+        }
+        return plan;
     }
 
     @Override
@@ -500,6 +506,9 @@ public class LogicalPlanBuilder extends DorisParserBaseVisitor<Object> {
             Expression expression = getExpression(ctx.expression());
             if (ctx.name != null) {
                 return new UnboundAlias(expression, ctx.name.getText());
+            } else if (ctx.strName != null) {
+                return new UnboundAlias(expression, ctx.strName.getText()
+                        .substring(1, ctx.strName.getText().length() - 1));
             } else {
                 return expression;
             }
@@ -664,7 +673,7 @@ public class LogicalPlanBuilder extends DorisParserBaseVisitor<Object> {
                     case DorisParser.MINUS:
                         return new Subtract(left, right);
                     case DorisParser.DIV:
-                        return new Divide(left, right);
+                        return new IntegralDivide(left, right);
                     case DorisParser.HAT:
                         return new BitXor(left, right);
                     case DorisParser.PIPE:
@@ -1246,20 +1255,27 @@ public class LogicalPlanBuilder extends DorisParserBaseVisitor<Object> {
             } else {
                 joinType = JoinType.CROSS_JOIN;
             }
-
+            JoinHint joinHint = Optional.ofNullable(join.joinHint()).map(hintCtx -> {
+                String hint = typedVisit(join.joinHint());
+                if (JoinHint.JoinHintType.SHUFFLE.toString().equalsIgnoreCase(hint)) {
+                    return JoinHint.SHUFFLE_RIGHT;
+                } else if (JoinHint.JoinHintType.BROADCAST.toString().equalsIgnoreCase(hint)) {
+                    return JoinHint.BROADCAST_RIGHT;
+                } else {
+                    throw new ParseException("Invalid join hint: " + hint, hintCtx);
+                }
+            }).orElse(JoinHint.NONE);
             // TODO: natural join, lateral join, using join, union join
             JoinCriteriaContext joinCriteria = join.joinCriteria();
             Optional<Expression> condition = Optional.empty();
+            List<UnboundSlot> ids = null;
             if (joinCriteria != null) {
                 if (joinCriteria.booleanExpression() != null) {
                     condition = Optional.ofNullable(getExpression(joinCriteria.booleanExpression()));
-                }
-                if (joinCriteria.USING() != null) {
-                    List<UnboundSlot> ids =
-                            visitIdentifierList(joinCriteria.identifierList())
+                } else if (joinCriteria.USING() != null) {
+                    ids = visitIdentifierList(joinCriteria.identifierList())
                                     .stream().map(UnboundSlot::quoted).collect(
                                             Collectors.toList());
-                    return new LogicalJoin(JoinType.USING_JOIN, ids, last, plan(join.relationPrimary()));
                 }
             } else {
                 // keep same with original planner, allow cross/inner join
@@ -1267,24 +1283,17 @@ public class LogicalPlanBuilder extends DorisParserBaseVisitor<Object> {
                     throw new ParseException("on mustn't be empty except for cross/inner join", join);
                 }
             }
-
-            JoinHint joinHint = Optional.ofNullable(join.joinHint()).map(hintCtx -> {
-                String hint = typedVisit(join.joinHint());
-                if (JoinHint.JoinHintType.SHUFFLE.toString().equalsIgnoreCase(hint)) {
-                    return JoinHint.SHUFFLE_RIGHT;
-                } else if (JoinHint.JoinHintType.BROADCAST.toString().equalsIgnoreCase(hint)) {
-                    return JoinHint.BROADCAST_RIGHT;
-                } else {
-                    throw new ParseException("Invalid join hint: " + hint, hintCtx);
-                }
-            }).orElse(JoinHint.NONE);
-
-            last = new LogicalJoin<>(joinType, ExpressionUtils.EMPTY_CONDITION,
-                    condition.map(ExpressionUtils::extractConjunction)
-                            .orElse(ExpressionUtils.EMPTY_CONDITION),
-                    joinHint,
-                    last,
-                    plan(join.relationPrimary()));
+            if (ids == null) {
+                last = new LogicalJoin<>(joinType, ExpressionUtils.EMPTY_CONDITION,
+                        condition.map(ExpressionUtils::extractConjunction)
+                                .orElse(ExpressionUtils.EMPTY_CONDITION),
+                        joinHint,
+                        last,
+                        plan(join.relationPrimary()));
+            } else {
+                last = new UsingJoin(joinType, last,
+                        plan(join.relationPrimary()), Collections.emptyList(), ids, joinHint);
+            }
         }
         return last;
     }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/properties/ChildOutputPropertyDeriver.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/properties/ChildOutputPropertyDeriver.java
index 099c8f5ab..ccdd299b9 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/properties/ChildOutputPropertyDeriver.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/properties/ChildOutputPropertyDeriver.java
@@ -28,6 +28,7 @@ import org.apache.doris.nereids.trees.expressions.functions.table.TableValuedFun
 import org.apache.doris.nereids.trees.plans.Plan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalAssertNumRows;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalDistribute;
+import org.apache.doris.nereids.trees.plans.physical.PhysicalFileScan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalFilter;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalGenerate;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalHashAggregate;
@@ -233,6 +234,11 @@ public class ChildOutputPropertyDeriver extends PlanVisitor<PhysicalProperties,
         }
     }
 
+    @Override
+    public PhysicalProperties visitPhysicalFileScan(PhysicalFileScan fileScan, PlanContext context) {
+        return PhysicalProperties.ANY;
+    }
+
     @Override
     public PhysicalProperties visitPhysicalStorageLayerAggregate(
             PhysicalStorageLayerAggregate storageLayerAggregate, PlanContext context) {
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleSet.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleSet.java
index ba0a4ffda..a4e28f7ba 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleSet.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleSet.java
@@ -29,6 +29,7 @@ import org.apache.doris.nereids.rules.exploration.join.SemiJoinSemiJoinTranspose
 import org.apache.doris.nereids.rules.implementation.LogicalAssertNumRowsToPhysicalAssertNumRows;
 import org.apache.doris.nereids.rules.implementation.LogicalEmptyRelationToPhysicalEmptyRelation;
 import org.apache.doris.nereids.rules.implementation.LogicalExceptToPhysicalExcept;
+import org.apache.doris.nereids.rules.implementation.LogicalFileScanToPhysicalFileScan;
 import org.apache.doris.nereids.rules.implementation.LogicalFilterToPhysicalFilter;
 import org.apache.doris.nereids.rules.implementation.LogicalGenerateToPhysicalGenerate;
 import org.apache.doris.nereids.rules.implementation.LogicalIntersectToPhysicalIntersect;
@@ -105,6 +106,7 @@ public class RuleSet {
             .add(new LogicalJoinToNestedLoopJoin())
             .add(new LogicalOlapScanToPhysicalOlapScan())
             .add(new LogicalSchemaScanToPhysicalSchemaScan())
+            .add(new LogicalFileScanToPhysicalFileScan())
             .add(new LogicalProjectToPhysicalProject())
             .add(new LogicalLimitToPhysicalLimit())
             .add(new LogicalSortToPhysicalQuickSort())
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleType.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleType.java
index 75612ab52..fb04269bf 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleType.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/RuleType.java
@@ -135,6 +135,9 @@ public enum RuleType {
     REWRITE_FILTER_EXPRESSION(RuleTypeClass.REWRITE),
     REWRITE_JOIN_EXPRESSION(RuleTypeClass.REWRITE),
     REWRITE_GENERATE_EXPRESSION(RuleTypeClass.REWRITE),
+    REWRITE_SORT_EXPRESSION(RuleTypeClass.REWRITE),
+    REWRITE_HAVING_EXPRESSSION(RuleTypeClass.REWRITE),
+    REWRITE_REPEAT_EXPRESSSION(RuleTypeClass.REWRITE),
     REORDER_JOIN(RuleTypeClass.REWRITE),
     // Merge Consecutive plan
     MERGE_PROJECTS(RuleTypeClass.REWRITE),
@@ -208,6 +211,7 @@ public enum RuleType {
     LOGICAL_LIMIT_TO_PHYSICAL_LIMIT_RULE(RuleTypeClass.IMPLEMENTATION),
     LOGICAL_OLAP_SCAN_TO_PHYSICAL_OLAP_SCAN_RULE(RuleTypeClass.IMPLEMENTATION),
     LOGICAL_SCHEMA_SCAN_TO_PHYSICAL_SCHEMA_SCAN_RULE(RuleTypeClass.IMPLEMENTATION),
+    LOGICAL_FILE_SCAN_TO_PHYSICAL_FILE_SCAN_RULE(RuleTypeClass.IMPLEMENTATION),
     LOGICAL_ASSERT_NUM_ROWS_TO_PHYSICAL_ASSERT_NUM_ROWS(RuleTypeClass.IMPLEMENTATION),
     STORAGE_LAYER_AGGREGATE_WITHOUT_PROJECT(RuleTypeClass.IMPLEMENTATION),
     STORAGE_LAYER_AGGREGATE_WITH_PROJECT(RuleTypeClass.IMPLEMENTATION),
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindRelation.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindRelation.java
index b740aadae..d706c8043 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindRelation.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindRelation.java
@@ -17,10 +17,15 @@
 
 package org.apache.doris.nereids.rules.analysis;
 
+import org.apache.doris.catalog.DatabaseIf;
+import org.apache.doris.catalog.Env;
 import org.apache.doris.catalog.OlapTable;
 import org.apache.doris.catalog.Partition;
 import org.apache.doris.catalog.Table;
-import org.apache.doris.catalog.TableIf.TableType;
+import org.apache.doris.catalog.TableIf;
+import org.apache.doris.catalog.View;
+import org.apache.doris.catalog.external.HMSExternalTable;
+import org.apache.doris.datasource.CatalogIf;
 import org.apache.doris.nereids.CascadesContext;
 import org.apache.doris.nereids.analyzer.UnboundRelation;
 import org.apache.doris.nereids.exceptions.AnalysisException;
@@ -29,6 +34,7 @@ import org.apache.doris.nereids.parser.NereidsParser;
 import org.apache.doris.nereids.rules.Rule;
 import org.apache.doris.nereids.rules.RuleType;
 import org.apache.doris.nereids.trees.plans.Plan;
+import org.apache.doris.nereids.trees.plans.logical.LogicalFileScan;
 import org.apache.doris.nereids.trees.plans.logical.LogicalOlapScan;
 import org.apache.doris.nereids.trees.plans.logical.LogicalPlan;
 import org.apache.doris.nereids.trees.plans.logical.LogicalSchemaScan;
@@ -64,12 +70,32 @@ public class BindRelation extends OneAnalysisRuleFactory {
                     // Use database name from table name parts.
                     return bindWithDbNameFromNamePart(ctx.cascadesContext, ctx.root);
                 }
+                case 3: { // catalog.db.table
+                    // Use catalog and database name from name parts.
+                    return bindWithCatalogNameFromNamePart(ctx.cascadesContext, ctx.root);
+                }
                 default:
                     throw new IllegalStateException("Table name [" + ctx.root.getTableName() + "] is invalid.");
             }
         }).toRule(RuleType.BINDING_RELATION);
     }
 
+    private TableIf getTable(String catalogName, String dbName, String tableName, Env env) {
+        CatalogIf catalog = env.getCatalogMgr().getCatalog(catalogName);
+        if (catalog == null) {
+            throw new RuntimeException(String.format("Catalog %s does not exist.", catalogName));
+        }
+        DatabaseIf<TableIf> db = null;
+        try {
+            db = (DatabaseIf<TableIf>) catalog.getDb(dbName)
+                    .orElseThrow(() -> new RuntimeException("Database [" + dbName + "] does not exist."));
+        } catch (Throwable e) {
+            throw new RuntimeException(e);
+        }
+        return db.getTable(tableName).orElseThrow(() -> new RuntimeException(
+            "Table [" + tableName + "] does not exist in database [" + dbName + "]."));
+    }
+
     private LogicalPlan bindWithCurrentDb(CascadesContext cascadesContext, UnboundRelation unboundRelation) {
         String tableName = unboundRelation.getNameParts().get(0);
         // check if it is a CTE's name
@@ -83,52 +109,64 @@ public class BindRelation extends OneAnalysisRuleFactory {
             }
             return new LogicalSubQueryAlias<>(tableName, ctePlan);
         }
-
+        String catalogName = cascadesContext.getConnectContext().getCurrentCatalog().getName();
         String dbName = cascadesContext.getConnectContext().getDatabase();
-        Table table = cascadesContext.getTable(dbName, tableName, cascadesContext.getConnectContext().getEnv());
+        TableIf table = getTable(catalogName, dbName, tableName, cascadesContext.getConnectContext().getEnv());
         // TODO: should generate different Scan sub class according to table's type
-        List<Long> partIds = getPartitionIds(table, unboundRelation);
-        if (table.getType() == TableType.OLAP) {
-            if (!CollectionUtils.isEmpty(partIds)) {
-                return new LogicalOlapScan(RelationUtil.newRelationId(),
-                        (OlapTable) table, ImmutableList.of(dbName), partIds);
-            } else {
-                return new LogicalOlapScan(RelationUtil.newRelationId(),
-                        (OlapTable) table, ImmutableList.of(dbName));
-            }
-        } else if (table.getType() == TableType.VIEW) {
-            Plan viewPlan = parseAndAnalyzeView(table.getDdlSql(), cascadesContext);
-            return new LogicalSubQueryAlias<>(table.getName(), viewPlan);
-        } else if (table.getType() == TableType.SCHEMA) {
-            return new LogicalSchemaScan(RelationUtil.newRelationId(), table, ImmutableList.of(dbName));
-        }
-        throw new AnalysisException("Unsupported tableType:" + table.getType());
+        return getLogicalPlan(table, unboundRelation, dbName, cascadesContext);
     }
 
     private LogicalPlan bindWithDbNameFromNamePart(CascadesContext cascadesContext, UnboundRelation unboundRelation) {
         List<String> nameParts = unboundRelation.getNameParts();
         ConnectContext connectContext = cascadesContext.getConnectContext();
+        String catalogName = cascadesContext.getConnectContext().getCurrentCatalog().getName();
         // if the relation is view, nameParts.get(0) is dbName.
         String dbName = nameParts.get(0);
         if (!dbName.equals(connectContext.getDatabase())) {
             dbName = connectContext.getClusterName() + ":" + dbName;
         }
-        Table table = cascadesContext.getTable(dbName, nameParts.get(1), connectContext.getEnv());
-        List<Long> partIds = getPartitionIds(table, unboundRelation);
-        if (table.getType() == TableType.OLAP) {
-            if (!CollectionUtils.isEmpty(partIds)) {
-                return new LogicalOlapScan(RelationUtil.newRelationId(), (OlapTable) table,
-                        ImmutableList.of(dbName), partIds);
-            } else {
-                return new LogicalOlapScan(RelationUtil.newRelationId(), (OlapTable) table, ImmutableList.of(dbName));
-            }
-        } else if (table.getType() == TableType.VIEW) {
-            Plan viewPlan = parseAndAnalyzeView(table.getDdlSql(), cascadesContext);
-            return new LogicalSubQueryAlias<>(table.getName(), viewPlan);
-        } else if (table.getType() == TableType.SCHEMA) {
-            return new LogicalSchemaScan(RelationUtil.newRelationId(), table, ImmutableList.of(dbName));
+        String tableName = nameParts.get(1);
+        TableIf table = getTable(catalogName, dbName, tableName, connectContext.getEnv());
+        return getLogicalPlan(table, unboundRelation, dbName, cascadesContext);
+    }
+
+    private LogicalPlan bindWithCatalogNameFromNamePart(CascadesContext cascadesContext,
+                                                        UnboundRelation unboundRelation) {
+        List<String> nameParts = unboundRelation.getNameParts();
+        ConnectContext connectContext = cascadesContext.getConnectContext();
+        String catalogName = nameParts.get(0);
+        String dbName = nameParts.get(1);
+        if (!dbName.equals(connectContext.getDatabase())) {
+            dbName = connectContext.getClusterName() + ":" + dbName;
+        }
+        String tableName = nameParts.get(2);
+        TableIf table = getTable(catalogName, dbName, tableName, connectContext.getEnv());
+        return getLogicalPlan(table, unboundRelation, dbName, cascadesContext);
+    }
+
+    private LogicalPlan getLogicalPlan(TableIf table, UnboundRelation unboundRelation, String dbName,
+                                       CascadesContext cascadesContext) {
+        switch (table.getType()) {
+            case OLAP:
+                List<Long> partIds = getPartitionIds(table, unboundRelation);
+                if (!CollectionUtils.isEmpty(partIds)) {
+                    return new LogicalOlapScan(RelationUtil.newRelationId(),
+                        (OlapTable) table, ImmutableList.of(dbName), partIds);
+                } else {
+                    return new LogicalOlapScan(RelationUtil.newRelationId(),
+                        (OlapTable) table, ImmutableList.of(dbName));
+                }
+            case VIEW:
+                Plan viewPlan = parseAndAnalyzeView(((View) table).getDdlSql(), cascadesContext);
+                return new LogicalSubQueryAlias<>(table.getName(), viewPlan);
+            case HMS_EXTERNAL_TABLE:
+                return new LogicalFileScan(cascadesContext.getStatementContext().getNextRelationId(),
+                    (HMSExternalTable) table, ImmutableList.of(dbName));
+            case SCHEMA:
+                return new LogicalSchemaScan(RelationUtil.newRelationId(), (Table) table, ImmutableList.of(dbName));
+            default:
+                throw new AnalysisException("Unsupported tableType:" + table.getType());
         }
-        throw new AnalysisException("Unsupported tableType:" + table.getType());
     }
 
     private Plan parseAndAnalyzeView(String viewSql, CascadesContext parentContext) {
@@ -141,12 +179,12 @@ public class BindRelation extends OneAnalysisRuleFactory {
         return viewContext.getMemo().copyOut(false);
     }
 
-    private List<Long> getPartitionIds(Table t, UnboundRelation unboundRelation) {
+    private List<Long> getPartitionIds(TableIf t, UnboundRelation unboundRelation) {
         List<String> parts = unboundRelation.getPartNames();
         if (CollectionUtils.isEmpty(parts)) {
             return Collections.emptyList();
         }
-        if (!t.getType().equals(TableType.OLAP)) {
+        if (!t.getType().equals(TableIf.TableType.OLAP)) {
             throw new IllegalStateException(String.format(
                     "Only OLAP table is support select by partition for now,"
                             + "Table: %s is not OLAP table", t.getName()));
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindSlotReference.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindSlotReference.java
index d438db12e..003967757 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindSlotReference.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/BindSlotReference.java
@@ -66,6 +66,7 @@ import org.apache.doris.nereids.trees.plans.logical.LogicalProject;
 import org.apache.doris.nereids.trees.plans.logical.LogicalRepeat;
 import org.apache.doris.nereids.trees.plans.logical.LogicalSetOperation;
 import org.apache.doris.nereids.trees.plans.logical.LogicalSort;
+import org.apache.doris.nereids.trees.plans.logical.UsingJoin;
 import org.apache.doris.planner.PlannerContext;
 
 import com.google.common.base.Preconditions;
@@ -76,6 +77,7 @@ import org.apache.commons.lang.StringUtils;
 
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -127,41 +129,63 @@ public class BindSlotReference implements AnalysisRuleFactory {
                     return new LogicalFilter<>(boundConjuncts, filter.child());
                 })
             ),
-            RuleType.BINDING_JOIN_SLOT.build(
-                    logicalJoin().when(Plan::canBind)
-                            .whenNot(j -> j.getJoinType().equals(JoinType.USING_JOIN)).thenApply(ctx -> {
-                                LogicalJoin<GroupPlan, GroupPlan> join = ctx.root;
-                                List<Expression> cond = join.getOtherJoinConjuncts().stream()
-                                        .map(expr -> bind(expr, join.children(), join, ctx.cascadesContext))
-                                        .collect(Collectors.toList());
-                                List<Expression> hashJoinConjuncts = join.getHashJoinConjuncts().stream()
-                                        .map(expr -> bind(expr, join.children(), join, ctx.cascadesContext))
-                                        .collect(Collectors.toList());
-                                return new LogicalJoin<>(join.getJoinType(),
-                                        hashJoinConjuncts, cond, join.getHint(), join.left(), join.right());
-                            })
-            ),
+
             RuleType.BINDING_USING_JOIN_SLOT.build(
-                    logicalJoin().when(j -> j.getJoinType().equals(JoinType.USING_JOIN)).thenApply(ctx -> {
-                        LogicalJoin<GroupPlan, GroupPlan> join = ctx.root;
-                        List<Expression> unboundSlots = join.getHashJoinConjuncts();
-                        List<Expression> leftSlots = unboundSlots.stream()
-                                .map(expr -> bind(expr, Collections.singletonList(join.left()),
-                                        join, ctx.cascadesContext))
-                                .collect(Collectors.toList());
-                        List<Expression> rightSlots = unboundSlots.stream()
-                                .map(expr -> bind(expr, Collections.singletonList(join.right()),
-                                        join, ctx.cascadesContext))
-                                .collect(Collectors.toList());
+                    usingJoin().thenApply(ctx -> {
+                        UsingJoin<GroupPlan, GroupPlan> using = ctx.root;
+                        LogicalJoin lj = new LogicalJoin(using.getJoinType() == JoinType.CROSS_JOIN
+                                ? JoinType.INNER_JOIN : using.getJoinType(),
+                                using.getHashJoinConjuncts(),
+                                using.getOtherJoinConjuncts(), using.getHint(), using.left(),
+                                using.right());
+                        List<Expression> unboundSlots = lj.getHashJoinConjuncts();
+                        Set<String> slotNames = new HashSet<>();
+                        List<Slot> leftOutput = new ArrayList<>(lj.left().getOutput());
+                        // Suppose A JOIN B USING(name) JOIN C USING(name), [A JOIN B] is the left node, in this case,
+                        // C should combine with table B on C.name=B.name. so we reverse the output to make sure that
+                        // the most right slot is matched with priority.
+                        Collections.reverse(leftOutput);
+                        List<Expression> leftSlots = new ArrayList<>();
+                        Scope scope = toScope(leftOutput.stream()
+                                .filter(s -> !slotNames.contains(s.getName()))
+                                .peek(s -> slotNames.add(s.getName())).collect(
+                                        Collectors.toList()));
+                        for (Expression unboundSlot : unboundSlots) {
+                            Expression expression = new SlotBinder(scope, lj, ctx.cascadesContext).bind(unboundSlot);
+                            leftSlots.add(expression);
+                        }
+                        slotNames.clear();
+                        scope = toScope(lj.right().getOutput().stream()
+                                .filter(s -> !slotNames.contains(s.getName()))
+                                .peek(s -> slotNames.add(s.getName())).collect(
+                                        Collectors.toList()));
+                        List<Expression> rightSlots = new ArrayList<>();
+                        for (Expression unboundSlot : unboundSlots) {
+                            Expression expression = new SlotBinder(scope, lj, ctx.cascadesContext).bind(unboundSlot);
+                            rightSlots.add(expression);
+                        }
                         int size = leftSlots.size();
                         List<Expression> hashEqExpr = new ArrayList<>();
                         for (int i = 0; i < size; i++) {
                             hashEqExpr.add(new EqualTo(leftSlots.get(i), rightSlots.get(i)));
                         }
-                        return new LogicalJoin(JoinType.INNER_JOIN, hashEqExpr,
-                                join.getOtherJoinConjuncts(), join.getHint(), join.left(), join.right());
+                        return lj.withHashJoinConjuncts(hashEqExpr);
                     })
                 ),
+                RuleType.BINDING_JOIN_SLOT.build(
+                        logicalJoin().when(Plan::canBind)
+                                .whenNot(j -> j.getJoinType().equals(JoinType.USING_JOIN)).thenApply(ctx -> {
+                                    LogicalJoin<GroupPlan, GroupPlan> join = ctx.root;
+                                    List<Expression> cond = join.getOtherJoinConjuncts().stream()
+                                            .map(expr -> bind(expr, join.children(), join, ctx.cascadesContext))
+                                            .collect(Collectors.toList());
+                                    List<Expression> hashJoinConjuncts = join.getHashJoinConjuncts().stream()
+                                            .map(expr -> bind(expr, join.children(), join, ctx.cascadesContext))
+                                            .collect(Collectors.toList());
+                                    return new LogicalJoin<>(join.getJoinType(),
+                                            hashJoinConjuncts, cond, join.getHint(), join.left(), join.right());
+                                })
+                ),
             RuleType.BINDING_AGGREGATE_SLOT.build(
                 logicalAggregate().when(Plan::canBind).thenApply(ctx -> {
                     LogicalAggregate<GroupPlan> agg = ctx.root;
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/ResolveOrdinalInOrderByAndGroupBy.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/ResolveOrdinalInOrderByAndGroupBy.java
index 9b551f68f..b04f085ce 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/ResolveOrdinalInOrderByAndGroupBy.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/analysis/ResolveOrdinalInOrderByAndGroupBy.java
@@ -66,6 +66,7 @@ public class ResolveOrdinalInOrderByAndGroupBy implements AnalysisRuleFactory {
                         logicalAggregate().then(agg -> {
                             List<NamedExpression> aggOutput = agg.getOutputExpressions();
                             List<Expression> groupByWithoutOrd = new ArrayList<>();
+                            boolean ordExists = false;
                             for (Expression groupByExpr : agg.getGroupByExpressions()) {
                                 groupByExpr = FoldConstantRule.INSTANCE.rewrite(groupByExpr);
                                 if (groupByExpr instanceof IntegerLikeLiteral) {
@@ -74,11 +75,17 @@ public class ResolveOrdinalInOrderByAndGroupBy implements AnalysisRuleFactory {
                                     checkOrd(ord, aggOutput.size());
                                     Expression aggExpr = aggOutput.get(ord - 1);
                                     groupByWithoutOrd.add(aggExpr);
+                                    ordExists = true;
                                 } else {
                                     groupByWithoutOrd.add(groupByExpr);
                                 }
                             }
-                            return new LogicalAggregate(groupByWithoutOrd, agg.getOutputExpressions(), agg.child());
+                            if (ordExists) {
+                                return new LogicalAggregate(groupByWithoutOrd, agg.getOutputExpressions(), agg.child());
+                            } else {
+                                return agg;
+                            }
+
                         }))).build();
     }
 
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/ExpressionRewrite.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/ExpressionRewrite.java
index dfa270992..11167739b 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/ExpressionRewrite.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/ExpressionRewrite.java
@@ -17,6 +17,7 @@
 
 package org.apache.doris.nereids.rules.expression.rewrite;
 
+import org.apache.doris.nereids.properties.OrderKey;
 import org.apache.doris.nereids.rules.Rule;
 import org.apache.doris.nereids.rules.RuleType;
 import org.apache.doris.nereids.rules.rewrite.OneRewriteRuleFactory;
@@ -35,6 +36,8 @@ import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Lists;
 
+import java.util.ArrayList;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Objects;
 import java.util.Set;
@@ -66,7 +69,10 @@ public class ExpressionRewrite implements RewriteRuleFactory {
                 new ProjectExpressionRewrite().build(),
                 new AggExpressionRewrite().build(),
                 new FilterExpressionRewrite().build(),
-                new JoinExpressionRewrite().build());
+                new JoinExpressionRewrite().build(),
+                new SortExpressionRewrite().build(),
+                new LogicalRepeatRewrite().build(),
+                new HavingExpressionRewrite().build());
     }
 
     private class GenerateExpressionRewrite extends OneRewriteRuleFactory {
@@ -183,4 +189,48 @@ public class ExpressionRewrite implements RewriteRuleFactory {
             }).toRule(RuleType.REWRITE_JOIN_EXPRESSION);
         }
     }
+
+    private class SortExpressionRewrite extends OneRewriteRuleFactory {
+
+        @Override
+        public Rule build() {
+            return logicalSort().then(sort -> {
+                List<OrderKey> orderKeys = sort.getOrderKeys();
+                List<OrderKey> rewrittenOrderKeys = new ArrayList<>();
+                for (OrderKey k : orderKeys) {
+                    Expression expression = rewriter.rewrite(k.getExpr());
+                    rewrittenOrderKeys.add(new OrderKey(expression, k.isAsc(), k.isNullFirst()));
+                }
+                return sort.withOrderByKey(rewrittenOrderKeys);
+            }).toRule(RuleType.REWRITE_SORT_EXPRESSION);
+        }
+    }
+
+    private class HavingExpressionRewrite extends OneRewriteRuleFactory {
+        @Override
+        public Rule build() {
+            return logicalHaving().then(having -> {
+                Set<Expression> rewrittenExpr = new HashSet<>();
+                for (Expression e : having.getExpressions()) {
+                    rewrittenExpr.add(rewriter.rewrite(e));
+                }
+                return having.withExpressions(rewrittenExpr);
+            }).toRule(RuleType.REWRITE_HAVING_EXPRESSSION);
+        }
+    }
+
+    private class LogicalRepeatRewrite extends OneRewriteRuleFactory {
+        @Override
+        public Rule build() {
+            return logicalRepeat().then(r -> {
+                List<List<Expression>> groupingExprs = new ArrayList<>();
+                for (List<Expression> expressions : r.getGroupingSets()) {
+                    groupingExprs.add(expressions.stream().map(rewriter::rewrite).collect(Collectors.toList()));
+                }
+                return r.withGroupSetsAndOutput(groupingExprs,
+                        r.getOutputExpressions().stream().map(rewriter::rewrite).map(e -> (NamedExpression) e)
+                                .collect(Collectors.toList()));
+            }).toRule(RuleType.REWRITE_REPEAT_EXPRESSSION);
+        }
+    }
 }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/rules/TypeCoercion.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/rules/TypeCoercion.java
index cee94f8af..a81aefc98 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/rules/TypeCoercion.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/expression/rewrite/rules/TypeCoercion.java
@@ -29,6 +29,7 @@ import org.apache.doris.nereids.trees.expressions.Cast;
 import org.apache.doris.nereids.trees.expressions.Divide;
 import org.apache.doris.nereids.trees.expressions.Expression;
 import org.apache.doris.nereids.trees.expressions.InPredicate;
+import org.apache.doris.nereids.trees.expressions.IntegralDivide;
 import org.apache.doris.nereids.trees.expressions.typecoercion.ImplicitCastInputTypes;
 import org.apache.doris.nereids.types.BigIntType;
 import org.apache.doris.nereids.types.DataType;
@@ -218,4 +219,12 @@ public class TypeCoercion extends AbstractExpressionRewriteRule {
             }
         });
     }
+
+    @Override
+    public Expression visitIntegralDivide(IntegralDivide integralDivide, ExpressionRewriteContext context) {
+        DataType commonType = BigIntType.INSTANCE;
+        Expression newLeft = TypeCoercionUtils.castIfNotSameType(integralDivide.left(), commonType);
+        Expression newRight = TypeCoercionUtils.castIfNotSameType(integralDivide.right(), commonType);
+        return integralDivide.withChildren(newLeft, newRight);
+    }
 }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/implementation/LogicalFileScanToPhysicalFileScan.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/implementation/LogicalFileScanToPhysicalFileScan.java
new file mode 100644
index 000000000..e277c6b7b
--- /dev/null
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/rules/implementation/LogicalFileScanToPhysicalFileScan.java
@@ -0,0 +1,43 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.nereids.rules.implementation;
+
+import org.apache.doris.nereids.properties.DistributionSpecAny;
+import org.apache.doris.nereids.rules.Rule;
+import org.apache.doris.nereids.rules.RuleType;
+import org.apache.doris.nereids.trees.plans.physical.PhysicalFileScan;
+
+import java.util.Optional;
+
+/**
+ * Implementation rule that convert logical FileScan to physical FileScan.
+ */
+public class LogicalFileScanToPhysicalFileScan extends OneImplementationRuleFactory {
+    @Override
+    public Rule build() {
+        return logicalFileScan().then(fileScan ->
+            new PhysicalFileScan(
+                    fileScan.getId(),
+                    fileScan.getTable(),
+                    fileScan.getQualifier(),
+                    DistributionSpecAny.INSTANCE,
+                    Optional.empty(),
+                    fileScan.getLogicalProperties())
+        ).toRule(RuleType.LOGICAL_FILE_SCAN_TO_PHYSICAL_FILE_SCAN_RULE);
+    }
+}
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/ExpressionEstimation.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/ExpressionEstimation.java
index 494dfe937..c65df4043 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/ExpressionEstimation.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/ExpressionEstimation.java
@@ -25,6 +25,7 @@ import org.apache.doris.nereids.trees.expressions.CaseWhen;
 import org.apache.doris.nereids.trees.expressions.Cast;
 import org.apache.doris.nereids.trees.expressions.Divide;
 import org.apache.doris.nereids.trees.expressions.Expression;
+import org.apache.doris.nereids.trees.expressions.IntegralDivide;
 import org.apache.doris.nereids.trees.expressions.Multiply;
 import org.apache.doris.nereids.trees.expressions.SlotReference;
 import org.apache.doris.nereids.trees.expressions.Subtract;
@@ -150,7 +151,7 @@ public class ExpressionEstimation extends ExpressionVisitor<ColumnStatistic, Sta
                     .setNumNulls(numNulls).setDataSize(dataSize).setMinValue(min).setMaxValue(max).setSelectivity(1.0)
                     .setMaxExpr(null).setMinExpr(null).build();
         }
-        if (binaryArithmetic instanceof Divide) {
+        if (binaryArithmetic instanceof Divide || binaryArithmetic instanceof IntegralDivide) {
             double min = Math.min(
                     Math.min(
                             Math.min(leftMin / noneZeroDivisor(rightMin), leftMin / noneZeroDivisor(rightMax)),
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/StatsCalculator.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/StatsCalculator.java
index 79b8b35d4..e464fd45a 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/StatsCalculator.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/stats/StatsCalculator.java
@@ -18,7 +18,7 @@
 package org.apache.doris.nereids.stats;
 
 import org.apache.doris.catalog.Env;
-import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.TableIf;
 import org.apache.doris.common.Id;
 import org.apache.doris.common.Pair;
 import org.apache.doris.nereids.memo.GroupExpression;
@@ -42,6 +42,7 @@ import org.apache.doris.nereids.trees.plans.logical.LogicalAggregate;
 import org.apache.doris.nereids.trees.plans.logical.LogicalAssertNumRows;
 import org.apache.doris.nereids.trees.plans.logical.LogicalEmptyRelation;
 import org.apache.doris.nereids.trees.plans.logical.LogicalExcept;
+import org.apache.doris.nereids.trees.plans.logical.LogicalFileScan;
 import org.apache.doris.nereids.trees.plans.logical.LogicalFilter;
 import org.apache.doris.nereids.trees.plans.logical.LogicalGenerate;
 import org.apache.doris.nereids.trees.plans.logical.LogicalIntersect;
@@ -60,6 +61,7 @@ import org.apache.doris.nereids.trees.plans.physical.PhysicalAssertNumRows;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalDistribute;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalEmptyRelation;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalExcept;
+import org.apache.doris.nereids.trees.plans.physical.PhysicalFileScan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalFilter;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalGenerate;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalHashAggregate;
@@ -171,6 +173,12 @@ public class StatsCalculator extends DefaultPlanVisitor<StatsDeriveResult, Void>
         return new StatsDeriveResult(1);
     }
 
+    @Override
+    public StatsDeriveResult visitLogicalFileScan(LogicalFileScan fileScan, Void context) {
+        fileScan.getExpressions();
+        return computeScan(fileScan);
+    }
+
     @Override
     public StatsDeriveResult visitLogicalTVFRelation(LogicalTVFRelation tvfRelation, Void context) {
         return tvfRelation.getFunction().computeStats(tvfRelation.getOutput());
@@ -256,6 +264,11 @@ public class StatsCalculator extends DefaultPlanVisitor<StatsDeriveResult, Void>
         return new StatsDeriveResult(1);
     }
 
+    @Override
+    public StatsDeriveResult visitPhysicalFileScan(PhysicalFileScan fileScan, Void context) {
+        return computeScan(fileScan);
+    }
+
     @Override
     public StatsDeriveResult visitPhysicalStorageLayerAggregate(
             PhysicalStorageLayerAggregate storageLayerAggregate, Void context) {
@@ -359,7 +372,7 @@ public class StatsCalculator extends DefaultPlanVisitor<StatsDeriveResult, Void>
         Set<SlotReference> slotSet = scan.getOutput().stream().filter(SlotReference.class::isInstance)
                 .map(s -> (SlotReference) s).collect(Collectors.toSet());
         Map<Id, ColumnStatistic> columnStatisticMap = new HashMap<>();
-        Table table = scan.getTable();
+        TableIf table = scan.getTable();
         double rowCount = scan.getTable().estimatedRowCount();
         for (SlotReference slotReference : slotSet) {
             String colName = slotReference.getName();
@@ -370,7 +383,6 @@ public class StatsCalculator extends DefaultPlanVisitor<StatsDeriveResult, Void>
                     Env.getCurrentEnv().getStatisticsCache().getColumnStatistics(table.getId(), colName);
             if (!colStats.isUnKnown) {
                 rowCount = colStats.count;
-
             }
             columnStatisticMap.put(slotReference.getExprId(), colStats);
         }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/IntegralDivide.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/IntegralDivide.java
new file mode 100644
index 000000000..42d6a5493
--- /dev/null
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/IntegralDivide.java
@@ -0,0 +1,60 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.nereids.trees.expressions;
+
+import org.apache.doris.analysis.ArithmeticExpr.Operator;
+import org.apache.doris.nereids.exceptions.UnboundException;
+import org.apache.doris.nereids.trees.expressions.visitor.ExpressionVisitor;
+import org.apache.doris.nereids.types.coercion.AbstractDataType;
+import org.apache.doris.nereids.types.coercion.NumericType;
+
+import com.google.common.base.Preconditions;
+
+import java.util.List;
+
+/**
+ * A DIV B
+ */
+public class IntegralDivide extends BinaryArithmetic {
+
+    public IntegralDivide(Expression left, Expression right) {
+        super(left, right, Operator.INT_DIVIDE);
+    }
+
+    @Override
+    public <R, C> R accept(ExpressionVisitor<R, C> visitor, C context) {
+        return visitor.visitIntegralDivide(this, context);
+    }
+
+    @Override
+    public AbstractDataType inputType() {
+        return NumericType.INSTANCE;
+    }
+
+    // Divide is implemented as a scalar function which return type is always nullable.
+    @Override
+    public boolean nullable() throws UnboundException {
+        return true;
+    }
+
+    @Override
+    public Expression withChildren(List<Expression> children) {
+        Preconditions.checkArgument(children.size() == 2);
+        return new IntegralDivide(children.get(0), children.get(1));
+    }
+}
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Avg.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Avg.java
index 3961d7ff1..2dbb0864d 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Avg.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Avg.java
@@ -21,7 +21,6 @@ import org.apache.doris.catalog.FunctionSignature;
 import org.apache.doris.nereids.exceptions.AnalysisException;
 import org.apache.doris.nereids.trees.expressions.Expression;
 import org.apache.doris.nereids.trees.expressions.functions.CustomSignature;
-import org.apache.doris.nereids.trees.expressions.functions.PropagateNullable;
 import org.apache.doris.nereids.trees.expressions.shape.UnaryExpression;
 import org.apache.doris.nereids.trees.expressions.visitor.ExpressionVisitor;
 import org.apache.doris.nereids.types.BigIntType;
@@ -38,14 +37,14 @@ import com.google.common.collect.ImmutableList;
 import java.util.List;
 
 /** avg agg function. */
-public class Avg extends AggregateFunction implements UnaryExpression, PropagateNullable, CustomSignature {
+public class Avg extends NullableAggregateFunction implements UnaryExpression, CustomSignature {
 
     public Avg(Expression child) {
-        super("avg", child);
+        this(false, false, child);
     }
 
-    public Avg(boolean isDistinct, Expression child) {
-        super("avg", isDistinct, child);
+    public Avg(boolean isDistinct, boolean isAlwaysNullable, Expression arg) {
+        super("avg", isAlwaysNullable, isDistinct, arg);
     }
 
     @Override
@@ -64,13 +63,18 @@ public class Avg extends AggregateFunction implements UnaryExpression, Propagate
     @Override
     public AggregateFunction withDistinctAndChildren(boolean isDistinct, List<Expression> children) {
         Preconditions.checkArgument(children.size() == 1);
-        return new Avg(isDistinct, children.get(0));
+        return new Avg(isDistinct, isAlwaysNullable, children.get(0));
     }
 
     @Override
     public Avg withChildren(List<Expression> children) {
         Preconditions.checkArgument(children.size() == 1);
-        return new Avg(isDistinct, children.get(0));
+        return new Avg(isDistinct, isAlwaysNullable, children.get(0));
+    }
+
+    @Override
+    public NullableAggregateFunction withAlwaysNullable(boolean isAlwaysNullable) {
+        return new Avg(isDistinct, isAlwaysNullable, children.get(0));
     }
 
     @Override
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Max.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Max.java
index 25764d8b7..f5bb11e87 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Max.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Max.java
@@ -20,7 +20,6 @@ package org.apache.doris.nereids.trees.expressions.functions.agg;
 import org.apache.doris.catalog.FunctionSignature;
 import org.apache.doris.nereids.trees.expressions.Expression;
 import org.apache.doris.nereids.trees.expressions.functions.CustomSignature;
-import org.apache.doris.nereids.trees.expressions.functions.PropagateNullable;
 import org.apache.doris.nereids.trees.expressions.shape.UnaryExpression;
 import org.apache.doris.nereids.trees.expressions.visitor.ExpressionVisitor;
 import org.apache.doris.nereids.types.DataType;
@@ -31,13 +30,13 @@ import com.google.common.collect.ImmutableList;
 import java.util.List;
 
 /** max agg function. */
-public class Max extends AggregateFunction implements UnaryExpression, PropagateNullable, CustomSignature {
+public class Max extends NullableAggregateFunction implements UnaryExpression, CustomSignature {
     public Max(Expression child) {
-        super("max", child);
+        this(false, false, child);
     }
 
-    public Max(boolean isDistinct, Expression arg) {
-        super("max", false, arg);
+    public Max(boolean isDistinct, boolean isAlwaysNullable, Expression arg) {
+        super("max", isAlwaysNullable, isDistinct, arg);
     }
 
     @Override
@@ -54,7 +53,7 @@ public class Max extends AggregateFunction implements UnaryExpression, Propagate
     @Override
     public Max withDistinctAndChildren(boolean isDistinct, List<Expression> children) {
         Preconditions.checkArgument(children.size() == 1);
-        return new Max(isDistinct, children.get(0));
+        return new Max(isDistinct, isAlwaysNullable, children.get(0));
     }
 
     @Override
@@ -63,6 +62,11 @@ public class Max extends AggregateFunction implements UnaryExpression, Propagate
         return new Max(children.get(0));
     }
 
+    @Override
+    public NullableAggregateFunction withAlwaysNullable(boolean isAlwaysNullable) {
+        return new Max(isDistinct, isAlwaysNullable, children.get(0));
+    }
+
     @Override
     public <R, C> R accept(ExpressionVisitor<R, C> visitor, C context) {
         return visitor.visitMax(this, context);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Min.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Min.java
index ea5e05584..c7f95ded7 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Min.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Min.java
@@ -20,7 +20,6 @@ package org.apache.doris.nereids.trees.expressions.functions.agg;
 import org.apache.doris.catalog.FunctionSignature;
 import org.apache.doris.nereids.trees.expressions.Expression;
 import org.apache.doris.nereids.trees.expressions.functions.CustomSignature;
-import org.apache.doris.nereids.trees.expressions.functions.PropagateNullable;
 import org.apache.doris.nereids.trees.expressions.shape.UnaryExpression;
 import org.apache.doris.nereids.trees.expressions.visitor.ExpressionVisitor;
 import org.apache.doris.nereids.types.DataType;
@@ -31,14 +30,14 @@ import com.google.common.collect.ImmutableList;
 import java.util.List;
 
 /** min agg function. */
-public class Min extends AggregateFunction implements UnaryExpression, PropagateNullable, CustomSignature {
+public class Min extends NullableAggregateFunction implements UnaryExpression, CustomSignature {
 
     public Min(Expression child) {
-        super("min", child);
+        this(false, false, child);
     }
 
-    public Min(boolean isDistinct, Expression arg) {
-        super("min", false, arg);
+    public Min(boolean isDistinct, boolean isAlwaysNullable, Expression arg) {
+        super("min", isAlwaysNullable, isDistinct, arg);
     }
 
     @Override
@@ -55,7 +54,7 @@ public class Min extends AggregateFunction implements UnaryExpression, Propagate
     @Override
     public Min withDistinctAndChildren(boolean isDistinct, List<Expression> children) {
         Preconditions.checkArgument(children.size() == 1);
-        return new Min(isDistinct, children.get(0));
+        return new Min(isDistinct, isAlwaysNullable, children.get(0));
     }
 
     @Override
@@ -64,6 +63,11 @@ public class Min extends AggregateFunction implements UnaryExpression, Propagate
         return new Min(children.get(0));
     }
 
+    @Override
+    public NullableAggregateFunction withAlwaysNullable(boolean isAlwaysNullable) {
+        return new Min(isDistinct, isAlwaysNullable, children.get(0));
+    }
+
     @Override
     public <R, C> R accept(ExpressionVisitor<R, C> visitor, C context) {
         return visitor.visitMin(this, context);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/NullableAggregateFunction.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/NullableAggregateFunction.java
new file mode 100644
index 000000000..7c47553f8
--- /dev/null
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/NullableAggregateFunction.java
@@ -0,0 +1,43 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.nereids.trees.expressions.functions.agg;
+
+import org.apache.doris.nereids.trees.expressions.Expression;
+import org.apache.doris.nereids.trees.expressions.functions.AlwaysNullable;
+import org.apache.doris.nereids.trees.expressions.functions.PropagateNullable;
+
+/**
+ * nullable aggregate function
+ */
+public abstract class NullableAggregateFunction extends AggregateFunction implements
+        PropagateNullable, AlwaysNullable {
+    protected final boolean isAlwaysNullable;
+
+    protected NullableAggregateFunction(String name, boolean isAlwaysNullable, boolean isDistinct,
+            Expression ...expressions) {
+        super(name, isDistinct, expressions);
+        this.isAlwaysNullable = isAlwaysNullable;
+    }
+
+    @Override
+    public boolean nullable() {
+        return isAlwaysNullable ? AlwaysNullable.super.nullable() : PropagateNullable.super.nullable();
+    }
+
+    public abstract NullableAggregateFunction withAlwaysNullable(boolean isAlwaysNullable);
+}
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Sum.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Sum.java
index 853f73f46..49a67724d 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Sum.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/functions/agg/Sum.java
@@ -21,7 +21,6 @@ import org.apache.doris.catalog.FunctionSignature;
 import org.apache.doris.nereids.exceptions.AnalysisException;
 import org.apache.doris.nereids.trees.expressions.Expression;
 import org.apache.doris.nereids.trees.expressions.functions.CustomSignature;
-import org.apache.doris.nereids.trees.expressions.functions.PropagateNullable;
 import org.apache.doris.nereids.trees.expressions.shape.UnaryExpression;
 import org.apache.doris.nereids.trees.expressions.visitor.ExpressionVisitor;
 import org.apache.doris.nereids.types.BigIntType;
@@ -38,13 +37,13 @@ import com.google.common.collect.ImmutableList;
 import java.util.List;
 
 /** sum agg function. */
-public class Sum extends AggregateFunction implements UnaryExpression, PropagateNullable, CustomSignature {
+public class Sum extends NullableAggregateFunction implements UnaryExpression, CustomSignature {
     public Sum(Expression child) {
-        super("sum", child);
+        this(false, false, child);
     }
 
-    public Sum(boolean isDistinct, Expression child) {
-        super("sum", isDistinct, child);
+    public Sum(boolean isDistinct, boolean isAlwaysNullable, Expression arg) {
+        super("sum", isAlwaysNullable, isDistinct, arg);
     }
 
     @Override
@@ -62,13 +61,18 @@ public class Sum extends AggregateFunction implements UnaryExpression, Propagate
     @Override
     public Sum withDistinctAndChildren(boolean isDistinct, List<Expression> children) {
         Preconditions.checkArgument(children.size() == 1);
-        return new Sum(isDistinct, children.get(0));
+        return new Sum(isDistinct, isAlwaysNullable, children.get(0));
     }
 
     @Override
     public Sum withChildren(List<Expression> children) {
         Preconditions.checkArgument(children.size() == 1);
-        return new Sum(isDistinct, children.get(0));
+        return new Sum(isDistinct, isAlwaysNullable, children.get(0));
+    }
+
+    @Override
+    public NullableAggregateFunction withAlwaysNullable(boolean isAlwaysNullable) {
+        return new Sum(isDistinct, isAlwaysNullable, children.get(0));
     }
 
     @Override
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/visitor/ExpressionVisitor.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/visitor/ExpressionVisitor.java
index a2383d5c2..5fc68638c 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/visitor/ExpressionVisitor.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/expressions/visitor/ExpressionVisitor.java
@@ -46,6 +46,7 @@ import org.apache.doris.nereids.trees.expressions.GreaterThan;
 import org.apache.doris.nereids.trees.expressions.GreaterThanEqual;
 import org.apache.doris.nereids.trees.expressions.InPredicate;
 import org.apache.doris.nereids.trees.expressions.InSubquery;
+import org.apache.doris.nereids.trees.expressions.IntegralDivide;
 import org.apache.doris.nereids.trees.expressions.IsNull;
 import org.apache.doris.nereids.trees.expressions.LessThan;
 import org.apache.doris.nereids.trees.expressions.LessThanEqual;
@@ -391,6 +392,10 @@ public abstract class ExpressionVisitor<R, C>
         return visit(boundStar, context);
     }
 
+    public R visitIntegralDivide(IntegralDivide integralDivide, C context) {
+        return visitBinaryArithmetic(integralDivide, context);
+    }
+
     /* ********************************************************************************************
      * Unbound expressions
      * ********************************************************************************************/
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/PlanType.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/PlanType.java
index c1681a08b..ab355837f 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/PlanType.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/PlanType.java
@@ -43,6 +43,7 @@ public enum PlanType {
     LOGICAL_LIMIT,
     LOGICAL_OLAP_SCAN,
     LOGICAL_SCHEMA_SCAN,
+    LOGICAL_FILE_SCAN,
     LOGICAL_APPLY,
     LOGICAL_SELECT_HINT,
     LOGICAL_ASSERT_NUM_ROWS,
@@ -52,12 +53,14 @@ public enum PlanType {
     LOGICAL_UNION,
     LOGICAL_EXCEPT,
     LOGICAL_INTERSECT,
+    LOGICAL_USING_JOIN,
     GROUP_PLAN,
 
     // physical plan
     PHYSICAL_EMPTY_RELATION,
     PHYSICAL_ONE_ROW_RELATION,
     PHYSICAL_OLAP_SCAN,
+    PHYSICAL_FILE_SCAN,
     PHYSICAL_TVF_RELATION,
     PHYSICAL_SCHEMA_SCAN,
     PHYSICAL_PROJECT,
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/algebra/Scan.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/algebra/Scan.java
index c6a0b454b..c221391e3 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/algebra/Scan.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/algebra/Scan.java
@@ -17,12 +17,12 @@
 
 package org.apache.doris.nereids.trees.plans.algebra;
 
-import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.TableIf;
 import org.apache.doris.nereids.analyzer.Relation;
 
 /**
  * Common interface for logical/physical scan.
  */
 public interface Scan extends Relation {
-    Table getTable();
+    TableIf getTable();
 }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalFileScan.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalFileScan.java
new file mode 100644
index 000000000..43152b9fd
--- /dev/null
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalFileScan.java
@@ -0,0 +1,87 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.nereids.trees.plans.logical;
+
+import org.apache.doris.catalog.external.ExternalTable;
+import org.apache.doris.nereids.memo.GroupExpression;
+import org.apache.doris.nereids.properties.LogicalProperties;
+import org.apache.doris.nereids.trees.plans.PlanType;
+import org.apache.doris.nereids.trees.plans.RelationId;
+import org.apache.doris.nereids.trees.plans.visitor.PlanVisitor;
+import org.apache.doris.nereids.util.Utils;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+import java.util.List;
+import java.util.Optional;
+
+/**
+ * Logical file scan for external catalog.
+ */
+public class LogicalFileScan extends LogicalRelation {
+
+    /**
+     * Constructor for LogicalFileScan.
+     */
+    public LogicalFileScan(RelationId id, ExternalTable table, List<String> qualifier,
+                           Optional<GroupExpression> groupExpression,
+                           Optional<LogicalProperties> logicalProperties) {
+        super(id, PlanType.LOGICAL_FILE_SCAN, table, qualifier,
+                groupExpression, logicalProperties);
+    }
+
+    public LogicalFileScan(RelationId id, ExternalTable table) {
+        this(id, table, ImmutableList.of());
+    }
+
+    public LogicalFileScan(RelationId id, ExternalTable table, List<String> qualifier) {
+        this(id, table, qualifier, Optional.empty(), Optional.empty());
+    }
+
+    @Override
+    public ExternalTable getTable() {
+        Preconditions.checkArgument(table instanceof ExternalTable);
+        return (ExternalTable) table;
+    }
+
+    @Override
+    public String toString() {
+        return Utils.toSqlString("LogicalFileScan",
+            "qualified", qualifiedName(),
+            "output", getOutput()
+        );
+    }
+
+    @Override
+    public LogicalFileScan withGroupExpression(Optional<GroupExpression> groupExpression) {
+        return new LogicalFileScan(id, (ExternalTable) table, qualifier, groupExpression,
+                Optional.of(getLogicalProperties()));
+    }
+
+    @Override
+    public LogicalFileScan withLogicalProperties(Optional<LogicalProperties> logicalProperties) {
+        return new LogicalFileScan(id, (ExternalTable) table, qualifier, groupExpression,
+            logicalProperties);
+    }
+
+    @Override
+    public <R, C> R accept(PlanVisitor<R, C> visitor, C context) {
+        return visitor.visitLogicalFileScan(this, context);
+    }
+}
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalHaving.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalHaving.java
index 3a0796b44..d6f0ae6d5 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalHaving.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalHaving.java
@@ -84,6 +84,11 @@ public class LogicalHaving<CHILD_TYPE extends Plan> extends LogicalUnary<CHILD_T
         return new LogicalHaving<>(conjuncts, Optional.empty(), logicalProperties, child());
     }
 
+    public Plan withExpressions(Set<Expression> expressions) {
+        return new LogicalHaving<Plan>(expressions, Optional.empty(),
+                Optional.of(getLogicalProperties()), child());
+    }
+
     @Override
     public List<Slot> computeOutput() {
         return child().getOutput();
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalOlapScan.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalOlapScan.java
index 2d6e40b94..b11b65d0c 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalOlapScan.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalOlapScan.java
@@ -27,7 +27,6 @@ import org.apache.doris.nereids.memo.GroupExpression;
 import org.apache.doris.nereids.properties.LogicalProperties;
 import org.apache.doris.nereids.trees.expressions.Slot;
 import org.apache.doris.nereids.trees.expressions.SlotReference;
-import org.apache.doris.nereids.trees.plans.Plan;
 import org.apache.doris.nereids.trees.plans.PlanType;
 import org.apache.doris.nereids.trees.plans.PreAggStatus;
 import org.apache.doris.nereids.trees.plans.RelationId;
@@ -196,33 +195,33 @@ public class LogicalOlapScan extends LogicalRelation implements CatalogRelation,
     }
 
     @Override
-    public Plan withGroupExpression(Optional<GroupExpression> groupExpression) {
-        return new LogicalOlapScan(id, table, qualifier, groupExpression, Optional.of(getLogicalProperties()),
+    public LogicalOlapScan withGroupExpression(Optional<GroupExpression> groupExpression) {
+        return new LogicalOlapScan(id, (Table) table, qualifier, groupExpression, Optional.of(getLogicalProperties()),
                 selectedPartitionIds, partitionPruned, selectedTabletIds, tabletPruned,
                 selectedIndexId, indexSelected, preAggStatus, manuallySpecifiedPartitions);
     }
 
     @Override
     public LogicalOlapScan withLogicalProperties(Optional<LogicalProperties> logicalProperties) {
-        return new LogicalOlapScan(id, table, qualifier, Optional.empty(), logicalProperties,
+        return new LogicalOlapScan(id, (Table) table, qualifier, Optional.empty(), logicalProperties,
                 selectedPartitionIds, partitionPruned, selectedTabletIds, tabletPruned,
                 selectedIndexId, indexSelected, preAggStatus, manuallySpecifiedPartitions);
     }
 
     public LogicalOlapScan withSelectedPartitionIds(List<Long> selectedPartitionIds) {
-        return new LogicalOlapScan(id, table, qualifier, Optional.empty(), Optional.of(getLogicalProperties()),
+        return new LogicalOlapScan(id, (Table) table, qualifier, Optional.empty(), Optional.of(getLogicalProperties()),
                 selectedPartitionIds, true, selectedTabletIds, tabletPruned,
                 selectedIndexId, indexSelected, preAggStatus, manuallySpecifiedPartitions);
     }
 
     public LogicalOlapScan withMaterializedIndexSelected(PreAggStatus preAgg, long indexId) {
-        return new LogicalOlapScan(id, table, qualifier, Optional.empty(), Optional.of(getLogicalProperties()),
+        return new LogicalOlapScan(id, (Table) table, qualifier, Optional.empty(), Optional.of(getLogicalProperties()),
                 selectedPartitionIds, partitionPruned, selectedTabletIds, tabletPruned,
                 indexId, true, preAgg, manuallySpecifiedPartitions);
     }
 
     public LogicalOlapScan withSelectedTabletIds(List<Long> selectedTabletIds) {
-        return new LogicalOlapScan(id, table, qualifier, Optional.empty(), Optional.of(getLogicalProperties()),
+        return new LogicalOlapScan(id, (Table) table, qualifier, Optional.empty(), Optional.of(getLogicalProperties()),
                 selectedPartitionIds, partitionPruned, selectedTabletIds, true,
                 selectedIndexId, indexSelected, preAggStatus, manuallySpecifiedPartitions);
     }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalRelation.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalRelation.java
index f1c88adfa..f959ecca4 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalRelation.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalRelation.java
@@ -18,7 +18,7 @@
 package org.apache.doris.nereids.trees.plans.logical;
 
 import org.apache.doris.catalog.OlapTable;
-import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.TableIf;
 import org.apache.doris.nereids.memo.GroupExpression;
 import org.apache.doris.nereids.properties.LogicalProperties;
 import org.apache.doris.nereids.trees.expressions.Expression;
@@ -42,12 +42,12 @@ import java.util.Optional;
  */
 public abstract class LogicalRelation extends LogicalLeaf implements Scan {
 
-    protected final Table table;
+    protected final TableIf table;
     protected final ImmutableList<String> qualifier;
 
     protected final RelationId id;
 
-    public LogicalRelation(RelationId id, PlanType type, Table table, List<String> qualifier) {
+    public LogicalRelation(RelationId id, PlanType type, TableIf table, List<String> qualifier) {
         this(id, type, table, qualifier, Optional.empty(), Optional.empty());
     }
 
@@ -63,7 +63,7 @@ public abstract class LogicalRelation extends LogicalLeaf implements Scan {
      * @param table Doris table
      * @param qualifier qualified relation name
      */
-    public LogicalRelation(RelationId id, PlanType type, Table table, List<String> qualifier,
+    public LogicalRelation(RelationId id, PlanType type, TableIf table, List<String> qualifier,
             Optional<GroupExpression> groupExpression, Optional<LogicalProperties> logicalProperties) {
         super(type, groupExpression, logicalProperties);
         this.table = Objects.requireNonNull(table, "table can not be null");
@@ -72,7 +72,7 @@ public abstract class LogicalRelation extends LogicalLeaf implements Scan {
     }
 
     @Override
-    public Table getTable() {
+    public TableIf getTable() {
         return table;
     }
 
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalSchemaScan.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalSchemaScan.java
index 3641e3631..402da0da4 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalSchemaScan.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/LogicalSchemaScan.java
@@ -18,7 +18,7 @@
 package org.apache.doris.nereids.trees.plans.logical;
 
 import org.apache.doris.catalog.SchemaTable;
-import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.TableIf;
 import org.apache.doris.nereids.memo.GroupExpression;
 import org.apache.doris.nereids.properties.LogicalProperties;
 import org.apache.doris.nereids.trees.expressions.Slot;
@@ -40,13 +40,13 @@ import java.util.Optional;
  */
 public class LogicalSchemaScan extends LogicalRelation implements Scan {
     public LogicalSchemaScan(RelationId id,
-            Table table,
+            TableIf table,
             List<String> qualifier) {
         super(id, PlanType.LOGICAL_SCHEMA_SCAN, table, qualifier);
     }
 
     public LogicalSchemaScan(RelationId id,
-            Table table,
+            TableIf table,
             List<String> qualifier, Optional<GroupExpression> groupExpression,
             Optional<LogicalProperties> logicalProperties) {
         super(id, PlanType.LOGICAL_SCHEMA_SCAN, table, qualifier, groupExpression, logicalProperties);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/UsingJoin.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/UsingJoin.java
new file mode 100644
index 000000000..8a9d7af75
--- /dev/null
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/logical/UsingJoin.java
@@ -0,0 +1,169 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.nereids.trees.plans.logical;
+
+import org.apache.doris.nereids.memo.GroupExpression;
+import org.apache.doris.nereids.properties.LogicalProperties;
+import org.apache.doris.nereids.trees.expressions.Expression;
+import org.apache.doris.nereids.trees.expressions.Slot;
+import org.apache.doris.nereids.trees.plans.JoinHint;
+import org.apache.doris.nereids.trees.plans.JoinType;
+import org.apache.doris.nereids.trees.plans.Plan;
+import org.apache.doris.nereids.trees.plans.PlanType;
+import org.apache.doris.nereids.trees.plans.algebra.Join;
+import org.apache.doris.nereids.trees.plans.visitor.PlanVisitor;
+import org.apache.doris.nereids.util.ExpressionUtils;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableList.Builder;
+import org.apache.commons.collections.CollectionUtils;
+
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/**
+ * select col1 from t1 join t2 using(col1);
+ */
+public class UsingJoin<LEFT_CHILD_TYPE extends Plan, RIGHT_CHILD_TYPE extends Plan>
+        extends LogicalBinary<LEFT_CHILD_TYPE, RIGHT_CHILD_TYPE> implements Join {
+
+    private final JoinType joinType;
+    private final ImmutableList<Expression> otherJoinConjuncts;
+    private final ImmutableList<Expression> hashJoinConjuncts;
+    private final JoinHint hint;
+
+    public UsingJoin(JoinType joinType, LEFT_CHILD_TYPE leftChild, RIGHT_CHILD_TYPE rightChild,
+            List<Expression> expressions, List<Expression> hashJoinConjuncts,
+            JoinHint hint) {
+        this(joinType, leftChild, rightChild, expressions,
+                hashJoinConjuncts, Optional.empty(), Optional.empty(), hint);
+    }
+
+    public UsingJoin(JoinType joinType, LEFT_CHILD_TYPE leftChild, RIGHT_CHILD_TYPE rightChild,
+            List<Expression> expressions, List<Expression> hashJoinConjuncts, Optional<GroupExpression> groupExpression,
+            Optional<LogicalProperties> logicalProperties,
+            JoinHint hint) {
+        super(PlanType.LOGICAL_USING_JOIN, groupExpression, logicalProperties, leftChild, rightChild);
+        this.joinType = joinType;
+        this.otherJoinConjuncts = ImmutableList.copyOf(expressions);
+        this.hashJoinConjuncts = ImmutableList.copyOf(hashJoinConjuncts);
+        this.hint = hint;
+    }
+
+    @Override
+    public List<Slot> computeOutput() {
+
+        List<Slot> newLeftOutput = left().getOutput().stream().map(o -> o.withNullable(true))
+                .collect(Collectors.toList());
+
+        List<Slot> newRightOutput = right().getOutput().stream().map(o -> o.withNullable(true))
+                .collect(Collectors.toList());
+
+        switch (joinType) {
+            case LEFT_SEMI_JOIN:
+            case LEFT_ANTI_JOIN:
+                return ImmutableList.copyOf(left().getOutput());
+            case RIGHT_SEMI_JOIN:
+            case RIGHT_ANTI_JOIN:
+                return ImmutableList.copyOf(right().getOutput());
+            case LEFT_OUTER_JOIN:
+                return ImmutableList.<Slot>builder()
+                        .addAll(left().getOutput())
+                        .addAll(newRightOutput)
+                        .build();
+            case RIGHT_OUTER_JOIN:
+                return ImmutableList.<Slot>builder()
+                        .addAll(newLeftOutput)
+                        .addAll(right().getOutput())
+                        .build();
+            case FULL_OUTER_JOIN:
+                return ImmutableList.<Slot>builder()
+                        .addAll(newLeftOutput)
+                        .addAll(newRightOutput)
+                        .build();
+            default:
+                return ImmutableList.<Slot>builder()
+                        .addAll(left().getOutput())
+                        .addAll(right().getOutput())
+                        .build();
+        }
+    }
+
+    @Override
+    public Plan withGroupExpression(Optional<GroupExpression> groupExpression) {
+        return new UsingJoin(joinType, child(0), child(1), otherJoinConjuncts,
+                hashJoinConjuncts, groupExpression, Optional.of(getLogicalProperties()), hint);
+    }
+
+    @Override
+    public Plan withLogicalProperties(Optional<LogicalProperties> logicalProperties) {
+        return new UsingJoin(joinType, child(0), child(1), otherJoinConjuncts,
+                hashJoinConjuncts, groupExpression, logicalProperties, hint);
+    }
+
+    @Override
+    public Plan withChildren(List<Plan> children) {
+        return new UsingJoin(joinType, children.get(0), children.get(1), otherJoinConjuncts,
+                hashJoinConjuncts, groupExpression, Optional.of(getLogicalProperties()), hint);
+    }
+
+    @Override
+    public <R, C> R accept(PlanVisitor<R, C> visitor, C context) {
+        return visitor.visit(this, context);
+    }
+
+    @Override
+    public List<? extends Expression> getExpressions() {
+        return new Builder<Expression>()
+                .addAll(hashJoinConjuncts)
+                .addAll(otherJoinConjuncts)
+                .build();
+    }
+
+    public JoinType getJoinType() {
+        return joinType;
+    }
+
+    public List<Expression> getOtherJoinConjuncts() {
+        return otherJoinConjuncts;
+    }
+
+    public List<Expression> getHashJoinConjuncts() {
+        return hashJoinConjuncts;
+    }
+
+    public JoinHint getHint() {
+        return hint;
+    }
+
+    @Override
+    public Optional<Expression> getOnClauseCondition() {
+        return ExpressionUtils.optionalAnd(hashJoinConjuncts, otherJoinConjuncts);
+    }
+
+    @Override
+    public boolean hasJoinHint() {
+        return hint != null;
+    }
+
+    @Override
+    public boolean hasJoinCondition() {
+        return !CollectionUtils.isEmpty(hashJoinConjuncts);
+    }
+}
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalFileScan.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalFileScan.java
new file mode 100644
index 000000000..49091d229
--- /dev/null
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalFileScan.java
@@ -0,0 +1,119 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.nereids.trees.plans.physical;
+
+import org.apache.doris.catalog.external.ExternalTable;
+import org.apache.doris.nereids.memo.GroupExpression;
+import org.apache.doris.nereids.properties.DistributionSpec;
+import org.apache.doris.nereids.properties.LogicalProperties;
+import org.apache.doris.nereids.properties.PhysicalProperties;
+import org.apache.doris.nereids.trees.plans.PlanType;
+import org.apache.doris.nereids.trees.plans.RelationId;
+import org.apache.doris.nereids.trees.plans.visitor.PlanVisitor;
+import org.apache.doris.nereids.util.Utils;
+import org.apache.doris.statistics.StatsDeriveResult;
+
+import java.util.List;
+import java.util.Objects;
+import java.util.Optional;
+
+/**
+ * Physical file scan for external catalog.
+ */
+public class PhysicalFileScan extends PhysicalRelation {
+
+    private final ExternalTable table;
+    private final DistributionSpec distributionSpec;
+
+    /**
+     * Constructor for PhysicalFileScan.
+     */
+    public PhysicalFileScan(RelationId id, ExternalTable table, List<String> qualifier,
+                            DistributionSpec distributionSpec, Optional<GroupExpression> groupExpression,
+                            LogicalProperties logicalProperties) {
+        super(id, PlanType.PHYSICAL_FILE_SCAN, qualifier, groupExpression, logicalProperties);
+        this.table = table;
+        this.distributionSpec = distributionSpec;
+    }
+
+    /**
+     * Constructor for PhysicalFileScan.
+     */
+    public PhysicalFileScan(RelationId id, ExternalTable table, List<String> qualifier,
+                            DistributionSpec distributionSpec, Optional<GroupExpression> groupExpression,
+                            LogicalProperties logicalProperties, PhysicalProperties physicalProperties,
+                            StatsDeriveResult statsDeriveResult) {
+        super(id, PlanType.PHYSICAL_FILE_SCAN, qualifier, groupExpression, logicalProperties,
+                physicalProperties, statsDeriveResult);
+        this.table = table;
+        this.distributionSpec = distributionSpec;
+    }
+
+    @Override
+    public String toString() {
+        return Utils.toSqlString("PhysicalFileScan",
+            "qualified", Utils.qualifiedName(qualifier, table.getName()),
+            "output", getOutput(),
+            "stats", statsDeriveResult
+        );
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass() || !super.equals(o)) {
+            return false;
+        }
+        PhysicalFileScan that = ((PhysicalFileScan) o);
+        return Objects.equals(table, that.table);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(id, table);
+    }
+
+    @Override
+    public <R, C> R accept(PlanVisitor<R, C> visitor, C context) {
+        return visitor.visitPhysicalFileScan(this, context);
+    }
+
+    @Override
+    public PhysicalFileScan withGroupExpression(Optional<GroupExpression> groupExpression) {
+        return new PhysicalFileScan(id, table, qualifier, distributionSpec, groupExpression, getLogicalProperties());
+    }
+
+    @Override
+    public PhysicalFileScan withLogicalProperties(Optional<LogicalProperties> logicalProperties) {
+        return new PhysicalFileScan(id, table, qualifier, distributionSpec, groupExpression, logicalProperties.get());
+    }
+
+    @Override
+    public ExternalTable getTable() {
+        return table;
+    }
+
+    @Override
+    public PhysicalFileScan withPhysicalPropertiesAndStats(PhysicalProperties physicalProperties,
+                                                       StatsDeriveResult statsDeriveResult) {
+        return new PhysicalFileScan(id, table, qualifier, distributionSpec, groupExpression, getLogicalProperties(),
+            physicalProperties, statsDeriveResult);
+    }
+}
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalStorageLayerAggregate.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalStorageLayerAggregate.java
index 08eebb3e3..461266e44 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalStorageLayerAggregate.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/physical/PhysicalStorageLayerAggregate.java
@@ -69,7 +69,7 @@ public class PhysicalStorageLayerAggregate extends PhysicalRelation {
 
     @Override
     public Table getTable() {
-        return relation.getTable();
+        return (Table) relation.getTable();
     }
 
     @Override
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/visitor/PlanVisitor.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/visitor/PlanVisitor.java
index 2af0bd3a3..af464b0fb 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/visitor/PlanVisitor.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/trees/plans/visitor/PlanVisitor.java
@@ -32,6 +32,7 @@ import org.apache.doris.nereids.trees.plans.logical.LogicalCTE;
 import org.apache.doris.nereids.trees.plans.logical.LogicalCheckPolicy;
 import org.apache.doris.nereids.trees.plans.logical.LogicalEmptyRelation;
 import org.apache.doris.nereids.trees.plans.logical.LogicalExcept;
+import org.apache.doris.nereids.trees.plans.logical.LogicalFileScan;
 import org.apache.doris.nereids.trees.plans.logical.LogicalFilter;
 import org.apache.doris.nereids.trees.plans.logical.LogicalGenerate;
 import org.apache.doris.nereids.trees.plans.logical.LogicalHaving;
@@ -57,6 +58,7 @@ import org.apache.doris.nereids.trees.plans.physical.PhysicalAssertNumRows;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalDistribute;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalEmptyRelation;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalExcept;
+import org.apache.doris.nereids.trees.plans.physical.PhysicalFileScan;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalFilter;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalGenerate;
 import org.apache.doris.nereids.trees.plans.physical.PhysicalHashAggregate;
@@ -168,6 +170,10 @@ public abstract class PlanVisitor<R, C> {
         return visitLogicalRelation(schemaScan, context);
     }
 
+    public R visitLogicalFileScan(LogicalFileScan fileScan, C context) {
+        return visitLogicalRelation(fileScan, context);
+    }
+
     public R visitLogicalTVFRelation(LogicalTVFRelation tvfRelation, C context) {
         return visitLogicalRelation(tvfRelation, context);
     }
@@ -261,6 +267,10 @@ public abstract class PlanVisitor<R, C> {
         return visitPhysicalScan(schemaScan, context);
     }
 
+    public R visitPhysicalFileScan(PhysicalFileScan fileScan, C context) {
+        return visitPhysicalScan(fileScan, context);
+    }
+
     public R visitPhysicalStorageLayerAggregate(PhysicalStorageLayerAggregate storageLayerAggregate, C context) {
         return storageLayerAggregate.getRelation().accept(this, context);
     }
diff --git a/fe/fe-core/src/main/java/org/apache/doris/nereids/util/TypeCoercionUtils.java b/fe/fe-core/src/main/java/org/apache/doris/nereids/util/TypeCoercionUtils.java
index 85b9f5a66..ccb0796ae 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/nereids/util/TypeCoercionUtils.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/nereids/util/TypeCoercionUtils.java
@@ -156,6 +156,14 @@ public class TypeCoercionUtils {
                 || leftType instanceof IntegralType && rightType instanceof DecimalV2Type) {
             return true;
         }
+        if (leftType instanceof FloatType && rightType instanceof DecimalV2Type
+                || leftType instanceof DecimalV2Type && rightType instanceof FloatType) {
+            return true;
+        }
+        if (leftType instanceof DoubleType && rightType instanceof DecimalV2Type
+                || leftType instanceof DecimalV2Type && rightType instanceof DoubleType) {
+            return true;
+        }
         // TODO: add decimal promotion support
         if (!(leftType instanceof DecimalV2Type)
                 && !(rightType instanceof DecimalV2Type)
@@ -230,7 +238,7 @@ public class TypeCoercionUtils {
                     || (right instanceof DateLikeType && left instanceof IntegralType)) {
             tightestCommonType = BigIntType.INSTANCE;
         }
-        return Optional.ofNullable(tightestCommonType);
+        return tightestCommonType == null ? Optional.of(DoubleType.INSTANCE) : Optional.of(tightestCommonType);
     }
 
     /**
diff --git a/fe/fe-core/src/main/java/org/apache/doris/planner/JdbcScanNode.java b/fe/fe-core/src/main/java/org/apache/doris/planner/JdbcScanNode.java
index b9da4b45c..c15a12371 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/planner/JdbcScanNode.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/planner/JdbcScanNode.java
@@ -195,6 +195,7 @@ public class JdbcScanNode extends ScanNode {
         msg.jdbc_scan_node.setTupleId(desc.getId().asInt());
         msg.jdbc_scan_node.setTableName(tableName);
         msg.jdbc_scan_node.setQueryString(getJdbcQueryStr());
+        msg.jdbc_scan_node.setTableType(jdbcType);
     }
 
     @Override
diff --git a/fe/fe-core/src/main/java/org/apache/doris/planner/external/ExternalFileScanNode.java b/fe/fe-core/src/main/java/org/apache/doris/planner/external/ExternalFileScanNode.java
index 4117f2053..37d7fd58f 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/planner/external/ExternalFileScanNode.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/planner/external/ExternalFileScanNode.java
@@ -192,6 +192,43 @@ public class ExternalFileScanNode extends ExternalScanNode {
         initParamCreateContexts(analyzer);
     }
 
+    /**
+     * Init ExternalFileScanNode, ONLY used for Nereids. Should NOT use this function in anywhere else.
+     */
+    public void init() throws UserException {
+        if (!Config.enable_vectorized_load) {
+            throw new UserException(
+                "Please set 'enable_vectorized_load=true' in fe.conf to enable external file scan node");
+        }
+
+        switch (type) {
+            case QUERY:
+                // prepare for partition prune
+                // computeColumnFilter();
+                if (this.desc.getTable() instanceof HMSExternalTable) {
+                    HMSExternalTable hmsTable = (HMSExternalTable) this.desc.getTable();
+                    initHMSExternalTable(hmsTable);
+                } else if (this.desc.getTable() instanceof FunctionGenTable) {
+                    FunctionGenTable table = (FunctionGenTable) this.desc.getTable();
+                    initFunctionGenTable(table, (ExternalFileTableValuedFunction) table.getTvf());
+                }
+                break;
+            default:
+                throw new UserException("Unknown type: " + type);
+        }
+
+        backendPolicy.init();
+        numNodes = backendPolicy.numBackends();
+        for (FileScanProviderIf scanProvider : scanProviders) {
+            ParamCreateContext context = scanProvider.createContext(analyzer);
+            context.createDestSlotMap();
+            initAndSetPrecedingFilter(context.fileGroup.getPrecedingFilterExpr(), context.srcTupleDescriptor, analyzer);
+            initAndSetWhereExpr(context.fileGroup.getWhereExpr(), context.destTupleDescriptor, analyzer);
+            context.conjuncts = conjuncts;
+            this.contexts.add(context);
+        }
+    }
+
     private void initHMSExternalTable(HMSExternalTable hmsTable) throws UserException {
         Preconditions.checkNotNull(hmsTable);
 
@@ -312,6 +349,25 @@ public class ExternalFileScanNode extends ExternalScanNode {
         }
     }
 
+    public void finalizeForNerieds() throws UserException {
+        Preconditions.checkState(contexts.size() == scanProviders.size(),
+                contexts.size() + " vs. " + scanProviders.size());
+        for (int i = 0; i < contexts.size(); ++i) {
+            ParamCreateContext context = contexts.get(i);
+            FileScanProviderIf scanProvider = scanProviders.get(i);
+            setDefaultValueExprs(scanProvider, context);
+            setColumnPositionMappingForTextFile(scanProvider, context);
+            finalizeParamsForLoad(context, analyzer);
+            createScanRangeLocations(context, scanProvider);
+            this.inputSplitsNum += scanProvider.getInputSplitNum();
+            this.totalFileSize += scanProvider.getInputFileSize();
+            if (scanProvider instanceof HiveScanProvider) {
+                this.totalPartitionNum = ((HiveScanProvider) scanProvider).getTotalPartitionNum();
+                this.readPartitionNum = ((HiveScanProvider) scanProvider).getReadPartitionNum();
+            }
+        }
+    }
+
     private void setColumnPositionMappingForTextFile(FileScanProviderIf scanProvider, ParamCreateContext context)
             throws UserException {
         if (type != Type.QUERY) {
diff --git a/fe/fe-core/src/main/java/org/apache/doris/policy/PolicyMgr.java b/fe/fe-core/src/main/java/org/apache/doris/policy/PolicyMgr.java
index b6321ada9..796513187 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/policy/PolicyMgr.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/policy/PolicyMgr.java
@@ -23,7 +23,10 @@ import org.apache.doris.analysis.CreatePolicyStmt;
 import org.apache.doris.analysis.DropPolicyStmt;
 import org.apache.doris.analysis.ShowPolicyStmt;
 import org.apache.doris.analysis.UserIdentity;
+import org.apache.doris.catalog.Database;
 import org.apache.doris.catalog.Env;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Table;
 import org.apache.doris.common.AnalysisException;
 import org.apache.doris.common.DdlException;
 import org.apache.doris.common.UserException;
@@ -137,6 +140,20 @@ public class PolicyMgr implements Writable {
      **/
     public void dropPolicy(DropPolicyStmt stmt) throws DdlException, AnalysisException {
         DropPolicyLog dropPolicyLog = DropPolicyLog.fromDropStmt(stmt);
+        if (dropPolicyLog.getType() == PolicyTypeEnum.STORAGE) {
+            List<Database> databases = Env.getCurrentEnv().getInternalCatalog().getDbs();
+            for (Database db : databases) {
+                List<Table> tables = db.getTables();
+                for (Table table : tables) {
+                    if (table instanceof OlapTable) {
+                        if (((OlapTable) table).getStoragePolicy().equals(dropPolicyLog.getPolicyName())) {
+                            throw new DdlException("the policy " + dropPolicyLog.getPolicyName() + " is used by table: "
+                                    + table.getName());
+                        }
+                    }
+                }
+            }
+        }
         writeLock();
         try {
             if (!existPolicy(dropPolicyLog)) {
diff --git a/fe/fe-core/src/main/java/org/apache/doris/qe/SessionVariable.java b/fe/fe-core/src/main/java/org/apache/doris/qe/SessionVariable.java
index 2ba8a5cfe..7faa49229 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/qe/SessionVariable.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/qe/SessionVariable.java
@@ -204,6 +204,9 @@ public class SessionVariable implements Serializable, Writable {
 
     // percentage of EXEC_MEM_LIMIT
     public static final String BROADCAST_HASHTABLE_MEM_LIMIT_PERCENTAGE = "broadcast_hashtable_mem_limit_percentage";
+
+    public static final String REWRITE_OR_TO_IN_PREDICATE_THRESHOLD = "rewrite_or_to_in_predicate_threshold";
+
     public static final String NEREIDS_STAR_SCHEMA_SUPPORT = "nereids_star_schema_support";
 
     public static final String NEREIDS_CBO_PENALTY_FACTOR = "nereids_cbo_penalty_factor";
@@ -554,6 +557,9 @@ public class SessionVariable implements Serializable, Writable {
     @VariableMgr.VarAttr(name = NEREIDS_STAR_SCHEMA_SUPPORT)
     private boolean nereidsStarSchemaSupport = true;
 
+    @VariableMgr.VarAttr(name = REWRITE_OR_TO_IN_PREDICATE_THRESHOLD)
+    private int rewriteOrToInPredicateThreshold = 2;
+
     @VariableMgr.VarAttr(name = NEREIDS_CBO_PENALTY_FACTOR)
     private double nereidsCboPenaltyFactor = 0.7;
     @VariableMgr.VarAttr(name = ENABLE_NEREIDS_TRACE)
@@ -702,6 +708,14 @@ public class SessionVariable implements Serializable, Writable {
         this.blockEncryptionMode = blockEncryptionMode;
     }
 
+    public void setRewriteOrToInPredicateThreshold(int threshold) {
+        this.rewriteOrToInPredicateThreshold = threshold;
+    }
+
+    public int getRewriteOrToInPredicateThreshold() {
+        return rewriteOrToInPredicateThreshold;
+    }
+
     public long getMaxExecMemByte() {
         return maxExecMemByte;
     }
@@ -1326,7 +1340,10 @@ public class SessionVariable implements Serializable, Writable {
         tResult.setEnablePipelineEngine(enablePipelineEngine);
         tResult.setReturnObjectDataAsBinary(returnObjectDataAsBinary);
         tResult.setTrimTailingSpacesForExternalTableQuery(trimTailingSpacesForExternalTableQuery);
-        tResult.setEnableShareHashTableForBroadcastJoin(enableShareHashTableForBroadcastJoin);
+
+        // TODO: enable share hashtable for broadcast after switching completely to pipeline engine.
+        tResult.setEnableShareHashTableForBroadcastJoin(
+                enablePipelineEngine ? false : enableShareHashTableForBroadcastJoin);
 
         tResult.setBatchSize(batchSize);
         tResult.setDisableStreamPreaggregations(disableStreamPreaggregations);
diff --git a/fe/fe-core/src/main/java/org/apache/doris/qe/StmtExecutor.java b/fe/fe-core/src/main/java/org/apache/doris/qe/StmtExecutor.java
index 065776ce6..f3c5e9d2c 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/qe/StmtExecutor.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/qe/StmtExecutor.java
@@ -314,6 +314,8 @@ public class StmtExecutor implements ProfileWriter {
         infos.put(ProfileManager.TOTAL_INSTANCES_NUM,
                 String.valueOf(beToInstancesNum.values().stream().reduce(0, Integer::sum)));
         infos.put(ProfileManager.INSTANCES_NUM_PER_BE, beToInstancesNum.toString());
+        infos.put(ProfileManager.PARALLEL_FRAGMENT_EXEC_INSTANCE,
+                String.valueOf(context.sessionVariable.parallelExecInstanceNum));
         return infos;
     }
 
diff --git a/fe/fe-core/src/main/java/org/apache/doris/rewrite/ExtractCommonFactorsRule.java b/fe/fe-core/src/main/java/org/apache/doris/rewrite/ExtractCommonFactorsRule.java
index 5a3bc34c8..3c808c610 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/rewrite/ExtractCommonFactorsRule.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/rewrite/ExtractCommonFactorsRule.java
@@ -28,6 +28,7 @@ import org.apache.doris.analysis.SlotRef;
 import org.apache.doris.analysis.TableName;
 import org.apache.doris.common.AnalysisException;
 import org.apache.doris.planner.PlanNode;
+import org.apache.doris.qe.ConnectContext;
 import org.apache.doris.rewrite.ExprRewriter.ClauseType;
 
 import com.google.common.base.Preconditions;
@@ -43,6 +44,7 @@ import org.apache.logging.log4j.Logger;
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.HashSet;
 import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Map;
@@ -462,6 +464,13 @@ public class ExtractCommonFactorsRule implements ExprRewriteRule {
         boolean isOrToInAllowed = true;
         Set<String> slotSet = new LinkedHashSet<>();
 
+        int rewriteThreshold;
+        if (ConnectContext.get() == null) {
+            rewriteThreshold = 2;
+        } else {
+            rewriteThreshold = ConnectContext.get().getSessionVariable().getRewriteOrToInPredicateThreshold();
+        }
+
         for (int i = 0; i < exprs.size(); i++) {
             Expr predicate = exprs.get(i);
             if (!(predicate instanceof BinaryPredicate) && !(predicate instanceof InPredicate)) {
@@ -492,22 +501,44 @@ public class ExtractCommonFactorsRule implements ExprRewriteRule {
         // isOrToInAllowed : true, means can rewrite
         // slotSet.size : nums of columnName in exprs, should be 1
         if (isOrToInAllowed && slotSet.size() == 1) {
-            // slotRef to get ColumnName
-
-            // SlotRef firstSlot = (SlotRef) exprs.get(0).getChild(0);
-            List<Expr> childrenList = exprs.get(0).getChildren();
-            inPredicate = new InPredicate(exprs.get(0).getChild(0),
-                    childrenList.subList(1, childrenList.size()), false);
-
-            for (int i = 1; i < exprs.size(); i++) {
-                childrenList = exprs.get(i).getChildren();
-                inPredicate.addChildren(childrenList.subList(1, childrenList.size()));
+            if (exprs.size() < rewriteThreshold) {
+                return null;
             }
+
+            // get deduplication list
+            List<Expr> deduplicationExprs = getDeduplicationList(exprs);
+            inPredicate = new InPredicate(deduplicationExprs.get(0),
+                    deduplicationExprs.subList(1, deduplicationExprs.size()), false);
         }
 
         return inPredicate;
     }
 
+    public List<Expr> getDeduplicationList(List<Expr> exprs) {
+        Set<Expr> set = new HashSet<>();
+        List<Expr> deduplicationExprList = new ArrayList<>();
+
+        deduplicationExprList.add(exprs.get(0).getChild(0));
+
+        for (Expr expr : exprs) {
+            if (expr instanceof BinaryPredicate) {
+                if (!set.contains(expr.getChild(1))) {
+                    set.add(expr.getChild(1));
+                    deduplicationExprList.add(expr.getChild(1));
+                }
+            } else {
+                List<Expr> childrenExprs = expr.getChildren();
+                for (Expr childrenExpr : childrenExprs.subList(1, childrenExprs.size())) {
+                    if (!set.contains(childrenExpr)) {
+                        set.add(childrenExpr);
+                        deduplicationExprList.add(childrenExpr);
+                    }
+                }
+            }
+        }
+        return deduplicationExprList;
+    }
+
     /**
      * Convert RangeSet to Compound Predicate
      * @param slotRef: <k1>
diff --git a/fe/fe-core/src/main/java/org/apache/doris/task/CreateReplicaTask.java b/fe/fe-core/src/main/java/org/apache/doris/task/CreateReplicaTask.java
index d416c7579..eeef983cd 100644
--- a/fe/fe-core/src/main/java/org/apache/doris/task/CreateReplicaTask.java
+++ b/fe/fe-core/src/main/java/org/apache/doris/task/CreateReplicaTask.java
@@ -88,7 +88,7 @@ public class CreateReplicaTask extends AgentTask {
     private boolean isRecoverTask = false;
 
     private DataSortInfo dataSortInfo;
-    private static String storagePolicy;
+    private String storagePolicy;
 
     private boolean enableUniqueKeyMergeOnWrite;
 
diff --git a/fe/fe-core/src/main/jflex/sql_scanner.flex b/fe/fe-core/src/main/jflex/sql_scanner.flex
index 2b0b02ecc..730e6affb 100644
--- a/fe/fe-core/src/main/jflex/sql_scanner.flex
+++ b/fe/fe-core/src/main/jflex/sql_scanner.flex
@@ -120,6 +120,7 @@ import org.apache.doris.qe.SqlModeHelper;
         keywordMap.put("bitmap", new Integer(SqlParserSymbols.KW_BITMAP));
         keywordMap.put("inverted", new Integer(SqlParserSymbols.KW_INVERTED));
         keywordMap.put("bitmap_union", new Integer(SqlParserSymbols.KW_BITMAP_UNION));
+        keywordMap.put("ngram_bf", new Integer(SqlParserSymbols.KW_NGRAM_BF));
         keywordMap.put("blob", new Integer(SqlParserSymbols.KW_BLOB));
         keywordMap.put("boolean", new Integer(SqlParserSymbols.KW_BOOLEAN));
         keywordMap.put("broker", new Integer(SqlParserSymbols.KW_BROKER));
diff --git a/fe/fe-core/src/test/java/org/apache/doris/catalog/ColumnTypeTest.java b/fe/fe-core/src/test/java/org/apache/doris/catalog/ColumnTypeTest.java
index 9a69d908c..8779c2d7d 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/catalog/ColumnTypeTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/catalog/ColumnTypeTest.java
@@ -125,7 +125,7 @@ public class ColumnTypeTest {
     public void testDatetimeV2() throws AnalysisException {
         TypeDef type = TypeDef.createDatetimeV2(3);
         type.analyze(null);
-        Assert.assertEquals("datetime(3)", type.toString());
+        Assert.assertEquals("datetimev2(3)", type.toString());
         Assert.assertEquals(PrimitiveType.DATETIMEV2, type.getType().getPrimitiveType());
         Assert.assertEquals(ScalarType.DATETIME_PRECISION, ((ScalarType) type.getType()).getScalarPrecision());
         Assert.assertEquals(3, ((ScalarType) type.getType()).getScalarScale());
diff --git a/fe/fe-core/src/test/java/org/apache/doris/metric/MetricsTest.java b/fe/fe-core/src/test/java/org/apache/doris/metric/MetricsTest.java
index b4f0e73ec..044649502 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/metric/MetricsTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/metric/MetricsTest.java
@@ -66,6 +66,10 @@ public class MetricsTest {
         for (Map.Entry<String, Histogram> entry : histograms.entrySet()) {
             visitor.visitHistogram(sb, MetricVisitor.FE_PREFIX, entry.getKey(), entry.getValue());
         }
-        Assert.assertTrue(sb.toString().contains("# TYPE doris_fe_query_latency_ms summary"));
+        String metricResult = sb.toString();
+        Assert.assertTrue(metricResult.contains("# TYPE doris_fe_query_latency_ms summary"));
+        Assert.assertTrue(metricResult.contains("doris_fe_query_latency_ms{quantile=\"0.999\"} 0.0"));
+        Assert.assertTrue(metricResult.contains("doris_fe_query_latency_ms{quantile=\"0.999\",db=\"test_db\"} 10.0"));
+
     }
 }
diff --git a/fe/fe-core/src/test/java/org/apache/doris/nereids/jobs/RewriteTopDownJobTest.java b/fe/fe-core/src/test/java/org/apache/doris/nereids/jobs/RewriteTopDownJobTest.java
index e993d764b..b80d9af8d 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/nereids/jobs/RewriteTopDownJobTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/nereids/jobs/RewriteTopDownJobTest.java
@@ -17,7 +17,7 @@
 
 package org.apache.doris.nereids.jobs;
 
-import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.TableIf;
 import org.apache.doris.nereids.analyzer.UnboundRelation;
 import org.apache.doris.nereids.memo.Group;
 import org.apache.doris.nereids.memo.GroupExpression;
@@ -103,11 +103,11 @@ public class RewriteTopDownJobTest {
 
     private static class LogicalBoundRelation extends LogicalRelation {
 
-        public LogicalBoundRelation(Table table, List<String> qualifier) {
+        public LogicalBoundRelation(TableIf table, List<String> qualifier) {
             super(RelationUtil.newRelationId(), PlanType.LOGICAL_BOUND_RELATION, table, qualifier);
         }
 
-        public LogicalBoundRelation(Table table, List<String> qualifier, Optional<GroupExpression> groupExpression,
+        public LogicalBoundRelation(TableIf table, List<String> qualifier, Optional<GroupExpression> groupExpression,
                 Optional<LogicalProperties> logicalProperties) {
             super(RelationUtil.newRelationId(), PlanType.LOGICAL_BOUND_RELATION, table, qualifier,
                     groupExpression, logicalProperties);
diff --git a/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/analysis/FillUpMissingSlotsTest.java b/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/analysis/FillUpMissingSlotsTest.java
index e325504c2..29d418593 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/analysis/FillUpMissingSlotsTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/analysis/FillUpMissingSlotsTest.java
@@ -32,6 +32,7 @@ import org.apache.doris.nereids.trees.expressions.SlotReference;
 import org.apache.doris.nereids.trees.expressions.functions.agg.Count;
 import org.apache.doris.nereids.trees.expressions.functions.agg.Min;
 import org.apache.doris.nereids.trees.expressions.functions.agg.Sum;
+import org.apache.doris.nereids.trees.expressions.literal.BigIntLiteral;
 import org.apache.doris.nereids.trees.expressions.literal.Literal;
 import org.apache.doris.nereids.trees.expressions.literal.SmallIntLiteral;
 import org.apache.doris.nereids.trees.expressions.literal.TinyIntLiteral;
@@ -312,14 +313,16 @@ public class FillUpMissingSlotsTest extends AnalyzeCheckTestBase implements Patt
 
         ExceptionChecker.expectThrowsWithMsg(
                 AnalysisException.class,
-                "Aggregate functions in having clause can't be nested: sum((a1 + avg(a2))).",
+                "Aggregate functions in having clause can't be nested:"
+                        + " sum(cast((cast(a1 as DOUBLE) + avg(cast(a2 as DOUBLE))) as SMALLINT)).",
                 () -> PlanChecker.from(connectContext).analyze(
                         "SELECT a1 FROM t1 GROUP BY a1 HAVING SUM(a1 + AVG(a2)) > 0"
                 ));
 
         ExceptionChecker.expectThrowsWithMsg(
                 AnalysisException.class,
-                "Aggregate functions in having clause can't be nested: sum(((a1 + a2) + avg(a2))).",
+                "Aggregate functions in having clause can't be nested:"
+                        + " sum(cast((cast((a1 + a2) as DOUBLE) + avg(cast(a2 as DOUBLE))) as INT)).",
                 () -> PlanChecker.from(connectContext).analyze(
                         "SELECT a1 FROM t1 GROUP BY a1 HAVING SUM(a1 + a2 + AVG(a2)) > 0"
                 ));
@@ -530,8 +533,8 @@ public class FillUpMissingSlotsTest extends AnalyzeCheckTestBase implements Patt
                                         ImmutableList.of(
                                                 new OrderKey(pk, true, true),
                                                 new OrderKey(countA11.toSlot(), true, true),
-                                                new OrderKey(new Add(sumA1A2.toSlot(), new TinyIntLiteral((byte) 1)), true, true),
-                                                new OrderKey(new Add(v1.toSlot(), new TinyIntLiteral((byte) 1)), true, true),
+                                                new OrderKey(new Add(sumA1A2.toSlot(), new BigIntLiteral((byte) 1)), true, true),
+                                                new OrderKey(new Add(v1.toSlot(), new BigIntLiteral((byte) 1)), true, true),
                                                 new OrderKey(v1.toSlot(), true, true)
                                         )
                                 ))
diff --git a/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/rewrite/logical/AggregateStrategiesTest.java b/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/rewrite/logical/AggregateStrategiesTest.java
index 0376aa23a..c59191e89 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/rewrite/logical/AggregateStrategiesTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/nereids/rules/rewrite/logical/AggregateStrategiesTest.java
@@ -267,7 +267,7 @@ public class AggregateStrategiesTest implements PatternMatchSupported {
         AggregateParam phaseOneSumAggParam = new AggregateParam(AggPhase.LOCAL, AggMode.INPUT_TO_BUFFER);
         AggregateParam phaseTwoSumAggParam = new AggregateParam(AggPhase.GLOBAL, AggMode.INPUT_TO_RESULT);
         // sum
-        Sum sumId = new Sum(false, id.toSlot());
+        Sum sumId = new Sum(false, false, id.toSlot());
 
         PlanChecker.from(MemoTestUtils.createConnectContext(), root)
                 .applyImplementation(twoPhaseAggregateWithDistinct())
diff --git a/fe/fe-core/src/test/java/org/apache/doris/nereids/util/TypeCoercionUtilsTest.java b/fe/fe-core/src/test/java/org/apache/doris/nereids/util/TypeCoercionUtilsTest.java
index cd40e2876..bbd75d48c 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/nereids/util/TypeCoercionUtilsTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/nereids/util/TypeCoercionUtilsTest.java
@@ -145,7 +145,7 @@ public class TypeCoercionUtilsTest {
         testFindTightestCommonType(BigIntType.INSTANCE, IntegerType.INSTANCE, BigIntType.INSTANCE);
         testFindTightestCommonType(StringType.INSTANCE, StringType.INSTANCE, IntegerType.INSTANCE);
         testFindTightestCommonType(StringType.INSTANCE, IntegerType.INSTANCE, StringType.INSTANCE);
-        testFindTightestCommonType(null, DecimalV2Type.SYSTEM_DEFAULT, DecimalV2Type.createDecimalV2Type(2, 1));
+        testFindTightestCommonType(DoubleType.INSTANCE, DecimalV2Type.SYSTEM_DEFAULT, DecimalV2Type.createDecimalV2Type(2, 1));
         testFindTightestCommonType(VarcharType.createVarcharType(10), CharType.createCharType(8), CharType.createCharType(10));
         testFindTightestCommonType(VarcharType.createVarcharType(10), VarcharType.createVarcharType(8), VarcharType.createVarcharType(10));
         testFindTightestCommonType(VarcharType.createVarcharType(10), VarcharType.createVarcharType(8), CharType.createCharType(10));
diff --git a/fe/fe-core/src/test/java/org/apache/doris/planner/QueryPlanTest.java b/fe/fe-core/src/test/java/org/apache/doris/planner/QueryPlanTest.java
index fb8c9866f..034f673dd 100644
--- a/fe/fe-core/src/test/java/org/apache/doris/planner/QueryPlanTest.java
+++ b/fe/fe-core/src/test/java/org/apache/doris/planner/QueryPlanTest.java
@@ -2238,5 +2238,22 @@ public class QueryPlanTest extends TestWithFeService {
         sql = "SELECT * from test1 where (query_time = 1 or query_time = 2) and (scan_bytes = 2 or scan_bytes = 3)";
         explainString = UtFrameUtils.getSQLPlanOrErrorMsg(connectContext, "EXPLAIN " + sql);
         Assert.assertTrue(explainString.contains("PREDICATES: `query_time` IN (1, 2), `scan_bytes` IN (2, 3)"));
+
+        sql = "SELECT * from test1 where query_time = 1 or query_time = 2 or query_time = 3 or query_time = 1";
+        explainString = UtFrameUtils.getSQLPlanOrErrorMsg(connectContext, "EXPLAIN " + sql);
+        Assert.assertTrue(explainString.contains("PREDICATES: `query_time` IN (1, 2, 3)"));
+
+        sql = "SELECT * from test1 where query_time = 1 or query_time = 2 or query_time in (3, 2)";
+        explainString = UtFrameUtils.getSQLPlanOrErrorMsg(connectContext, "EXPLAIN " + sql);
+        Assert.assertTrue(explainString.contains("PREDICATES: `query_time` IN (1, 2, 3)"));
+
+        connectContext.getSessionVariable().setRewriteOrToInPredicateThreshold(100);
+        sql = "SELECT * from test1 where query_time = 1 or query_time = 2 or query_time in (3, 4)";
+        explainString = UtFrameUtils.getSQLPlanOrErrorMsg(connectContext, "EXPLAIN " + sql);
+        Assert.assertTrue(explainString.contains("PREDICATES: (`query_time` = 1 OR `query_time` = 2 OR `query_time` IN (3, 4))"));
+
+        sql = "SELECT * from test1 where (query_time = 1 or query_time = 2) and query_time in (3, 4)";
+        explainString = UtFrameUtils.getSQLPlanOrErrorMsg(connectContext, "EXPLAIN " + sql);
+        Assert.assertTrue(explainString.contains("PREDICATES: (`query_time` = 1 OR `query_time` = 2), `query_time` IN (3, 4)"));
     }
 }
diff --git a/gensrc/proto/olap_file.proto b/gensrc/proto/olap_file.proto
index 3e20ff834..ab0791006 100644
--- a/gensrc/proto/olap_file.proto
+++ b/gensrc/proto/olap_file.proto
@@ -190,6 +190,7 @@ enum IndexType {
     BITMAP = 0;
     INVERTED = 1;
     BLOOMFILTER = 2;
+    NGRAM_BF = 3;
 }
 
 message TabletIndexPB {
diff --git a/gensrc/proto/segment_v2.proto b/gensrc/proto/segment_v2.proto
index 7a6928604..629f1a4fc 100644
--- a/gensrc/proto/segment_v2.proto
+++ b/gensrc/proto/segment_v2.proto
@@ -265,11 +265,13 @@ message BitmapIndexPB {
 
 enum HashStrategyPB {
     HASH_MURMUR3_X64_64 = 0;
+    CITY_HASH_64 = 1;
 }
 
 enum BloomFilterAlgorithmPB {
     BLOCK_BLOOM_FILTER = 0;
     CLASSIC_BLOOM_FILTER = 1;
+    NGRAM_BLOOM_FILTER = 2;
 }
 
 message BloomFilterIndexPB {
diff --git a/gensrc/thrift/Descriptors.thrift b/gensrc/thrift/Descriptors.thrift
index d133b3a27..1db9bd053 100644
--- a/gensrc/thrift/Descriptors.thrift
+++ b/gensrc/thrift/Descriptors.thrift
@@ -34,6 +34,9 @@ struct TColumn {
     10: optional list<TColumn> children_column
     11: optional i32 col_unique_id  = -1
     12: optional bool has_bitmap_index = false
+    13: optional bool has_ngram_bf_index = false
+    14: optional i32 gram_size
+    15: optional i32 gram_bf_size
 }
 
 struct TSlotDescriptor {
@@ -120,7 +123,8 @@ enum THdfsCompression {
 enum TIndexType {
   BITMAP,
   INVERTED,
-  BLOOMFILTER
+  BLOOMFILTER,
+  NGRAM_BF
 }
 
 // Mapping from names defined by Avro to the enum.
diff --git a/gensrc/thrift/PlanNodes.thrift b/gensrc/thrift/PlanNodes.thrift
index a7a21a049..613c884a0 100644
--- a/gensrc/thrift/PlanNodes.thrift
+++ b/gensrc/thrift/PlanNodes.thrift
@@ -406,6 +406,7 @@ struct TJdbcScanNode {
   1: optional Types.TTupleId tuple_id
   2: optional string table_name
   3: optional string query_string
+  4: optional Types.TOdbcTableType table_type
 }
 
 
diff --git a/regression-test/data/decimalv3/test_decimalv3.out b/regression-test/data/decimalv3/test_decimalv3.out
index 1bb8b045c..f8d56b4c4 100644
Binary files a/regression-test/data/decimalv3/test_decimalv3.out and b/regression-test/data/decimalv3/test_decimalv3.out differ
diff --git a/regression-test/data/external_table_emr_p2/hive/test_external_yandex_nereids.out b/regression-test/data/external_table_emr_p2/hive/test_external_yandex_nereids.out
new file mode 100644
index 000000000..dc56cd16a
Binary files /dev/null and b/regression-test/data/external_table_emr_p2/hive/test_external_yandex_nereids.out differ
diff --git a/regression-test/data/nereids_syntax_p0/lateral_view.out b/regression-test/data/nereids_syntax_p0/lateral_view.out
index 553afb4e5..ee4cb2b69 100644
Binary files a/regression-test/data/nereids_syntax_p0/lateral_view.out and b/regression-test/data/nereids_syntax_p0/lateral_view.out differ
diff --git a/regression-test/data/nereids_syntax_p0/select_const.out b/regression-test/data/nereids_syntax_p0/select_const.out
new file mode 100644
index 000000000..90b14f899
Binary files /dev/null and b/regression-test/data/nereids_syntax_p0/select_const.out differ
diff --git a/regression-test/data/nereids_syntax_p0/test_arithmetic_operators.out b/regression-test/data/nereids_syntax_p0/test_arithmetic_operators.out
new file mode 100644
index 000000000..fbe9d1802
Binary files /dev/null and b/regression-test/data/nereids_syntax_p0/test_arithmetic_operators.out differ
diff --git a/regression-test/data/nereids_syntax_p0/test_join3.out b/regression-test/data/nereids_syntax_p0/test_join3.out
new file mode 100644
index 000000000..3a1541bde
Binary files /dev/null and b/regression-test/data/nereids_syntax_p0/test_join3.out differ
diff --git a/regression-test/data/nereids_syntax_p0/test_query_between.out b/regression-test/data/nereids_syntax_p0/test_query_between.out
new file mode 100644
index 000000000..c97f6621c
Binary files /dev/null and b/regression-test/data/nereids_syntax_p0/test_query_between.out differ
diff --git a/regression-test/data/nereids_syntax_p0/unary_binary_arithmetic.out b/regression-test/data/nereids_syntax_p0/unary_binary_arithmetic.out
index f403badf7..78e08d773 100644
Binary files a/regression-test/data/nereids_syntax_p0/unary_binary_arithmetic.out and b/regression-test/data/nereids_syntax_p0/unary_binary_arithmetic.out differ
diff --git a/regression-test/data/performance_p0/redundant_conjuncts.out b/regression-test/data/performance_p0/redundant_conjuncts.out
index 7dbabccf3..98178f31a 100644
Binary files a/regression-test/data/performance_p0/redundant_conjuncts.out and b/regression-test/data/performance_p0/redundant_conjuncts.out differ
diff --git a/regression-test/suites/cold_heat_separation/policy/drop.groovy b/regression-test/suites/cold_heat_separation/policy/drop.groovy
index 57976f8f0..e947e20c7 100644
--- a/regression-test/suites/cold_heat_separation/policy/drop.groovy
+++ b/regression-test/suites/cold_heat_separation/policy/drop.groovy
@@ -92,8 +92,8 @@ suite("drop_policy") {
         def drop_policy_ret = try_sql """
             DROP STORAGE POLICY drop_policy_test
         """
-        // errCode = 2, detailMessage = current not support drop storage policy.
-        assertEquals(drop_policy_ret, null)
+        // can drop, no table use
+        assertEquals(drop_policy_ret.size(), 1)
     }
 
 }
diff --git a/regression-test/suites/cold_heat_separation/use_policy/alter_table_add_policy.groovy b/regression-test/suites/cold_heat_separation/use_policy/alter_table_add_policy.groovy
index 296365090..bff146f6e 100644
--- a/regression-test/suites/cold_heat_separation/use_policy/alter_table_add_policy.groovy
+++ b/regression-test/suites/cold_heat_separation/use_policy/alter_table_add_policy.groovy
@@ -83,8 +83,8 @@ suite("add_table_policy_by_alter_table") {
     def alter_table_when_table_has_storage_policy_result = try_sql """
         ALTER TABLE create_table_not_have_policy set ("storage_policy" = "created_create_table_alter_policy");
     """
-    // errCode = 2, detailMessage = Do not support alter table's storage policy , this table [create_table_not_have_policy] has storage policy created_create_table_alter_policy
-    assertEquals(alter_table_when_table_has_storage_policy_result, null);
+    // OK
+    assertEquals(alter_table_when_table_has_storage_policy_result.size(), 1);
 
     if (!storage_exist.call("created_create_table_alter_policy_1")) {
         def create_s3_resource = try_sql """
@@ -113,8 +113,8 @@ suite("add_table_policy_by_alter_table") {
     def cannot_modify_exist_storage_policy_table_result = try_sql """
         ALTER TABLE create_table_not_have_policy set ("storage_policy" = "created_create_table_alter_policy_1");
     """
-    //  errCode = 2, detailMessage = Do not support alter table's storage policy , this table [create_table_not_have_policy] has storage policy created_create_table_alter_policy
-    assertEquals(cannot_modify_exist_storage_policy_table_result, null);
+    // OK
+    assertEquals(cannot_modify_exist_storage_policy_table_result.size(), 1);
 
     // you can change created_create_table_alter_policy's policy cooldown time, cooldown ttl property,
     // by alter storage policy
diff --git a/regression-test/suites/correctness_p0/test_bucket_join_with_colocate_table.groovy b/regression-test/suites/correctness_p0/test_bucket_join_with_colocate_table.groovy
index 809f415d8..3829b5d0a 100644
--- a/regression-test/suites/correctness_p0/test_bucket_join_with_colocate_table.groovy
+++ b/regression-test/suites/correctness_p0/test_bucket_join_with_colocate_table.groovy
@@ -72,6 +72,6 @@
          ;
      """
 
-     order_qt_select """  select * from ${colocateTableName} right outer join ${rightTable} on ${colocateTableName}.c1 = ${rightTable}.k1; """
+     order_qt_select """  select * from ${colocateTableName} right outer join ${rightTable} on ${colocateTableName}.c1 = ${rightTable}.k1 order by c1; """
  }
 
diff --git a/regression-test/suites/decimalv3/test_decimalv3.groovy b/regression-test/suites/decimalv3/test_decimalv3.groovy
index 374e554b9..8b8b01024 100644
--- a/regression-test/suites/decimalv3/test_decimalv3.groovy
+++ b/regression-test/suites/decimalv3/test_decimalv3.groovy
@@ -26,4 +26,5 @@ suite("test_decimalv3") {
 	sql "create view test5_v (amout) as select cast(a*b as decimalv3(38,18)) from test5"
 
 	qt_decimalv3 "select * from test5_v"
+	qt_decimalv3 "select cast(a as decimalv3(12,10)) * cast(b as decimalv3(18,10)) from test5"
 }
diff --git a/regression-test/suites/external_table_emr_p2/hive/test_external_yandex_nereids.groovy b/regression-test/suites/external_table_emr_p2/hive/test_external_yandex_nereids.groovy
new file mode 100644
index 000000000..9f06df310
--- /dev/null
+++ b/regression-test/suites/external_table_emr_p2/hive/test_external_yandex_nereids.groovy
@@ -0,0 +1,83 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+suite("test_external_yandex_nereids", "p2") {
+
+    def formats = ["_parquet"]
+    def duplicateAggregationKeys = "SELECT URL, EventDate, max(URL) FROM hitsSUFFIX WHERE CounterID = 1704509 AND UserID = 4322253409885123546 GROUP BY URL, EventDate, EventDate ORDER BY URL, EventDate;"
+    def like1 = """SELECT count() FROM hitsSUFFIX WHERE URL LIKE '%/avtomobili_s_probegom/_%__%__%__%';"""
+    def like2 = """SELECT count() FROM hitsSUFFIX WHERE URL LIKE '/avtomobili_s_probegom/_%__%__%__%';"""
+    def like3 = """SELECT count() FROM hitsSUFFIX WHERE URL LIKE '%_/avtomobili_s_probegom/_%__%__%__%';"""
+    def like4 = """SELECT count() FROM hitsSUFFIX WHERE URL LIKE '%avtomobili%';"""
+    def loyalty = """SELECT loyalty, count() AS c
+        FROM
+        (
+            SELECT UserID, CAST(((if(yandex > google, yandex / (yandex + google), 0 - google / (yandex + google))) * 10) AS TINYINT) AS loyalty
+            FROM
+            (
+                SELECT UserID, sum(if(SearchEngineID = 2, 1, 0)) AS yandex, sum(if(SearchEngineID = 3, 1, 0)) AS google
+                FROM hitsSUFFIX
+                WHERE SearchEngineID = 2 OR SearchEngineID = 3 GROUP BY UserID HAVING yandex + google > 10
+            ) t1
+        ) t2
+        GROUP BY loyalty
+        ORDER BY loyalty;"""
+    def maxStringIf = """SELECT CounterID, count(), max(if(SearchPhrase != "", SearchPhrase, "")) FROM hitsSUFFIX GROUP BY CounterID ORDER BY count() DESC LIMIT 20;"""
+    def minMax = """SELECT CounterID, min(WatchID), max(WatchID) FROM hitsSUFFIX GROUP BY CounterID ORDER BY count() DESC LIMIT 20;"""
+    def monotonicEvaluationSegfault = """SELECT max(0) FROM visitsSUFFIX WHERE (CAST(CAST(StartDate AS DATETIME) AS INT)) > 1000000000;"""
+    def subqueryInWhere = """SELECT count() FROM hitsSUFFIX WHERE UserID IN (SELECT UserID FROM hitsSUFFIX WHERE CounterID = 800784);"""
+    def where01 = """SELECT CounterID, count(distinct UserID) FROM hitsSUFFIX WHERE 0 != 0 GROUP BY CounterID;"""
+    def where02 = """SELECT CounterID, count(distinct UserID) FROM hitsSUFFIX WHERE CAST(0 AS BOOLEAN) AND CounterID = 1704509 GROUP BY CounterID;"""
+
+    String enabled = context.config.otherConfigs.get("enableExternalHiveTest")
+    if (enabled != null && enabled.equalsIgnoreCase("true")) {
+        String extHiveHmsHost = context.config.otherConfigs.get("extHiveHmsHost")
+        String extHiveHmsPort = context.config.otherConfigs.get("extHiveHmsPort")
+        String catalog_name = "external_yandex_nereids"
+
+        sql """drop catalog if exists ${catalog_name};"""
+        sql """
+            create catalog if not exists ${catalog_name} properties (
+                'type'='hms',
+                'hive.metastore.uris' = 'thrift://${extHiveHmsHost}:${extHiveHmsPort}'
+            );
+        """
+        logger.info("catalog " + catalog_name + " created")
+        sql """switch ${catalog_name};"""
+        logger.info("switched to catalog " + catalog_name)
+        sql """use multi_catalog;"""
+        logger.info("use multi_catalog")
+        sql """set enable_nereids_planner=true"""
+
+        for (String format in formats) {
+            logger.info("Process format " + format)
+            qt_01 duplicateAggregationKeys.replace("SUFFIX", format)
+            qt_02 like1.replace("SUFFIX", format)
+            qt_03 like2.replace("SUFFIX", format)
+            qt_04 like3.replace("SUFFIX", format)
+            qt_05 like4.replace("SUFFIX", format)
+            qt_06 loyalty.replace("SUFFIX", format)
+            qt_07 maxStringIf.replace("SUFFIX", format)
+            qt_08 minMax.replace("SUFFIX", format)
+            qt_09 monotonicEvaluationSegfault.replace("SUFFIX", format)
+            qt_10 subqueryInWhere.replace("SUFFIX", format)
+            qt_11 where01.replace("SUFFIX", format)
+            qt_12 where02.replace("SUFFIX", format)
+        }
+    }
+}
+
diff --git a/regression-test/suites/nereids_syntax_p0/lateral_view.groovy b/regression-test/suites/nereids_syntax_p0/lateral_view.groovy
index af7410a32..2c1cd9f1f 100644
--- a/regression-test/suites/nereids_syntax_p0/lateral_view.groovy
+++ b/regression-test/suites/nereids_syntax_p0/lateral_view.groovy
@@ -63,4 +63,12 @@ suite("nereids_lateral_view") {
           LATERAL VIEW explode_json_array_int(c3) lv3 AS clv3
           LATERAL VIEW explode_json_array_double_outer(c4) lv4 AS clv4
     """
+
+    qt_alias_query """
+        SELECT clv1, clv3, c2, c4 FROM (SELECT * FROM nlv_test) tmp
+          LATERAL VIEW explode_numbers(c1) lv1 AS clv1
+          LATERAL VIEW explode_json_array_string_outer(c2) lv2 AS clv2
+          LATERAL VIEW explode_json_array_int(c3) lv3 AS clv3
+          LATERAL VIEW explode_json_array_double_outer(c4) lv4 AS clv4
+    """
 }
diff --git a/regression-test/suites/nereids_syntax_p0/select_const.groovy b/regression-test/suites/nereids_syntax_p0/select_const.groovy
new file mode 100644
index 000000000..fff8831c0
--- /dev/null
+++ b/regression-test/suites/nereids_syntax_p0/select_const.groovy
@@ -0,0 +1,50 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+suite("select_with_const") {
+    sql "SET enable_nereids_planner=true"
+
+    sql """
+        DROP TABLE IF EXISTS select_with_const
+       """
+
+    sql """CREATE TABLE IF NOT EXISTS select_with_const (col1 int not null, col2 int not null, col3 int not null)
+        DISTRIBUTED BY HASH(col3)
+        BUCKETS 1
+        PROPERTIES(
+            "replication_num"="1"
+        )
+        """
+
+    sql """
+    insert into select_with_const values(1994, 1994, 1995)
+    """
+
+    sql "SET enable_fallback_to_original_planner=false"
+
+    qt_select """
+        select '' as 'b', col1 from select_with_const
+    """
+
+    qt_select """
+        select '' as 'b' from select_with_const
+    """
+
+    qt_select """
+        SELECT col1 AS 'str' FROM select_with_const
+    """
+}
diff --git a/regression-test/suites/nereids_syntax_p0/test_arithmetic_operators.groovy b/regression-test/suites/nereids_syntax_p0/test_arithmetic_operators.groovy
new file mode 100644
index 000000000..e7fe4a8c4
--- /dev/null
+++ b/regression-test/suites/nereids_syntax_p0/test_arithmetic_operators.groovy
@@ -0,0 +1,80 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+
+suite("nereids_test_arithmetic_operators", "query,p0") {
+    def tableName = "test"
+
+    sql "use test_query_db"
+    sql "SET enable_vectorized_engine=true"
+    sql "SET enable_nereids_planner=true"
+    sql "SET enable_fallback_to_original_planner=false"
+
+    qt_arith_op1 "select k1, k4 div k1, k4 div k2, k4 div k3, k4 div k4 \
+		    from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op2 "select k1, k1+ '1', k5,100000*k5 from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op3 "select k1,k5,k2*k5 from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op4 "select k1,k5,k8*k5,k5*k9,k2*k9,k2*k8 from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op5 "select k1, k5*0.1, k8*0.1, k9*0.1 from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op6 "select k1, k2*(-0.1), k3*(-0.1), k4*(-0.1), \
+		    k5*(-0.1), k8*(-0.1), k9*(-0.1) from  ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op7 "select k1, k5*(9223372036854775807/100), k8*9223372036854775807, \
+		    k9*9223372036854775807 from  ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op8 "select k1, k2/9223372036854775807, k3/9223372036854775807, \
+		    k4/9223372036854775807,k5/9223372036854775807, \
+		    k8/9223372036854775807,k9/9223372036854775807 \
+		    from  ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op9 "select k1, k5+9223372036854775807/100, k8+9223372036854775807, \
+		    k9+9223372036854775807 from  ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op10 "select k1, k5-9223372036854775807/100, k8-9223372036854775807, \
+		    k9-9223372036854775807 from  ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op11 "select k1, k5/0.000001, k8/0.000001, \
+		    k9/0.000001 from  ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op12 "select k1, k1*0.1, k2*0.1, k3*0.1, k4*0.1, k5*0.1, k8*0.1, k9*0.1 \
+		    from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op13 "select k1, k1/10, k2/10, k3/10, k4/10, k5/10, k8/10, k9/10 \
+		    from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op14 "select k1, k1-0.1, k2-0.1, k3-0.1, k4-0.1, k5-0.1, k8-0.1, k9-0.1 \
+		    from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op15 "select k1, k1+0.1, k2+0.1, k3+0.1, k4+0.1, k5+0.1, k8+0.1, k9+0.1 \
+		    from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op16 "select k1+10, k2+10.0, k3+1.6, k4*1, k5-6, k8-234.66, k9-0 \
+		    from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op17 "select * from ${tableName} where k1+k9<0 order by k1, k2, k3, k4"
+    qt_arith_op18 "select k1*k2*k3*k5 from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op19 "select k1*k2*k3*k5*k8*k9 from ${tableName} order by k1, k2, k3, k4"
+    qt_arith_op20 "select k1*10000/k4/k8/k9 from ${tableName} order by k1, k2, k3, k4"
+    
+    for( i in [1, 2, 3, 5, 8, 9]) {
+        for( j in [1, 2, 3, 5, 8, 9]) {
+            qt_arith_op21 "select k${i}*k${j}, k${i}+k${j}, k${i}-k${j}, k${i}/k${j} from ${tableName} \
+			    where abs(k${i})<9223372036854775807 and k${j}<>0 and\
+			    abs(k${i})<922337203685477580 order by k1, k2, k3, k4"
+        }
+    }
+    
+    qt_arith_op22 "select 1.1*1.1 + k2 from ${tableName} order by 1 limit 10"
+    qt_arith_op23 "select 1.1*1.1 + k5 from ${tableName} order by 1 limit 10"
+    qt_arith_op24 "select 1.1*1.1+1.1"
+
+    // divide mod zero
+    qt_arith_op25 "select 10.2 / 0.0, 10.2 / 0, 10.2 % 0.0, 10.2 % 0"
+    qt_arith_op26 "select 0.0 / 0.0, 0.0 / 0, 0.0 % 0.0, 0.0 % 0"
+    qt_arith_op27 "select -10.2 / 0.0, -10.2 / 0, -10.2 % 0.0, -10.2 % 0"
+    qt_arith_op28 "select k5 / 0, k8 / 0, k9 / 0 from ${tableName} order by k1,k2,k3,k4"
+    qt_arith_op29 "select k5 % 0, k8 % 0, k9 % 0 from ${tableName} order by k1,k2,k3,k4"
+}
diff --git a/regression-test/suites/nereids_syntax_p0/test_join3.groovy b/regression-test/suites/nereids_syntax_p0/test_join3.groovy
new file mode 100644
index 000000000..17a0ad76c
--- /dev/null
+++ b/regression-test/suites/nereids_syntax_p0/test_join3.groovy
@@ -0,0 +1,101 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+suite("nereids_test_join3", "query,p0") {
+
+    sql "SET enable_vectorized_engine=true"
+    sql "SET enable_nereids_planner=true"
+    sql "SET enable_fallback_to_original_planner=false"
+
+    def DBname = "regression_test_join3"
+    sql "DROP DATABASE IF EXISTS ${DBname}"
+    sql "CREATE DATABASE IF NOT EXISTS ${DBname}"
+    sql "use ${DBname}"
+
+    def tbName1 = "t1"
+    def tbName2 = "t2"
+    def tbName3 = "t3"
+
+    sql """CREATE TABLE IF NOT EXISTS ${tbName1} (name varchar(255), n INTEGER) DISTRIBUTED BY HASH(name) properties("replication_num" = "1");"""
+    sql """CREATE TABLE IF NOT EXISTS ${tbName2} (name varchar(255), n INTEGER) DISTRIBUTED BY HASH(name) properties("replication_num" = "1");"""
+    sql """CREATE TABLE IF NOT EXISTS ${tbName3} (name varchar(255), n INTEGER) DISTRIBUTED BY HASH(name) properties("replication_num" = "1");"""
+
+    sql "INSERT INTO ${tbName1} VALUES ( 'bb', 11 );"
+    sql "INSERT INTO ${tbName2} VALUES ( 'bb', 12 );"
+    sql "INSERT INTO ${tbName2} VALUES ( 'cc', 22 );"
+    sql "INSERT INTO ${tbName2} VALUES ( 'ee', 42 );"
+    sql "INSERT INTO ${tbName3} VALUES ( 'bb', 13 );"
+    sql "INSERT INTO ${tbName3} VALUES ( 'cc', 23 );"
+    sql "INSERT INTO ${tbName3} VALUES ( 'dd', 33 );"
+
+    qt_join1 """
+            SELECT * FROM ${tbName1} FULL JOIN ${tbName2} USING (name) FULL JOIN ${tbName3} USING (name) ORDER BY 1,2,3,4,5,6;
+        """
+    qt_join2 """
+            SELECT * FROM
+            (SELECT * FROM ${tbName2}) as s2
+            INNER JOIN
+            (SELECT * FROM ${tbName3}) s3
+            USING (name)
+            ORDER BY 1,2,3,4;
+        """
+    qt_join3 """
+            SELECT * FROM
+            (SELECT * FROM ${tbName2}) as s2
+            LEFT JOIN
+            (SELECT * FROM ${tbName3}) s3
+            USING (name)
+            ORDER BY 1,2,3,4;
+        """
+    qt_join4 """
+            SELECT * FROM
+            (SELECT * FROM ${tbName2}) as s2
+            FULL JOIN
+            (SELECT * FROM ${tbName3}) s3
+            USING (name)
+            ORDER BY 1,2,3,4;
+        """
+
+// wait fix
+//     qt_join5 """
+//             SELECT * FROM
+//             (SELECT name, n as s2_n, 2 as s2_2 FROM ${tbName2}) as s2
+//             NATURAL INNER JOIN
+//             (SELECT name, n as s3_n, 3 as s3_2 FROM ${tbName3}) s3
+//             ORDER BY 1,2,3,4;
+//         """
+
+//     qt_join6 """
+//             SELECT * FROM
+//             (SELECT name, n as s1_n, 1 as s1_1 FROM ${tbName1}) as s1
+//             NATURAL INNER JOIN
+//             (SELECT name, n as s2_n, 2 as s2_2 FROM ${tbName2}) as s2
+//             NATURAL INNER JOIN
+//             (SELECT name, n as s3_n, 3 as s3_2 FROM ${tbName3}) s3;
+//         """
+
+    qt_join7 """
+        SELECT * FROM
+          (SELECT name, n as s1_n FROM ${tbName1}) as s1
+        FULL JOIN
+          (SELECT name, 2 as s2_n FROM ${tbName2}) as s2
+        ON (s1_n = s2_n)
+        ORDER BY 1,2,3,4;
+        """
+
+    // sql "DROP DATABASE IF EXISTS ${DBname}"
+}
diff --git a/regression-test/suites/nereids_syntax_p0/test_query_between.groovy b/regression-test/suites/nereids_syntax_p0/test_query_between.groovy
new file mode 100644
index 000000000..d3b4b1830
--- /dev/null
+++ b/regression-test/suites/nereids_syntax_p0/test_query_between.groovy
@@ -0,0 +1,42 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+suite("nereids_test_query_between", "query,p0") {
+    sql"use test_query_db"
+
+    sql "SET enable_vectorized_engine=true"
+    sql "SET enable_nereids_planner=true"
+    sql "SET enable_fallback_to_original_planner=false"
+
+    def tableName = "test"
+    qt_between1 "select if(k1 between 1 and 2, 2, 0) as wj from ${tableName} order by wj"
+    qt_between2 "select k1 from ${tableName} where k1 between 3 and 4 order by k1, k2, k3, k4"
+    qt_between3 "select k2 from ${tableName} where k2 between 1980 and 1990 order by k1, k2, k3, k4"
+    qt_between4 "select k3 from ${tableName} where k3 between 1000 and 2000 order by k1, k2, k3, k4"
+    qt_between5 "select k4 from ${tableName} where k4 between -100000000 and 0 order by k1, k2, k3, k4"
+    qt_between6 "select k6 from ${tableName} where lower(k6) between 'f' and 'false' order by k1, k2, k3, k4"
+    qt_between7 "select k7 from ${tableName} where lower(k7) between 'a' and 'g' order by k1, k2, k3, k4"
+    qt_between8 "select k8 from ${tableName} where k8 between -2 and 0 order by k1, k2, k3, k4"
+    qt_between9 """select k10 from ${tableName} where k10 between \"2015-04-02 00:00:00\"
+                and \"9999-12-31 12:12:12\" order by k1, k2, k3, k4"""
+    qt_between10 """select k11 from ${tableName} where k11 between \"2015-04-02 00:00:00\"
+                and \"9999-12-31 12:12:12\" order by k1, k2, k3, k4"""
+    qt_between11 """select k10 from ${tableName} where k10 between \"2015-04-02\"
+                and \"9999-12-31\" order by k1, k2, k3, k4"""
+    qt_between12 "select k9 from ${tableName} where k9 between -1 and 6.333 order by k1, k2, k3, k4"
+    qt_between13 "select k5 from ${tableName} where k5 between 0 and 1243.5 order by k1, k2, k3, k4"
+}
diff --git a/regression-test/suites/nereids_syntax_p0/using_join.groovy b/regression-test/suites/nereids_syntax_p0/using_join.groovy
index ecbc714d9..d53d37865 100644
--- a/regression-test/suites/nereids_syntax_p0/using_join.groovy
+++ b/regression-test/suites/nereids_syntax_p0/using_join.groovy
@@ -66,11 +66,11 @@ suite("nereids_using_join") {
     sql """INSERT INTO t2 VALUES('6', 3, 1)"""
     sql """INSERT INTO t2 VALUES('7', 4, 1)"""
 
-    qt_sql """
+    order_qt_sql """
         SELECT t1.col1 FROM t1 JOIN t2 USING (col1)
     """
 
-    qt_sql """
+    order_qt_sql """
         SELECT t1.col1 FROM t1 JOIN t2 USING (col1, col2)
     """
 
diff --git a/regression-test/suites/performance_p0/redundant_conjuncts.groovy b/regression-test/suites/performance_p0/redundant_conjuncts.groovy
index 3bf5f2130..ef0434aa0 100644
--- a/regression-test/suites/performance_p0/redundant_conjuncts.groovy
+++ b/regression-test/suites/performance_p0/redundant_conjuncts.groovy
@@ -35,6 +35,7 @@ suite("redundant_conjuncts") {
     EXPLAIN SELECT v1 FROM redundant_conjuncts WHERE k1 = 1 AND k1 = 1;
     """
 
+    sql "set REWRITE_OR_TO_IN_PREDICATE_THRESHOLD = 100"
     qt_redundant_conjuncts_gnerated_by_extract_common_filter """
     EXPLAIN SELECT v1 FROM redundant_conjuncts WHERE k1 = 1 OR k1 = 2;
     """
diff --git a/regression-test/suites/primary_index/test_pk_uk_case.groovy b/regression-test/suites/primary_index/test_pk_uk_case.groovy
index 9403d5e92..1ae47e3b1 100644
--- a/regression-test/suites/primary_index/test_pk_uk_case.groovy
+++ b/regression-test/suites/primary_index/test_pk_uk_case.groovy
@@ -246,7 +246,7 @@ suite("test_pk_uk_case") {
         for (int i = 0; i < result0.size(); ++i) {
             for (j = 0; j < result0[0].size(); j++) {
                 logger.info("result: " + result0[i][j] + "|" + result1[i][j])
-                assertTrue(result0[0]==result1[0])
+                assertTrue(result0[i][j]==result1[i][j])
             }
         }       
 
